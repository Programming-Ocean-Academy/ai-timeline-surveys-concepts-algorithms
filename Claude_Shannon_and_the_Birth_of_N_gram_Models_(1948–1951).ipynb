{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![download.jpg](data:image/jpeg;base64,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)"
      ],
      "metadata": {
        "id": "L410T4ZajEym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Claude Shannon and the Birth of N-gram Models (1948–1951)\n",
        "\n",
        "# https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf\n",
        "\n",
        "#### 1. Origin Paper\n",
        "\n",
        "**Author:** Claude E. Shannon  \n",
        "**Paper:** “A Mathematical Theory of Communication”  \n",
        "**Published in:** *Bell System Technical Journal*, July and October 1948  \n",
        "\n",
        "Claude E. Shannon’s 1948 publication marked the birth of *information theory* and introduced a mathematical framework for digital communication and probabilistic language modeling.  \n",
        "He defined **entropy**, **redundancy**, **information content**, and **mutual information**, showing that language could be treated as a probabilistic signal rather than a purely grammatical structure.  \n",
        "\n",
        "Shannon’s insight was that **language is statistically predictable**, and its structure can be represented through probability distributions over sequences of symbols.  \n",
        "This became the foundation of **language modeling**, decades before the formal emergence of computational linguistics.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. Language Modeling Experiments\n",
        "\n",
        "In his 1948 paper and later notes, Shannon asked:  \n",
        "> “How well can we predict the next symbol in a sequence of English text?”\n",
        "\n",
        "To answer this, he generated text sequences by sampling letters with varying degrees of contextual dependency, introducing what became known as **n-gram models**.  \n",
        "Each model added more context, producing increasingly realistic English-like text.\n",
        "\n",
        "| Model | Example Output | Description |\n",
        "| ------ | --------------- | ------------ |\n",
        "| **0-gram (uniform random)** | “XFOML RXKHRJFFJUJ ZLPWCFWKCYJ…” | Random letters with no structure. |\n",
        "| **1-gram (unigram)** | “OCRO HLI RGWR NMIELWIS…” | Reflects single-letter frequencies but no order. |\n",
        "| **2-gram (bigram)** | “ON IE ANTSOUTINYS ARE T…” | Captures pairwise letter dependencies; some structure appears. |\n",
        "| **3-gram (trigram)** | “IN NO IST LAT WHEY CRATICT…” | Generates word-like and rhythmic sequences. |\n",
        "\n",
        "These experiments represent the **first generative text models**. Shannon demonstrated that probabilistic dependencies alone could produce meaningful linguistic patterns—decades before neural networks.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. Key Concepts Introduced by Shannon\n",
        "\n",
        "| Concept | Contribution |\n",
        "| -------- | ------------- |\n",
        "| **Markov assumption** | Proposed that the probability of a symbol depends on a fixed number of prior symbols — the *finite-order Markov process*, foundational to n-gram models. |\n",
        "| **Entropy of English** | Estimated the information per letter to be between **1 and 1.5 bits**, proving that English is redundant yet structured. |\n",
        "| **Generative modeling** | By sampling from probabilistic distributions, Shannon effectively created the first generative model of text. |\n",
        "| **Statistical language modeling** | His approach inspired **unigram**, **bigram**, and **trigram** models, dominating statistical NLP for decades. |\n",
        "\n",
        "Shannon’s work mathematically unified linguistics, probability, and computation, transforming language into a measurable signal.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. Later Influence and Evolution\n",
        "\n",
        "| Period | Development | Connection to Shannon |\n",
        "| ------- | ------------ | --------------------- |\n",
        "| **1950s–1970s** | Early computational linguistics and speech modeling used *Markov chains* to describe phoneme and letter transitions. | Direct continuation of Shannon’s stochastic process framework. |\n",
        "| **1980s–1990s** | N-gram models became central to speech recognition and machine translation (IBM, Bell Labs). Techniques such as **Katz back-off (1987)** and **Kneser–Ney (1995)** improved estimation. | Practical extension of Shannon’s probabilistic modeling. |\n",
        "| **2000s** | *Neural probabilistic language models* (Bengio et al., 2003) introduced distributed representations, replacing discrete n-gram counts. | Same predictive principle, applied in continuous vector space. |\n",
        "| **2017–present** | *Transformers* (Vaswani et al., 2017) replaced fixed-length contexts with self-attention, capturing global dependencies. | Realization of Shannon’s vision at scale — predictive text generation from learned statistics. |\n",
        "\n",
        "From letter-based probabilities to trillion-parameter models, the underlying task — **predict the next symbol given prior context** — remains Shannon’s enduring legacy.\n",
        "\n",
        "---\n",
        "\n",
        "#### 5. Summary and Legacy\n",
        "\n",
        "| Aspect | Shannon’s Role |\n",
        "| ------- | --------------- |\n",
        "| **Inventor of the n-gram concept** | Modeled language statistically through sequential symbol dependencies. |\n",
        "| **Purpose** | Quantify uncertainty and predictability in language via information theory. |\n",
        "| **Impact** | All modern language models, from statistical bigrams to GPT-scale transformers, trace conceptual lineage to Shannon’s work. |\n",
        "\n",
        "Shannon’s 1948 experiments proved that **language could be generated, predicted, and quantified**.  \n",
        "This union of mathematics and linguistics laid the intellectual groundwork for the evolution of probabilistic and neural language models — from **n-grams** to **LLMs**.\n",
        "\n",
        "$$\n",
        "P(w_i \\mid w_{i-1}, \\ldots, w_{i-n+1}) = \\frac{C(w_{i-n+1}, \\ldots, w_i)}{C(w_{i-n+1}, \\ldots, w_{i-1})}\n",
        "$$\n",
        "\n",
        "This equation — the **n-gram conditional probability** — formalizes Shannon’s core insight:  \n",
        "the likelihood of the next word depends statistically on its preceding context.\n"
      ],
      "metadata": {
        "id": "zJw8QAzwip4r"
      }
    }
  ]
}