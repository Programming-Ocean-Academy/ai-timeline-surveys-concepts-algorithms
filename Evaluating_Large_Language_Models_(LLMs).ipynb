{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating **Large Language Models (LLMs)** requires multiple complementary measures, as no single metric captures all aspects of performance. These measures can be grouped into **five main evaluation dimensions**:\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Language Understanding & Generation Quality\n",
        "\n",
        "These measure fluency, coherence, and semantic accuracy of generated text.\n",
        "\n",
        "* **Perplexity (PPL):** Measures how well the model predicts a sample. Lower is better.\n",
        "\n",
        "  $$\n",
        "  PPL = e^{-\\frac{1}{N}\\sum_i \\log P(x_i)}\n",
        "  $$\n",
        "\n",
        "* **BLEU (Bilingual Evaluation Understudy):** Measures n-gram overlap with reference text (used in translation).\n",
        "* **ROUGE (Recall-Oriented Understudy for Gisting Evaluation):** Measures recall of overlapping n-grams (used in summarization).\n",
        "* **METEOR, CIDEr, BERTScore:** Use semantic similarity or contextual embeddings for better alignment with human judgment.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Knowledge & Reasoning Benchmarks\n",
        "\n",
        "Test factuality, reasoning, and world knowledge.\n",
        "\n",
        "* **MMLU (Massive Multitask Language Understanding):** Measures accuracy across 57 academic tasks (STEM, humanities, etc.).\n",
        "* **ARC (AI2 Reasoning Challenge):** Multiple-choice science reasoning.\n",
        "* **HellaSwag / PIQA / Winogrande:** Commonsense reasoning and physical understanding.\n",
        "* **TruthfulQA:** Evaluates factual accuracy and resistance to false or misleading statements.\n",
        "* **BIG-Bench / BIG-Bench Hard:** Wide set of reasoning, logic, and linguistic tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Instruction Following & Chat Quality\n",
        "\n",
        "Used to test models fine-tuned for dialogue or alignment.\n",
        "\n",
        "* **MT-Bench:** Multi-turn conversation benchmark graded by GPT-4.\n",
        "* **AlpacaEval / Arena-Hard:** Human and model-based pairwise comparisons of helpfulness and reasoning.\n",
        "* **ToxiGen / RealToxicityPrompts:** Evaluates harmful or biased content generation.\n",
        "* **OpenAI’s Internal Reward Model Scores:** Combine helpfulness, honesty, and harmlessness dimensions (HHH).\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Efficiency & Robustness Metrics\n",
        "\n",
        "Assess computational, robustness, and generalization properties.\n",
        "\n",
        "* **Inference Latency:** Time per token generation.\n",
        "* **Memory Efficiency:** GPU/CPU usage, parameter efficiency.\n",
        "* **Robustness Tests:** Performance under noisy or adversarial prompts.\n",
        "* **Calibration Metrics:** Measures how well predicted probabilities match actual correctness.\n",
        "* **Compression/Distillation Metrics:** Quality retention after pruning or quantization.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Human & Societal Evaluation\n",
        "\n",
        "Assess trustworthiness, alignment, and user satisfaction.\n",
        "\n",
        "* **Human Evaluation Scores:** Experts rate coherence, usefulness, factuality, safety.\n",
        "* **Bias & Fairness Metrics:** Gender/race bias tests (e.g., BBQ, StereoSet).\n",
        "* **Hallucination Rate:** Percentage of incorrect factual outputs.\n",
        "* **Safety & Alignment Benchmarks:** Red-teaming results, refusal accuracy, and jailbreak resilience.\n",
        "\n",
        "---\n",
        "\n",
        "## Summary Table\n",
        "\n",
        "| Dimension                   | Example Metrics                    | Evaluates                           |\n",
        "| --------------------------- | ---------------------------------- | ----------------------------------- |\n",
        "| **Language Quality**        | Perplexity, BLEU, ROUGE, BERTScore | Fluency, coherence, text similarity |\n",
        "| **Knowledge & Reasoning**   | MMLU, ARC, HellaSwag, TruthfulQA   | Factual and reasoning ability       |\n",
        "| **Instruction Following**   | MT-Bench, AlpacaEval, Arena-Hard   | Dialogue quality, alignment         |\n",
        "| **Efficiency & Robustness** | Latency, calibration, robustness   | Performance stability               |\n",
        "| **Human/Societal**          | Bias, hallucination, human evals   | Safety, ethics, user trust          |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JRj_Fm2L4AN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GLUE (General Language Understanding Evaluation)** is one of the most influential benchmark suites in NLP, introduced in **2018** to measure the **general language understanding** ability of models — not just their performance on one dataset. It marked a turning point in the evaluation of pre-trained models like **BERT**, **RoBERTa**, **XLNet**, and **DeBERTa**.\n",
        "\n",
        "---\n",
        "\n",
        "## What is GLUE?\n",
        "\n",
        "GLUE is a **multi-task benchmark** designed to evaluate how well models can understand, reason, and generalize across diverse **linguistic phenomena**.  \n",
        "It tests capabilities such as grammar, semantics, entailment, and sentence similarity using nine separate tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## Purpose of GLUE\n",
        "\n",
        "* Encourage the development of **general-purpose NLU models** that can handle varied linguistic inputs.  \n",
        "* Provide a **standardized, comparative evaluation framework** for pre-trained models.  \n",
        "* Assess **transfer learning effectiveness** — how pretraining on one corpus transfers to multiple downstream NLP tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## Structure of GLUE\n",
        "\n",
        "GLUE consists of **nine tasks**, each targeting a distinct aspect of linguistic understanding:\n",
        "\n",
        "| Task | Dataset | Description | Metric |\n",
        "|------|----------|-------------|---------|\n",
        "| **CoLA** | Corpus of Linguistic Acceptability | Detects if a sentence is grammatically correct. | Matthews Corr. |\n",
        "| **SST-2** | Stanford Sentiment Treebank | Sentiment classification (positive/negative). | Accuracy |\n",
        "| **MRPC** | Microsoft Research Paraphrase Corpus | Determines whether two sentences are paraphrases. | F1 / Accuracy |\n",
        "| **STS-B** | Semantic Textual Similarity Benchmark | Scores semantic similarity between sentences (0–5). | Pearson / Spearman Corr. |\n",
        "| **QQP** | Quora Question Pairs | Detects duplicate questions. | F1 / Accuracy |\n",
        "| **MNLI** | Multi-Genre Natural Language Inference | Classifies entailment, contradiction, or neutral. | Accuracy |\n",
        "| **QNLI** | Question Natural Language Inference | Checks if a sentence answers a given question. | Accuracy |\n",
        "| **RTE** | Recognizing Textual Entailment | Determines entailment relationship between sentences. | Accuracy |\n",
        "| **WNLI** | Winograd NLI | Tests coreference and commonsense reasoning. | Accuracy |\n",
        "\n",
        "---\n",
        "\n",
        "## Evaluation Metric\n",
        "\n",
        "The **GLUE Score** is the average performance across all nine tasks:\n",
        "\n",
        "$$\n",
        "GLUE = \\frac{1}{9} \\sum_{i=1}^{9} S_i\n",
        "$$\n",
        "\n",
        "where \\( S_i \\) is the normalized score (e.g., accuracy, F1, correlation) on task \\( i \\).  \n",
        "A **higher GLUE score** indicates stronger general language understanding and transfer capability.\n",
        "\n",
        "---\n",
        "\n",
        "## SuperGLUE: The Successor (2019)\n",
        "\n",
        "Once models like **RoBERTa** and **DeBERTa** achieved near-human GLUE scores, the benchmark’s difficulty plateaued.  \n",
        "To address this, **SuperGLUE** was introduced with **harder tasks** and **human-level baselines**.\n",
        "\n",
        "Key tasks include:\n",
        "\n",
        "* **BoolQ** — Boolean question answering  \n",
        "* **COPA** — Causal reasoning  \n",
        "* **WiC** — Word sense disambiguation  \n",
        "* **ReCoRD** — Reading comprehension  \n",
        "* **MultiRC**, **CB**, **RTE**, **WSC**\n",
        "\n",
        "SuperGLUE emphasizes **reasoning**, **commonsense**, and **contextual inference** rather than surface-level correlations.\n",
        "\n",
        "---\n",
        "\n",
        "## Why GLUE Still Matters\n",
        "\n",
        "Despite the rise of massive LLMs like **GPT-4**, **Claude**, and **Gemini**, GLUE remains essential because:\n",
        "\n",
        "* It provides a **diagnostic tool** for NLU depth and linguistic generalization.  \n",
        "* It benchmarks **transfer learning performance** in pretraining research.  \n",
        "* It allows **historical comparison** between early models (ELMo, BERT) and newer ones (DeBERTa, T5).\n",
        "\n",
        "Many model papers still include GLUE and **SuperGLUE** scores as standard evaluation baselines.\n",
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "| Category | Description |\n",
        "|-----------|-------------|\n",
        "| **Type** | Multi-task NLP benchmark |\n",
        "| **Purpose** | Evaluate general natural language understanding |\n",
        "| **Tasks** | 9 tasks across syntax, semantics, and inference |\n",
        "| **Metrics** | Accuracy, F1, correlation |\n",
        "| **Introduced** | 2018 (Wang et al.) |\n",
        "| **Successor** | SuperGLUE (2019) with harder reasoning tasks |\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AgOK7VtV4KEx"
      }
    }
  ]
}