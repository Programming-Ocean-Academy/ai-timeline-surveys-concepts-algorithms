{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Chronological Evolution of Attention: From Philosophy to Artificial Intelligence (1700–2025)**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Enlightenment and Early Psychology (1700s–1800s)**\n",
        "\n",
        "| **Scholar** | **Contribution** | **Key Idea** |\n",
        "|:-------------|:----------------|:--------------|\n",
        "| **Locke (1690)** | Treated attention as a *mode of thinking*, not an independent faculty. | Early philosophical framing — awareness as an aspect of cognition. |\n",
        "| **Wolff (1738)** | First to dedicate a textbook chapter to attention. | Marked the formal beginning of attention as a topic in psychology. |\n",
        "| **Kames (1769)** | Defined attention as a mental state preparing for impressions. | Connected attention with perceptual readiness. |\n",
        "| **Stewart (1792)** | Linked attention to memory and skill learning. | Introduced attention as a mechanism for learning and retention. |\n",
        "| **Wundt (1879)** | Founded experimental psychology; distinguished *apperception* (focused awareness). | Positioned attention as the central process of conscious experience. |\n",
        "| **Helmholtz (1880s)** | Demonstrated *covert attention* — shifting focus without eye movement. | Showed separation between physical gaze and mental focus. |\n",
        "| **James (1890)** | Defined attention as “taking possession by the mind” of one object among many. | Emphasized *selectivity* and *limited capacity* — enduring themes in modern models. |\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Behaviorist Period (1900–1950)**\n",
        "\n",
        "| **Scholar / Period** | **Contribution** | **Significance** |\n",
        "|:-----------------------|:----------------|:------------------|\n",
        "| **Titchener & Pillsbury (Early 1900s)** | Treated attention as the *clarity enhancement* of mental content. | Transition between introspectionism and experimentalism. |\n",
        "| **Watson (1913)** | Behaviorism dismisses internal mental states as unobservable. | Temporarily sidelines attention research. |\n",
        "| **Telford (1931)** | Discovered the *psychological refractory period*. | Provided early evidence for serial processing bottlenecks. |\n",
        "| **Stroop (1935)** | Demonstrated *involuntary processing* of irrelevant stimuli. | The *Stroop effect* becomes a cornerstone in studying selective attention. |\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Cognitive Revolution (1950s–1970s)**\n",
        "\n",
        "| **Scholar / Model** | **Theory** | **Core Concept** |\n",
        "|:----------------------|:-----------|:------------------|\n",
        "| **Broadbent (1958)** | *Filter Model* | Attention acts as a bottleneck allowing one input for deep processing. |\n",
        "| **Cherry (1953)** | *Cocktail Party Effect* | Selective attention — unattended meaningful input (like one’s name) can intrude. |\n",
        "| **Treisman (1960/1964)** | *Attenuation Theory* | Unattended inputs are weakened, not fully blocked. |\n",
        "| **Deutsch & Deutsch (1963)** | *Late Selection Theory* | All stimuli processed for meaning before selection occurs. |\n",
        "| **Kahneman (1973)** | *Capacity Model* | Attention as a limited resource distributed by mental effort. |\n",
        "| **Posner (1978–1980)** | *Spatial Cueing Paradigm* | Attention as a movable “spotlight” enhancing perceptual efficiency. |\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Cognitive Neuroscience Integration (1980–2000)**\n",
        "\n",
        "| **Scholar / Model** | **Contribution** | **Neural or Computational Insight** |\n",
        "|:----------------------|:----------------|:------------------------------------|\n",
        "| **Treisman & Gelade (1980)** | *Feature Integration Theory* | Attention binds features (color, shape) into unified percepts. |\n",
        "| **Posner (1980)** | Model of orienting (shift, engage, disengage). | Describes cognitive stages of attentional movement. |\n",
        "| **Moran & Desimone (1985)** | Neural basis of selective enhancement. | Attention amplifies firing rates for attended stimuli. |\n",
        "| **Posner & Petersen (1990)** | Identified *alerting*, *orienting*, and *executive control* networks. | Defined tripartite neural architecture of attention. |\n",
        "| **Desimone & Duncan (1995)** | *Biased Competition Theory.* | Attention biases neural competition toward task-relevant inputs. |\n",
        "| **Koch & Ullman (1985)** | Proposed *computational saliency maps.* | Laid groundwork for visual and computational models of attention. |\n",
        "| **1990s Neuroimaging Era** | PET/fMRI reveal distributed fronto-parietal attention networks. | Integrated cognitive theory with neural evidence. |\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Attention Enters Computer Science (2000–2025)**\n",
        "\n",
        "| **Period / Model** | **Contribution** | **Impact on AI** |\n",
        "|:--------------------|:----------------|:------------------|\n",
        "| **Schmidhuber (1990s)** | Proposed “fast weight” networks — early dynamic weight modulation. | Anticipated the mechanism of learned attention control. |\n",
        "| **Bahdanau et al. (2015)** | Introduced neural attention for sequence-to-sequence translation. | Enabled dynamic focus on input tokens; revolutionized NLP. |\n",
        "| **Xu et al. (2015)** | *Show, Attend and Tell* — visual attention for captioning. | Extended attention to image–text understanding. |\n",
        "| **Vaswani et al. (2017)** | *Transformer* architecture. | Replaced recurrence with self-attention; foundation of modern LLMs. |\n",
        "| **2018–2020** | *BERT, Non-local Neural Networks, GAT, Vision Transformers (ViT)* | Unified attention paradigm across language, vision, and graphs. |\n",
        "| **2021–2025** | *CLIP, AlphaFold2, GPT-4 and successors.* | Attention becomes the universal computational mechanism for multimodal intelligence. |\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Conceptual Evolution**\n",
        "\n",
        "| **Transition** | **Shift in Understanding** |\n",
        "|:----------------|:----------------------------|\n",
        "| **Philosophy → Psychology** | From introspection and metaphysical focus to measurable mental phenomena. |\n",
        "| **Psychology → Neuroscience** | From behavioral inference to neural correlates and cortical networks. |\n",
        "| **Neuroscience → Computer Science** | From brain-inspired models to algorithmic implementations. |\n",
        "| **AI Analogy** | Both brains and machines use attention to filter, prioritize, and bind relevant information in complex environments. |\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Core Insight**\n",
        "\n",
        "> Over three centuries, **attention has evolved** from a metaphysical construct of *mental focus* into a **formal computational mechanism** governing selective information processing.\n",
        "\n",
        "It has become the **conceptual bridge between mind and machine** — linking philosophy, psychology, neuroscience, and artificial intelligence through a shared principle:  \n",
        "the allocation of limited resources to what matters most.\n"
      ],
      "metadata": {
        "id": "ERwBE8Gnttyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chronological Evolution of Attention in Machine Learning\n",
        "\n",
        "---\n",
        "\n",
        "## Pre-Neural Foundations (Conceptual Roots)\n",
        "\n",
        "### 1950s–1960s — Cognitive Attention Theory\n",
        "\n",
        "**Field:** Psychology & Neuroscience  \n",
        "\n",
        "**Concepts:**\n",
        "\n",
        "- Cocktail party effect  \n",
        "- Filter models of attention  \n",
        "- Partial report paradigm  \n",
        "- Saccadic eye control  \n",
        "\n",
        "**Impact**\n",
        "\n",
        "Established the principle of selective information processing.  \n",
        "This conceptual foundation later inspired computational models that dynamically allocate representational capacity.\n",
        "\n",
        "---\n",
        "\n",
        "## Proto-Attention Mechanisms (Pre-Deep Learning)\n",
        "\n",
        "### 1980s — Higher-Order Neural Interactions\n",
        "\n",
        "**Concept:** Sigma-Pi Units  \n",
        "\n",
        "- Modeled multiplicative interactions between inputs.  \n",
        "- Implemented pairwise feature interactions:\n",
        "\n",
        "$$\n",
        "y = \\sum_{i,j} w_{ij} x_i x_j\n",
        "$$\n",
        "\n",
        "These multiplicative forms resemble similarity-based weighting used in later attention mechanisms.\n",
        "\n",
        "---\n",
        "\n",
        "### 1990s — Fast Weight Controllers\n",
        "\n",
        "**Researchers:** Jürgen Schmidhuber and collaborators  \n",
        "\n",
        "**Concept:** Fast weights & dynamic neural links  \n",
        "\n",
        "**Contribution**\n",
        "\n",
        "- One network dynamically generates weights for another.\n",
        "- Implemented context-dependent transformations:\n",
        "\n",
        "$$\n",
        "W_t = f(h_{t-1})\n",
        "$$\n",
        "\n",
        "- Anticipated key-value memory systems.\n",
        "- Later interpreted as a precursor to linearized self-attention.\n",
        "\n",
        "---\n",
        "\n",
        "## Attention-like Mechanisms in Vision\n",
        "\n",
        "### 1998 — Bilateral Filtering\n",
        "\n",
        "**Field:** Image Processing  \n",
        "\n",
        "Used pairwise similarity matrices:\n",
        "\n",
        "$$\n",
        "I'(p) = \\frac{1}{Z_p} \\sum_q G_s(\\|p-q\\|) G_r(\\|I_p - I_q\\|) I(q)\n",
        "$$\n",
        "\n",
        "This weighting over pairwise similarities resembles attention weight propagation.\n",
        "\n",
        "---\n",
        "\n",
        "### 2005 — Non-local Means\n",
        "\n",
        "Applied Gaussian similarity kernels for denoising:\n",
        "\n",
        "$$\n",
        "I'(i) = \\sum_j w(i,j) I(j)\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "w(i,j) \\propto \\exp\\left(-\\frac{\\|I(i)-I(j)\\|^2}{h^2}\\right)\n",
        "$$\n",
        "\n",
        "This is structurally analogous to softmax-normalized attention weights.\n",
        "\n",
        "---\n",
        "\n",
        "# Neural Attention Era Begins\n",
        "\n",
        "---\n",
        "\n",
        "## 2014 — Additive Attention (Neural Machine Translation)\n",
        "\n",
        "**Authors:** Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio  \n",
        "\n",
        "### Contribution\n",
        "\n",
        "- First neural attention mechanism.\n",
        "- Applied to Seq2Seq RNN encoder–decoder.\n",
        "- Removed fixed-length context bottleneck.\n",
        "- Introduced learned alignment scoring.\n",
        "\n",
        "### Mathematical Form\n",
        "\n",
        "Alignment score:\n",
        "\n",
        "$$\n",
        "e_{t,i} = v_a^\\top \\tanh(W_a h_{t-1} + U_a s_i)\n",
        "$$\n",
        "\n",
        "Attention weights:\n",
        "\n",
        "$$\n",
        "\\alpha_{t,i} = \\text{softmax}(e_{t,i})\n",
        "$$\n",
        "\n",
        "Context vector:\n",
        "\n",
        "$$\n",
        "c_t = \\sum_i \\alpha_{t,i} s_i\n",
        "$$\n",
        "\n",
        "**Impact**\n",
        "\n",
        "Marked the birth of modern neural attention.\n",
        "\n",
        "---\n",
        "\n",
        "## 2015 — Multiplicative / Dot-Product Attention\n",
        "\n",
        "**Authors:** Minh-Thang Luong, Hieu Pham, Christopher D. Manning  \n",
        "\n",
        "### Contribution\n",
        "\n",
        "- Introduced multiplicative attention.\n",
        "- Replaced additive scoring with dot products.\n",
        "- Improved computational efficiency.\n",
        "\n",
        "Score function:\n",
        "\n",
        "$$\n",
        "e_{t,i} = h_t^\\top s_i\n",
        "$$\n",
        "\n",
        "Generalized form:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q,K,V) = \\text{softmax}(QK^\\top)V\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 2015 — Attention in Vision (Image Captioning)\n",
        "\n",
        "**Authors:** Xu et al.\n",
        "\n",
        "- Extended attention to computer vision.\n",
        "- Introduced spatial attention over image regions.\n",
        "- Allowed dynamic focus on image patches during caption generation.\n",
        "\n",
        "---\n",
        "\n",
        "## 2016 — Self-Attention (Intra-Attention)\n",
        "\n",
        "**Authors:** Jianpeng Cheng et al.\n",
        "\n",
        "### Contribution\n",
        "\n",
        "- Introduced self-attention within RNN frameworks.\n",
        "- Modeled intra-sequence dependencies.\n",
        "- Each token attends to all other tokens.\n",
        "\n",
        "General form:\n",
        "\n",
        "$$\n",
        "Q = K = V = X\n",
        "$$\n",
        "\n",
        "Also introduced during this period:\n",
        "\n",
        "- Decomposable attention (Parikh et al.)\n",
        "- Structured self-attentive sentence embeddings\n",
        "\n",
        "---\n",
        "\n",
        "# 2017 — The Transformer Revolution\n",
        "\n",
        "## Scaled Dot-Product Self-Attention\n",
        "\n",
        "**Authors:** Ashish Vaswani et al.  \n",
        "**Paper:** *Attention Is All You Need*\n",
        "\n",
        "### Contribution\n",
        "\n",
        "- Eliminated recurrence and convolution.\n",
        "- Introduced scaled dot-product attention:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q,K,V) =\n",
        "\\text{softmax}\\left(\n",
        "\\frac{QK^\\top}{\\sqrt{d_k}}\n",
        "\\right)V\n",
        "$$\n",
        "\n",
        "- Introduced multi-head attention:\n",
        "\n",
        "$$\n",
        "\\text{MultiHead}(Q,K,V) =\n",
        "\\text{Concat}(head_1,\\dots,head_h)W^O\n",
        "$$\n",
        "\n",
        "- Introduced positional encoding.\n",
        "\n",
        "### Impact\n",
        "\n",
        "Replaced RNN-based sequence modeling.  \n",
        "Enabled massive parallelization and large-scale training.\n",
        "\n",
        "---\n",
        "\n",
        "## 2017 — Relation Networks\n",
        "\n",
        "**Authors:** Santoro et al.\n",
        "\n",
        "- Applied attention-like reasoning to relational inference tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## 2017 — Set Transformers\n",
        "\n",
        "**Authors:** Lee et al.\n",
        "\n",
        "- Applied attention to unordered sets.\n",
        "- Formalized permutation-equivariant architectures.\n",
        "\n",
        "---\n",
        "\n",
        "# Expansion Across Domains\n",
        "\n",
        "---\n",
        "\n",
        "## 2018 — Non-local Neural Networks\n",
        "\n",
        "**Authors:** Wang et al.\n",
        "\n",
        "Extended attention to spatial-temporal vision:\n",
        "\n",
        "$$\n",
        "y_i = \\frac{1}{C(x)} \\sum_j f(x_i, x_j) g(x_j)\n",
        "$$\n",
        "\n",
        "Captured long-range dependencies in video and image tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## 2018 — Graph Attention Networks (GAT)\n",
        "\n",
        "**Authors:** Veličković et al.\n",
        "\n",
        "Applied attention to graph data:\n",
        "\n",
        "$$\n",
        "\\alpha_{ij} =\n",
        "\\frac{\n",
        "\\exp(\\text{LeakyReLU}(a^\\top [Wh_i \\Vert Wh_j]))\n",
        "}{\n",
        "\\sum_k \\exp(\\text{LeakyReLU}(a^\\top [Wh_i \\Vert Wh_k]))\n",
        "}\n",
        "$$\n",
        "\n",
        "Enabled adaptive neighbor aggregation.\n",
        "\n",
        "---\n",
        "\n",
        "## 2018 — BERT\n",
        "\n",
        "**Authors:** Devlin et al.\n",
        "\n",
        "- Encoder-only transformer.\n",
        "- Deep bidirectional self-attention.\n",
        "- Masked language modeling objective:\n",
        "\n",
        "$$\n",
        "\\max_\\theta \\sum \\log P_\\theta(x_i \\mid X_{\\setminus i})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 2018+ — GPT Series\n",
        "\n",
        "**Authors:** Radford et al.\n",
        "\n",
        "- Decoder-only transformer.\n",
        "- Autoregressive masked self-attention:\n",
        "\n",
        "$$\n",
        "P(x_1,\\dots,x_n)\n",
        "=\n",
        "\\prod_{t=1}^n P(x_t \\mid x_{<t})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "# Scalability & Efficiency Era (2019–2020)\n",
        "\n",
        "As sequence length increased, quadratic complexity:\n",
        "\n",
        "$$\n",
        "O(n^2)\n",
        "$$\n",
        "\n",
        "became a bottleneck.\n",
        "\n",
        "## Efficient Transformer Variants\n",
        "\n",
        "| Model     | Core Idea                          |\n",
        "|------------|------------------------------------|\n",
        "| Reformer   | Locality-sensitive hashing         |\n",
        "| Linformer  | Low-rank projection of attention   |\n",
        "| Performer  | Kernelized linear attention        |\n",
        "\n",
        "Linear attention approximates:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q,K,V)\n",
        "\\approx\n",
        "\\phi(Q)(\\phi(K)^\\top V)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Hopfield Networks Reinterpreted (2019+)\n",
        "\n",
        "**Authors:** Ramsauer et al.\n",
        "\n",
        "Showed modern Hopfield networks are mathematically equivalent to attention:\n",
        "\n",
        "$$\n",
        "\\text{softmax}(QK^\\top)V\n",
        "$$\n",
        "\n",
        "connected to associative memory energy minimization.\n",
        "\n",
        "---\n",
        "\n",
        "## 2020 — Vision Transformers (ViT)\n",
        "\n",
        "**Authors:** Dosovitskiy et al.\n",
        "\n",
        "- Applied pure self-attention to image patches.\n",
        "- Removed convolutional inductive bias.\n",
        "\n",
        "Patch embedding:\n",
        "\n",
        "$$\n",
        "z_0 = [x_{class}; x_p^1E; \\dots; x_p^NE]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "# Scientific and Multimodal Expansion\n",
        "\n",
        "Attention became a general interaction operator:\n",
        "\n",
        "- Protein folding (AlphaFold)\n",
        "- Vision-language models (CLIP)\n",
        "- Dense segmentation models\n",
        "\n",
        "---\n",
        "\n",
        "# Optimization Advances\n",
        "\n",
        "## Flash Attention\n",
        "\n",
        "- Memory-efficient exact attention.\n",
        "- Reduces GPU memory via tiling and recomputation.\n",
        "\n",
        "## Flexible Attention Mechanisms\n",
        "\n",
        "- Adaptive score modification.\n",
        "- Dynamic attention computation.\n",
        "\n",
        "---\n",
        "\n",
        "# Conceptual Evolution Summary\n",
        "\n",
        "| Era         | Focus                                   |\n",
        "|-------------|------------------------------------------|\n",
        "| 1950s–1990s | Cognitive and fast-weight foundations    |\n",
        "| 2014–2016   | Attention inside recurrent models        |\n",
        "| 2017        | Self-attention and Transformer           |\n",
        "| 2018        | Cross-domain generalization              |\n",
        "| 2019–2020   | Efficient and scalable attention         |\n",
        "| 2020+       | Multimodal and scientific dominance      |\n",
        "\n",
        "---\n",
        "\n",
        "# The Core Turning Point\n",
        "\n",
        "The decisive structural transition:\n",
        "\n",
        "$$\n",
        "\\text{From recurrence} \\;\\longrightarrow\\; \\text{Global self-attention}\n",
        "$$\n",
        "\n",
        "This shift enabled:\n",
        "\n",
        "- Full parallelization  \n",
        "- Long-range dependency modeling  \n",
        "- Scaling to billions of parameters  \n",
        "- Emergence of modern large language models  \n"
      ],
      "metadata": {
        "id": "EANpcSGCkLje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chronological Evolution of the Attention Mechanism\n",
        "\n",
        "---\n",
        "\n",
        "## 1. 2014 — Additive Attention (First Neural Attention)\n",
        "\n",
        "**Authors:** Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio  \n",
        "**Paper:** *Neural Machine Translation by Jointly Learning to Align and Translate* (2014)\n",
        "\n",
        "### Contribution\n",
        "\n",
        "- Introduced the first neural attention mechanism.\n",
        "- Designed for Seq2Seq RNN-based machine translation.\n",
        "- Solved the fixed-length context vector bottleneck in encoder–decoder LSTMs.\n",
        "- Introduced **Additive Attention**.\n",
        "\n",
        "Alignment score computed via a feedforward neural network:\n",
        "\n",
        "$$\n",
        "e_{t,i} = v_a^\\top \\tanh(W_a h_{t-1} + U_a s_i)\n",
        "$$\n",
        "\n",
        "Attention weights:\n",
        "\n",
        "$$\n",
        "\\alpha_{t,i} = \\text{softmax}(e_{t,i})\n",
        "$$\n",
        "\n",
        "Context vector:\n",
        "\n",
        "$$\n",
        "c_t = \\sum_i \\alpha_{t,i} s_i\n",
        "$$\n",
        "\n",
        "### Historical Significance\n",
        "\n",
        "This marked the birth of neural attention. The decoder could attend to all encoder hidden states rather than relying only on the final hidden state.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. 2015 — Multiplicative / Dot-Product Attention\n",
        "\n",
        "**Authors:** Minh-Thang Luong, Hieu Pham, Christopher D. Manning  \n",
        "**Paper:** *Effective Approaches to Attention-based Neural Machine Translation* (2015)\n",
        "\n",
        "### Contribution\n",
        "\n",
        "- Introduced dot-product (multiplicative) attention.\n",
        "- Replaced additive scoring with vector dot products.\n",
        "- Improved computational efficiency via matrix multiplication.\n",
        "- Required query and key vectors to share dimensionality.\n",
        "\n",
        "Score function:\n",
        "\n",
        "$$\n",
        "e_{t,i} = h_t^\\top s_i\n",
        "$$\n",
        "\n",
        "Context vector:\n",
        "\n",
        "$$\n",
        "c_t = \\sum_i \\alpha_{t,i} s_i\n",
        "$$\n",
        "\n",
        "### Historical Significance\n",
        "\n",
        "Improved efficiency and scalability, paving the way for large-scale attention models.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. 2016 — Self-Attention (Intra-Attention)\n",
        "\n",
        "**Authors:** Jianpeng Cheng et al.  \n",
        "**Paper:** *Long Short-Term Memory-Networks for Machine Reading* (2016)\n",
        "\n",
        "### Contribution\n",
        "\n",
        "- Introduced self-attention (intra-attention).\n",
        "- Queries, keys, and values derived from the same sequence.\n",
        "- Modeled intra-sequence relationships.\n",
        "- Enabled reasoning across tokens within a sentence.\n",
        "\n",
        "General form:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V)\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "Q = K = V = X\n",
        "$$\n",
        "\n",
        "### Historical Significance\n",
        "\n",
        "Shifted attention from cross-sequence alignment (translation) to general language understanding.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. 2017 — Scaled Dot-Product Attention & Transformer\n",
        "\n",
        "**Authors:** Ashish Vaswani et al.  \n",
        "**Paper:** *Attention Is All You Need* (2017)\n",
        "\n",
        "### Major Innovations\n",
        "\n",
        "- Introduced the Transformer architecture.\n",
        "- Removed recurrence and convolution.\n",
        "- Formalized scaled dot-product attention:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) =\n",
        "\\text{softmax}\\left(\n",
        "\\frac{QK^\\top}{\\sqrt{d_k}}\n",
        "\\right)V\n",
        "$$\n",
        "\n",
        "- Introduced multi-head attention:\n",
        "\n",
        "$$\n",
        "\\text{MultiHead}(Q,K,V) =\n",
        "\\text{Concat}(head_1, \\dots, head_h)W^O\n",
        "$$\n",
        "\n",
        "- Added positional encoding:\n",
        "\n",
        "$$\n",
        "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)\n",
        "$$\n",
        "\n",
        "### Historical Significance\n",
        "\n",
        "- Rendered RNNs unnecessary for NLP.\n",
        "- Enabled massive parallelization.\n",
        "- Became the foundation of modern large language models.\n",
        "\n",
        "This was the architectural revolution.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. 2018 — Encoder-Only Transformers (BERT)\n",
        "\n",
        "**Authors:** Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova  \n",
        "**Paper:** *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding* (2018)\n",
        "\n",
        "### Contribution\n",
        "\n",
        "- Encoder-only transformer.\n",
        "- Deep bidirectional self-attention.\n",
        "- Pretraining via masked language modeling.\n",
        "\n",
        "Objective:\n",
        "\n",
        "$$\n",
        "\\max_\\theta \\sum \\log P_\\theta(x_i \\mid X_{\\setminus i})\n",
        "$$\n",
        "\n",
        "### Historical Significance\n",
        "\n",
        "Demonstrated that attention-based encoders dominate language understanding benchmarks.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. 2018 — Decoder-Only Transformers (GPT)\n",
        "\n",
        "**Authors:** Alec Radford et al.  \n",
        "**Paper:** *Improving Language Understanding by Generative Pre-Training* (2018)\n",
        "\n",
        "### Contribution\n",
        "\n",
        "- Decoder-only autoregressive transformer.\n",
        "- Self-attention for next-token prediction.\n",
        "\n",
        "Autoregressive objective:\n",
        "\n",
        "$$\n",
        "P(x_1, \\dots, x_n) =\n",
        "\\prod_{t=1}^{n} P(x_t \\mid x_{<t})\n",
        "$$\n",
        "\n",
        "### Historical Significance\n",
        "\n",
        "Laid the foundation for modern generative large language models.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. 2020s — Attention in Vision & Generative Models\n",
        "\n",
        "### Vision Transformers (ViT)\n",
        "\n",
        "**Authors:** Alexey Dosovitskiy et al.  \n",
        "**Paper:** *An Image is Worth 16×16 Words* (2020)\n",
        "\n",
        "### Contribution\n",
        "\n",
        "- Applied transformer self-attention to image patches.\n",
        "- Replaced CNN inductive biases with global attention.\n",
        "\n",
        "Patch embedding:\n",
        "\n",
        "$$\n",
        "z_0 = [x_{class}; x_p^1E; \\dots; x_p^NE]\n",
        "$$\n",
        "\n",
        "### Diffusion Models with Attention\n",
        "\n",
        "**Authors:** Jonathan Ho et al.  \n",
        "**Paper:** *Denoising Diffusion Probabilistic Models* (2020)\n",
        "\n",
        "### Contribution\n",
        "\n",
        "- Integrated attention layers inside U-Nets.\n",
        "- Enabled high-fidelity generative modeling.\n",
        "\n",
        "Reverse diffusion step:\n",
        "\n",
        "$$\n",
        "p_\\theta(x_{t-1} \\mid x_t)\n",
        "$$\n",
        "\n",
        "Attention modules improved long-range coherence in generated images.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Advanced Attention Variants (Late Evolution)\n",
        "\n",
        "### Multi-Query Attention\n",
        "\n",
        "- Shares keys and values across heads.\n",
        "- Reduces memory complexity.\n",
        "\n",
        "### Grouped Query Attention\n",
        "\n",
        "- Intermediate solution between full multi-head and multi-query.\n",
        "\n",
        "### Rotary Positional Encoding (RoPE)\n",
        "\n",
        "Rotary embedding transformation:\n",
        "\n",
        "$$\n",
        "\\tilde{q}_i = R_\\theta q_i\n",
        "$$\n",
        "\n",
        "Improves long-context extrapolation.\n",
        "\n",
        "---\n",
        "\n",
        "# Condensed Timeline\n",
        "\n",
        "| Year | Authors | Paper | Innovation |\n",
        "|------|----------|--------|------------|\n",
        "| 2014 | Bahdanau et al. | Neural Machine Translation | Additive attention |\n",
        "| 2015 | Luong et al. | Effective Approaches to NMT | Dot-product attention |\n",
        "| 2016 | Cheng et al. | LSTMN / Intra-attention | Self-attention |\n",
        "| 2017 | Vaswani et al. | Attention Is All You Need | Transformer + scaled attention |\n",
        "| 2018 | Devlin et al. | BERT | Encoder-only transformer |\n",
        "| 2018 | Radford et al. | GPT | Decoder-only transformer |\n",
        "| 2020 | Dosovitskiy et al. | Vision Transformer | Attention in vision |\n",
        "| 2020 | Ho et al. | DDPM | Attention in diffusion |\n",
        "\n",
        "---\n",
        "\n",
        "# Conceptual Evolution Arc\n",
        "\n",
        "1. Attention as alignment for translation (2014)  \n",
        "2. Computational efficiency improvements (2015)  \n",
        "3. Self-referential sequence modeling (2016)  \n",
        "4. Attention-only architecture (2017)  \n",
        "5. Large-scale language modeling (2018+)  \n",
        "6. Multimodal and generative dominance (2020+)  \n",
        "\n",
        "Attention evolved from a translation alignment tool into the central computational primitive of modern artificial intelligence.\n"
      ],
      "metadata": {
        "id": "lqjW5ATVkVlX"
      }
    }
  ]
}