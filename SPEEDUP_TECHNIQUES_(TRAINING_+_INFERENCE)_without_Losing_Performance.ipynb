{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MASTER LIST — SPEEDUP TECHNIQUES (TRAINING + INFERENCE)\n",
        "\n",
        "Organized into **8 Mega-Categories** + **59 Subcategories**.\n",
        "\n",
        "---\n",
        "\n",
        "# 1) Architectural Optimization\n",
        "\n",
        "## 1.1 Smaller or Efficient Architectures\n",
        "- MobileNet (Depthwise Separable Convolutions)  \n",
        "- ShuffleNet (Channel Shuffle)  \n",
        "- SqueezeNet (Fire modules)  \n",
        "- EfficientNet (Compound scaling)  \n",
        "- ConvNeXt / RepVGG (reparameterization)  \n",
        "- MLP-Mixer / gMLP  \n",
        "- Linformer / Nyströmformer / Performer (linear attention)\n",
        "\n",
        "## 1.2 Token / Input Reduction\n",
        "- Patch merging (Vision Transformers)  \n",
        "- Token pruning / early exiting  \n",
        "- Text compression (Byte-level, SentencePiece)  \n",
        "- Sliding-window attention (Longformer, BigBird)  \n",
        "- Temporal downsampling (audio, video)\n",
        "\n",
        "## 1.3 Low-Rank Structure\n",
        "- LoRA-style low-rank decomposition  \n",
        "- Tensor-train decomposition  \n",
        "- Kronecker factorization  \n",
        "- SVD on weight matrices\n",
        "\n",
        "## 1.4 Parameter Sharing\n",
        "- ALBERT-style parameter sharing  \n",
        "- Recurrent MLP blocks  \n",
        "- Weight tying in embedding/output layers\n",
        "\n",
        "---\n",
        "\n",
        "# 2) Training Optimization\n",
        "\n",
        "## 2.1 Mixed Precision Training\n",
        "- FP16 / bfloat16  \n",
        "- FP8 training (NVIDIA Hopper)  \n",
        "- Dynamic loss scaling\n",
        "\n",
        "## 2.2 Gradient Optimization\n",
        "- Gradient accumulation  \n",
        "- Gradient checkpointing  \n",
        "- Selective activation recomputation  \n",
        "- Adaptive optimizers (AdamW, Adafactor)\n",
        "\n",
        "## 2.3 Batch & Data Pipeline Optimization\n",
        "- Larger batch sizes with LARS / LAMB  \n",
        "- Prefetching  \n",
        "- Fused dataloaders  \n",
        "- Asynchronous augmentation\n",
        "\n",
        "## 2.4 Distributed Training\n",
        "- Data Parallelism  \n",
        "- Model Parallelism  \n",
        "- Pipeline Parallelism  \n",
        "- ZeRO (Stage 1–3)  \n",
        "- Fully Sharded Data Parallel (FSDP)  \n",
        "- Tensor Parallelism  \n",
        "- Mixture-of-Experts parallelism\n",
        "\n",
        "## 2.5 Curriculum Learning\n",
        "- Progressive task difficulty  \n",
        "- Progressive layer training  \n",
        "- Freeze-unfreeze schedules\n",
        "\n",
        "---\n",
        "\n",
        "# 3) Inference Optimization\n",
        "\n",
        "## 3.1 Quantization  \n",
        "(These preserve performance if calibrated well.)\n",
        "\n",
        "- Post-training quantization (PTQ)  \n",
        "- Quantization-aware training (QAT)  \n",
        "- 8-bit / 4-bit / 2-bit weights  \n",
        "- SmoothQuant  \n",
        "- GPTQ / AWQ / AQLM  \n",
        "- FP8 inference\n",
        "\n",
        "## 3.2 Pruning  \n",
        "(Structured pruning has minimal performance drop.)\n",
        "\n",
        "- Block pruning  \n",
        "- Head pruning (Transformers)  \n",
        "- Neuron / channel pruning  \n",
        "- Magnitude pruning  \n",
        "- Movement pruning\n",
        "\n",
        "## 3.3 Distillation\n",
        "- Knowledge distillation (teacher → student)  \n",
        "- TinyBERT / DistilBERT  \n",
        "- Layer-wise distillation  \n",
        "- Logit matching + feature distillation\n",
        "\n",
        "## 3.4 Caching & Reuse\n",
        "- KV cache for autoregressive transformers  \n",
        "- Attention cache  \n",
        "- Prefix decoding  \n",
        "- Recurrent memory tokens\n",
        "\n",
        "---\n",
        "\n",
        "# 4) Algorithmic Improvements\n",
        "\n",
        "## 4.1 Efficient Attention\n",
        "- FlashAttention (1/2)  \n",
        "- Memory-efficient attention  \n",
        "- Sparse attention  \n",
        "- Linear attention (Performer, ETC)  \n",
        "- Kernel-based attention  \n",
        "- Reformer LSH attention\n",
        "\n",
        "## 4.2 Inference-Time Algorithms\n",
        "- Speculative decoding  \n",
        "- Medusa decoding  \n",
        "- Multi-step parallel decoding  \n",
        "- Lookahead decoding  \n",
        "- Tree-based decoding\n",
        "\n",
        "## 4.3 Training-Time Algorithms\n",
        "- SAM optimizer (flat minima allow smaller models)  \n",
        "- Sharpness-aware reparameterization  \n",
        "- Label smoothing\n",
        "\n",
        "---\n",
        "\n",
        "# 5) Hardware Optimization\n",
        "\n",
        "## 5.1 GPU-Level Optimizations\n",
        "- Tensor Cores  \n",
        "- CUDA Graphs  \n",
        "- Kernel fusion  \n",
        "- Operator fusion (FlashAttention, Flash-Decoding)  \n",
        "- cuDNN / cuBLAS optimized kernels\n",
        "\n",
        "## 5.2 Multi-GPU & Cluster\n",
        "- NCCL optimizations  \n",
        "- Efficient all-reduce  \n",
        "- Interconnect: NVLink, InfiniBand  \n",
        "- Sharded state (ZeRO, FSDP)\n",
        "\n",
        "## 5.3 Compilation\n",
        "- TensorRT  \n",
        "- ONNX Runtime  \n",
        "- XLA Compilation  \n",
        "- JAX jit  \n",
        "- PyTorch 2.0 Inductor compiler\n",
        "\n",
        "---\n",
        "\n",
        "# 6) Numerical & Mathematical Tricks\n",
        "\n",
        "## 6.1 Normalization Improvements\n",
        "- RMSNorm  \n",
        "- LayerNorm-free designs  \n",
        "- Pre-norm Transformer  \n",
        "- ScaleNorm  \n",
        "- Cosine normalization\n",
        "\n",
        "## 6.2 Activation Function Improvements\n",
        "- GELU vs ReLU  \n",
        "- SwiGLU  \n",
        "- SiLU  \n",
        "- ReLU6 / hard-swish (edge devices)\n",
        "\n",
        "## 6.3 Initialization & Scaling\n",
        "- Xavier / Kaiming initialization  \n",
        "- μ-parameterization  \n",
        "- QK normalization (Transformers)\n",
        "\n",
        "---\n",
        "\n",
        "# 7) Data-Centric Speedups\n",
        "\n",
        "## 7.1 Better Data, Less Compute\n",
        "- Data deduplication  \n",
        "- Data filtering  \n",
        "- High-quality pretraining corpora  \n",
        "- Synthetic data bootstrapping\n",
        "\n",
        "## 7.2 Faster Data Sampling\n",
        "- Token packing  \n",
        "- Efficient shuffling  \n",
        "- Weighted sampling for curriculum\n",
        "\n",
        "## 7.3 Self-Supervised Efficiency\n",
        "- SimCLR → BYOL → DINO → iBOT  \n",
        "- Teacher-free self-distillation\n",
        "\n",
        "---\n",
        "\n",
        "# 8) Model Compression (Without Hurting Performance)\n",
        "\n",
        "## 8.1 Reparameterization\n",
        "- RePruning + re-training  \n",
        "- Structural re-parameterization (RepVGG)  \n",
        "- Weight clustering\n",
        "\n",
        "## 8.2 Tensor Compression\n",
        "- Weight sharing  \n",
        "- Huffman coding  \n",
        "- Matrix factorization\n",
        "\n",
        "## 8.3 KV-Cache Compression\n",
        "- Sublayer KV merging  \n",
        "- KV token dropping  \n",
        "- Dynamic KV eviction\n",
        "\n",
        "---\n",
        "\n",
        "# BONUS: Transformer-Specific Modern Techniques\n",
        "\n",
        "## 9.1 Scaling Laws for Efficient Training\n",
        "- Chinchilla scaling (compute-optimal)  \n",
        "- Data-optimal training  \n",
        "- Small model + more data > large model + less data  \n",
        "\n",
        "## 9.2 Efficient Long-Context\n",
        "- FlashAttention-2 + paged attention  \n",
        "- Streaming attention  \n",
        "- RingAttention  \n",
        "- H2O attention  \n",
        "\n",
        "## 9.3 Efficient Decoding\n",
        "- Grouped-query attention (GQA)  \n",
        "- Multi-query attention (MQA)  \n",
        "- Speculative sampling  \n",
        "- Recurrent memory (RWKV, Mamba)\n",
        "\n",
        "---\n",
        "\n",
        "# FINAL EXTREME SPEEDUPS (No Performance Loss)\n",
        "\n",
        "These provide **5×–20×** improvements:\n",
        "\n",
        "- FP8 Training + FlashAttention2  \n",
        "- GQA + MQA  \n",
        "- Speculative Decoding  \n",
        "- TensorRT / Inductor compilation  \n",
        "- Quantization-aware training  \n",
        "- Distillation + low-rank adapters  \n",
        "- Chinchilla scaling  \n",
        "- Data filtering + token packing  \n",
        "- Modular MoE routing (top-1 gating)\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "2ZS4zXqwk6lt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some Papers that use that techniques\n",
        "\n",
        "Below is the **same content**, **every word preserved**, but **all links/arXiv/website lines removed** and **only the paper names kept**.\n",
        "\n",
        "---\n",
        "\n",
        "# 1. Knowledge Distillation (KD) & Teacher–Student Compression\n",
        "\n",
        "**Core idea:** Train a smaller “student” model to mimic a larger “teacher” using soft targets → keeps accuracy, big speed/inference gains.\n",
        "\n",
        "## Flagship papers & authors\n",
        "\n",
        "**Distilling the Knowledge in a Neural Network – Hinton, Vinyals, Dean (2015)**  \n",
        "The classic KD paper. Defines temperature scaling, soft targets, and shows a large ensemble distilled into a single smaller model without losing performance.\n",
        "\n",
        "**Knowledge Distillation Surveys – Gou et al., 2021; Mansourian et al., 2024**  \n",
        "Comprehensive surveys on KD variants (logit-based, feature-based, relation-based, self-distillation, multi-teacher, etc.).\n",
        "\n",
        "**TinyBERT, MobileBERT, DistilBERT**\n",
        "\n",
        "- TinyBERT – Jiao et al. (2019)  \n",
        "- MobileBERT – Sun et al. (2020)  \n",
        "- DistilBERT – Sanh et al. (2019)\n",
        "\n",
        "All are “teacher–student” distillations of BERT designed for faster inference with minimal accuracy loss; they appear prominently in recent KD surveys.\n",
        "\n",
        "**What to take away:** Hinton’s KD paper is the conceptual root; almost every modern small transformer / mobile model is some KD variant.\n",
        "\n",
        "---\n",
        "\n",
        "# 2. Pruning & Structured Sparsity\n",
        "\n",
        "**Core idea:** Delete weights, channels, or blocks that don’t matter → fewer FLOPs, smaller memory footprint, similar accuracy.\n",
        "\n",
        "## Key references\n",
        "\n",
        "**Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding – Song Han, Huizi Mao, Bill Dally (2016)**  \n",
        "Pipeline: magnitude pruning → weight quantization → Huffman coding. Compresses AlexNet from 233MB to 8.9MB with no accuracy loss.\n",
        "\n",
        "Han’s follow-up pruning pipeline papers extend this prune-then-quantize approach.\n",
        "\n",
        "**Modern sparse training / Lottery Ticket Hypothesis** also fit here, but Deep Compression is the canonical starting point.\n",
        "\n",
        "---\n",
        "\n",
        "# 3. Quantization & Low-Precision Inference (especially for LLMs)\n",
        "\n",
        "**Core idea:** Represent weights (and sometimes activations) in INT8 / INT4 / INT2, etc. → 2–4× speed & memory savings, with minimal accuracy loss.\n",
        "\n",
        "## Flagship LLM quantization papers\n",
        "\n",
        "**SmoothQuant – Xiao, Lin, Seznec, Wu, Demouth, Song Han (2022)**  \n",
        "“SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models.”\n",
        "\n",
        "**GPTQ – Frantar et al. (2023)**  \n",
        "“GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers.”\n",
        "\n",
        "**AWQ – Ji Lin et al. (2024)**  \n",
        "“AWQ: Activation-aware Weight Quantization for LLM Compression / On-Device LLM.”\n",
        "\n",
        "**FBGEMM – Khudia et al. (2021)**  \n",
        "“FBGEMM: Enabling High-Performance Low-Precision Deep Learning Inference.”\n",
        "\n",
        "---\n",
        "\n",
        "# 4. Efficient CNN / Vision Architectures (lightweight by design)\n",
        "\n",
        "**Core idea:** Build from efficient blocks (depthwise convolutions, bottlenecks) to get SOTA accuracy at low FLOPs.\n",
        "\n",
        "## Canonical works\n",
        "\n",
        "**MobileNet – Howard et al. (2017)**  \n",
        "“MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.”\n",
        "\n",
        "**EfficientNet – Tan & Quoc Le (2019)**  \n",
        "“EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.”\n",
        "\n",
        "MobileNetV2, MobileNetV3, ShuffleNet, GhostNet also belong here.\n",
        "\n",
        "---\n",
        "\n",
        "# 5. Efficient Transformer Architectures & Attention\n",
        "\n",
        "**Core idea:** Make attention cheaper (less memory, faster) without large accuracy drops.\n",
        "\n",
        "## Important papers\n",
        "\n",
        "**Reformer – Kitaev, Kaiser, Levskaya (2020)**  \n",
        "“Reformer: The Efficient Transformer.”\n",
        "\n",
        "**Performer – Choromanski et al. (2021)**  \n",
        "“Rethinking Attention with Performers.”\n",
        "\n",
        "**FlashAttention – Tri Dao et al. (2022)**  \n",
        "“FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.”\n",
        "\n",
        "**Switch Transformer – Fedus, Zoph, Shazeer (2022)**  \n",
        "“Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.”\n",
        "\n",
        "---\n",
        "\n",
        "# 6. Faster Decoding / Inference Algorithms (for LLMs)\n",
        "\n",
        "**Core idea:** Change the decoding algorithm, not the model → fewer serial forward passes.\n",
        "\n",
        "**Speculative Decoding – Yaniv Leviathan et al. (2023)**  \n",
        "“Fast Inference from Transformers via Speculative Decoding.”\n",
        "\n",
        "**KV-cache / paged attention**  \n",
        "Not one canonical paper, but many systems works; core idea is caching and memory-efficient management of attention states.\n",
        "\n",
        "---\n",
        "\n",
        "# 7. Parameter-Efficient Fine-Tuning (PEFT)\n",
        "\n",
        "**Core idea:** Freeze the large backbone, add a small number of tunable parameters → faster training, minimal inference overhead.\n",
        "\n",
        "**LoRA – Edward J. Hu et al. (2021)**  \n",
        "“LoRA: Low-Rank Adaptation of Large Language Models.”\n",
        "\n",
        "---\n",
        "\n",
        "# 8. Compute-Optimal Training & Scaling Laws\n",
        "\n",
        "**Core idea:** Choose optimal model size vs. number of training tokens → same or better performance at lower compute.\n",
        "\n",
        "**Chinchilla – Hoffmann et al. (2022)**  \n",
        "“Training Compute-Optimal Large Language Models.”\n",
        "\n",
        "Shows that earlier LLMs were under-trained; Chinchilla (70B, 1.3T tokens) outperforms much larger models while being cheaper.\n",
        "\n",
        "---\n",
        "\n",
        "# 9. Distributed Training & Memory Optimizations\n",
        "\n",
        "**Core idea:** Shard parameters, gradients, optimizer states; overlap communication and compute.\n",
        "\n",
        "**ZeRO – Rajbhandari et al. (2020)**  \n",
        "“ZeRO: Memory Optimizations Toward Training Trillion Parameter Models.”\n",
        "\n",
        "**FSDP – Fully Sharded Data Parallel**  \n",
        "“Fully Sharded Data Parallel: faster AI training with fewer GPUs.”\n",
        "\n",
        "**PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel – Zhao et al. (2023)**  \n",
        "Describes production-grade FSDP implementation.\n",
        "\n",
        "---\n",
        "\n",
        "# 10. Hardware-Aware Kernels & Libraries\n",
        "\n",
        "**Core idea:** Optimize low-level kernels (matmul, convolutions, quantized ops) for massive speed gains with same math.\n",
        "\n",
        "**FBGEMM – Khudia et al. (2021)**  \n",
        "“FBGEMM: Enabling High-Performance Low-Precision Deep Learning Inference.”\n",
        "\n",
        "FlashAttention (above) is this idea applied to attention.\n",
        "\n",
        "---\n",
        "\n",
        "# 11. Classic but Still Important Techniques\n",
        "\n",
        "- Mixed-precision training (FP16/BF16) + loss scaling  \n",
        "- Gradient checkpointing  \n",
        "- Reversible layers (used in Reformer)\n",
        "\n",
        "These are widely used to speed training and reduce memory.\n",
        "\n",
        "---\n",
        "\n",
        "# 12. Where Hinton Fits in This Landscape\n",
        "\n",
        "Hinton contributes most directly via:\n",
        "\n",
        "- **Distilling the Knowledge in a Neural Network** – foundation of KD  \n",
        "- Earlier foundational work on efficient representations and autoencoders\n",
        "\n",
        "KD is the root of most modern model compression.\n",
        "\n",
        "---\n",
        "\n",
        "# How to Use This as a Research Roadmap\n",
        "\n",
        "## Start with the surveys\n",
        "- Knowledge Distillation Surveys – Gou et al. (2021)  \n",
        "- Knowledge Distillation Survey – Mansourian et al. (2024)  \n",
        "- SmoothQuant  \n",
        "- GPTQ  \n",
        "- AWQ\n",
        "\n",
        "## Then go technique by technique\n",
        "Read the 1–3 canonical papers in each family (KD, pruning, quantization, efficient CNNs, efficient attention, speculative decoding, PEFT, ZeRO/FSDP, etc.).\n",
        "\n",
        "## Use citation graphs\n",
        "ConnectedPapers and Semantic Scholar help explore neighborhoods around:  \n",
        "EfficientNet, FlashAttention, LoRA, GPTQ, AWQ, Speculative Decoding.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "X6R3ZHbznyIE"
      }
    }
  ]
}