{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MASTER LIST — SPEEDUP TECHNIQUES (TRAINING + INFERENCE)\n",
        "\n",
        "Organized into **8 Mega-Categories** + **59 Subcategories**.\n",
        "\n",
        "---\n",
        "\n",
        "# 1) Architectural Optimization\n",
        "\n",
        "## 1.1 Smaller or Efficient Architectures\n",
        "- MobileNet (Depthwise Separable Convolutions)  \n",
        "- ShuffleNet (Channel Shuffle)  \n",
        "- SqueezeNet (Fire modules)  \n",
        "- EfficientNet (Compound scaling)  \n",
        "- ConvNeXt / RepVGG (reparameterization)  \n",
        "- MLP-Mixer / gMLP  \n",
        "- Linformer / Nyströmformer / Performer (linear attention)\n",
        "\n",
        "## 1.2 Token / Input Reduction\n",
        "- Patch merging (Vision Transformers)  \n",
        "- Token pruning / early exiting  \n",
        "- Text compression (Byte-level, SentencePiece)  \n",
        "- Sliding-window attention (Longformer, BigBird)  \n",
        "- Temporal downsampling (audio, video)\n",
        "\n",
        "## 1.3 Low-Rank Structure\n",
        "- LoRA-style low-rank decomposition  \n",
        "- Tensor-train decomposition  \n",
        "- Kronecker factorization  \n",
        "- SVD on weight matrices\n",
        "\n",
        "## 1.4 Parameter Sharing\n",
        "- ALBERT-style parameter sharing  \n",
        "- Recurrent MLP blocks  \n",
        "- Weight tying in embedding/output layers\n",
        "\n",
        "---\n",
        "\n",
        "# 2) Training Optimization\n",
        "\n",
        "## 2.1 Mixed Precision Training\n",
        "- FP16 / bfloat16  \n",
        "- FP8 training (NVIDIA Hopper)  \n",
        "- Dynamic loss scaling\n",
        "\n",
        "## 2.2 Gradient Optimization\n",
        "- Gradient accumulation  \n",
        "- Gradient checkpointing  \n",
        "- Selective activation recomputation  \n",
        "- Adaptive optimizers (AdamW, Adafactor)\n",
        "\n",
        "## 2.3 Batch & Data Pipeline Optimization\n",
        "- Larger batch sizes with LARS / LAMB  \n",
        "- Prefetching  \n",
        "- Fused dataloaders  \n",
        "- Asynchronous augmentation\n",
        "\n",
        "## 2.4 Distributed Training\n",
        "- Data Parallelism  \n",
        "- Model Parallelism  \n",
        "- Pipeline Parallelism  \n",
        "- ZeRO (Stage 1–3)  \n",
        "- Fully Sharded Data Parallel (FSDP)  \n",
        "- Tensor Parallelism  \n",
        "- Mixture-of-Experts parallelism\n",
        "\n",
        "## 2.5 Curriculum Learning\n",
        "- Progressive task difficulty  \n",
        "- Progressive layer training  \n",
        "- Freeze-unfreeze schedules\n",
        "\n",
        "---\n",
        "\n",
        "# 3) Inference Optimization\n",
        "\n",
        "## 3.1 Quantization  \n",
        "(These preserve performance if calibrated well.)\n",
        "\n",
        "- Post-training quantization (PTQ)  \n",
        "- Quantization-aware training (QAT)  \n",
        "- 8-bit / 4-bit / 2-bit weights  \n",
        "- SmoothQuant  \n",
        "- GPTQ / AWQ / AQLM  \n",
        "- FP8 inference\n",
        "\n",
        "## 3.2 Pruning  \n",
        "(Structured pruning has minimal performance drop.)\n",
        "\n",
        "- Block pruning  \n",
        "- Head pruning (Transformers)  \n",
        "- Neuron / channel pruning  \n",
        "- Magnitude pruning  \n",
        "- Movement pruning\n",
        "\n",
        "## 3.3 Distillation\n",
        "- Knowledge distillation (teacher → student)  \n",
        "- TinyBERT / DistilBERT  \n",
        "- Layer-wise distillation  \n",
        "- Logit matching + feature distillation\n",
        "\n",
        "## 3.4 Caching & Reuse\n",
        "- KV cache for autoregressive transformers  \n",
        "- Attention cache  \n",
        "- Prefix decoding  \n",
        "- Recurrent memory tokens\n",
        "\n",
        "---\n",
        "\n",
        "# 4) Algorithmic Improvements\n",
        "\n",
        "## 4.1 Efficient Attention\n",
        "- FlashAttention (1/2)  \n",
        "- Memory-efficient attention  \n",
        "- Sparse attention  \n",
        "- Linear attention (Performer, ETC)  \n",
        "- Kernel-based attention  \n",
        "- Reformer LSH attention\n",
        "\n",
        "## 4.2 Inference-Time Algorithms\n",
        "- Speculative decoding  \n",
        "- Medusa decoding  \n",
        "- Multi-step parallel decoding  \n",
        "- Lookahead decoding  \n",
        "- Tree-based decoding\n",
        "\n",
        "## 4.3 Training-Time Algorithms\n",
        "- SAM optimizer (flat minima allow smaller models)  \n",
        "- Sharpness-aware reparameterization  \n",
        "- Label smoothing\n",
        "\n",
        "---\n",
        "\n",
        "# 5) Hardware Optimization\n",
        "\n",
        "## 5.1 GPU-Level Optimizations\n",
        "- Tensor Cores  \n",
        "- CUDA Graphs  \n",
        "- Kernel fusion  \n",
        "- Operator fusion (FlashAttention, Flash-Decoding)  \n",
        "- cuDNN / cuBLAS optimized kernels\n",
        "\n",
        "## 5.2 Multi-GPU & Cluster\n",
        "- NCCL optimizations  \n",
        "- Efficient all-reduce  \n",
        "- Interconnect: NVLink, InfiniBand  \n",
        "- Sharded state (ZeRO, FSDP)\n",
        "\n",
        "## 5.3 Compilation\n",
        "- TensorRT  \n",
        "- ONNX Runtime  \n",
        "- XLA Compilation  \n",
        "- JAX jit  \n",
        "- PyTorch 2.0 Inductor compiler\n",
        "\n",
        "---\n",
        "\n",
        "# 6) Numerical & Mathematical Tricks\n",
        "\n",
        "## 6.1 Normalization Improvements\n",
        "- RMSNorm  \n",
        "- LayerNorm-free designs  \n",
        "- Pre-norm Transformer  \n",
        "- ScaleNorm  \n",
        "- Cosine normalization\n",
        "\n",
        "## 6.2 Activation Function Improvements\n",
        "- GELU vs ReLU  \n",
        "- SwiGLU  \n",
        "- SiLU  \n",
        "- ReLU6 / hard-swish (edge devices)\n",
        "\n",
        "## 6.3 Initialization & Scaling\n",
        "- Xavier / Kaiming initialization  \n",
        "- μ-parameterization  \n",
        "- QK normalization (Transformers)\n",
        "\n",
        "---\n",
        "\n",
        "# 7) Data-Centric Speedups\n",
        "\n",
        "## 7.1 Better Data, Less Compute\n",
        "- Data deduplication  \n",
        "- Data filtering  \n",
        "- High-quality pretraining corpora  \n",
        "- Synthetic data bootstrapping\n",
        "\n",
        "## 7.2 Faster Data Sampling\n",
        "- Token packing  \n",
        "- Efficient shuffling  \n",
        "- Weighted sampling for curriculum\n",
        "\n",
        "## 7.3 Self-Supervised Efficiency\n",
        "- SimCLR → BYOL → DINO → iBOT  \n",
        "- Teacher-free self-distillation\n",
        "\n",
        "---\n",
        "\n",
        "# 8) Model Compression (Without Hurting Performance)\n",
        "\n",
        "## 8.1 Reparameterization\n",
        "- RePruning + re-training  \n",
        "- Structural re-parameterization (RepVGG)  \n",
        "- Weight clustering\n",
        "\n",
        "## 8.2 Tensor Compression\n",
        "- Weight sharing  \n",
        "- Huffman coding  \n",
        "- Matrix factorization\n",
        "\n",
        "## 8.3 KV-Cache Compression\n",
        "- Sublayer KV merging  \n",
        "- KV token dropping  \n",
        "- Dynamic KV eviction\n",
        "\n",
        "---\n",
        "\n",
        "# BONUS: Transformer-Specific Modern Techniques\n",
        "\n",
        "## 9.1 Scaling Laws for Efficient Training\n",
        "- Chinchilla scaling (compute-optimal)  \n",
        "- Data-optimal training  \n",
        "- Small model + more data > large model + less data  \n",
        "\n",
        "## 9.2 Efficient Long-Context\n",
        "- FlashAttention-2 + paged attention  \n",
        "- Streaming attention  \n",
        "- RingAttention  \n",
        "- H2O attention  \n",
        "\n",
        "## 9.3 Efficient Decoding\n",
        "- Grouped-query attention (GQA)  \n",
        "- Multi-query attention (MQA)  \n",
        "- Speculative sampling  \n",
        "- Recurrent memory (RWKV, Mamba)\n",
        "\n",
        "---\n",
        "\n",
        "# FINAL EXTREME SPEEDUPS (No Performance Loss)\n",
        "\n",
        "These provide **5×–20×** improvements:\n",
        "\n",
        "- FP8 Training + FlashAttention2  \n",
        "- GQA + MQA  \n",
        "- Speculative Decoding  \n",
        "- TensorRT / Inductor compilation  \n",
        "- Quantization-aware training  \n",
        "- Distillation + low-rank adapters  \n",
        "- Chinchilla scaling  \n",
        "- Data filtering + token packing  \n",
        "- Modular MoE routing (top-1 gating)\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "2ZS4zXqwk6lt"
      }
    }
  ]
}