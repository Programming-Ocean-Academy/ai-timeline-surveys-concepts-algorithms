{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“– Chronological Evolution of NLP Subfields (1900â€“2025)\n",
        "\n",
        "---\n",
        "\n",
        "## 1. ğŸ§© Foundations of Computational Linguistics (1900â€“1960s)\n",
        "\n",
        "- 1900sâ€“1930s â€“ Mathematical linguistics (Saussureâ€™s structuralism, Zipfâ€™s law on word frequency).  \n",
        "- 1940s â€“ First work on machine translation (Warren Weaver, 1949 memo).  \n",
        "- 1950 â€“ Alan Turing, â€œComputing Machinery and Intelligenceâ€ â†’ Turing Test.  \n",
        "- 1952â€“1954 â€“ Georgetown-IBM experiment: automatic English â†’ Russian word-for-word MT.  \n",
        "- 1957 â€“ Chomskyâ€™s *Syntactic Structures* introduces transformational grammar.  \n",
        "- 1960s â€“ ELIZA (Weizenbaum, 1966): first chatbot (rule-based).  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. ğŸ“š Rule-Based & Symbolic NLP (1960sâ€“1980s)\n",
        "\n",
        "- 1960s â€“ SYSTRAN: rule-based MT system (used by NASA/EC).  \n",
        "- 1970 â€“ SHRDLU (Winograd): natural language understanding in constrained microworlds.  \n",
        "- 1970s â€“ Knowledge-based systems (frames, semantic networks).  \n",
        "- 1980s â€“ Expert systems & symbolic parsing (ATNs, DCGs).  \n",
        "\n",
        "Limited scalability â†’ motivated shift to statistical methods.  \n",
        "\n",
        "---\n",
        "\n",
        "## 3. ğŸ“Š Statistical NLP Era (1980sâ€“1990s)\n",
        "\n",
        "- 1983 â€“ IBM Candide Project: statistical MT using aligned bilingual corpora.  \n",
        "- 1988 â€“ Church & Mercer: Hidden Markov Models for speech recognition.  \n",
        "- 1990s â€“ SMT dominates (Brown et al., IBM Models 1â€“5).  \n",
        "- 1993 â€“ BLEU precursor metrics introduced.  \n",
        "- 1998 â€“ Maximum Entropy models (Berger, Della Pietra).  \n",
        "\n",
        "---\n",
        "\n",
        "## 4. ğŸ—£ï¸ Speech Recognition & Spoken NLP (1970sâ€“2000s)\n",
        "\n",
        "- 1970s â€“ HARPY (CMU): early speech recognition.  \n",
        "- 1980s â€“ HMMs become standard for ASR.  \n",
        "- 1990s â€“ Dragon Dictate: commercial speech-to-text.  \n",
        "- 2000s â€“ GMM-HMM hybrid models for large vocab speech.  \n",
        "- 2006+ â€“ Deep learning-based ASR (Hinton, 2012): DNN acoustic models.  \n",
        "\n",
        "---\n",
        "\n",
        "## 5. ğŸŒ Machine Translation (MT)\n",
        "\n",
        "- 1950sâ€“1970s â€“ Rule-based MT (SYSTRAN, Georgetown).  \n",
        "- 1990s â€“ Phrase-based SMT (Koehn, Och).  \n",
        "- 2013 â€“ RCTM (Kalchbrenner & Blunsom): convolution + RNN LM for MT.  \n",
        "- 2014 â€“ Seq2Seq (Sutskever, Vinyals, Le): RNN encoder-decoder.  \n",
        "- 2015 â€“ Bahdanau attention â†’ dynamic context vectors.  \n",
        "- 2017 â€“ Transformer (*Attention is All You Need*, Vaswani et al.).  \n",
        "- 2018 â€“ Ott et al. *Scaling NMT*: large-batch Transformer training.  \n",
        "- 2019â€“2025 â€“ GPT/ChatGPT-style systems unify translation into universal LLM frameworks.  \n",
        "\n",
        "---\n",
        "\n",
        "## 6. ğŸ“– Language Modeling & Representation Learning\n",
        "\n",
        "- 1980s â€“ N-gram language models.  \n",
        "- 2003 â€“ Bengio et al.: first neural LM.  \n",
        "- 2013 â€“ Word2Vec (Mikolov et al.): distributional word embeddings.  \n",
        "- 2014 â€“ GloVe (Pennington et al.).  \n",
        "- 2018 â€“ BERT (Devlin et al.): bidirectional transformers.  \n",
        "- 2019 â€“ XLNet, RoBERTa â†’ improved pretraining.  \n",
        "- 2020â€“2025 â€“ GPT-3/4/5, PaLM, LLaMA, Mixtral, DeepSeek models.  \n",
        "\n",
        "---\n",
        "\n",
        "## 7. ğŸ¤– Question Answering & Information Retrieval\n",
        "\n",
        "- 1960s â€“ BASEBALL QA system.  \n",
        "- 1970sâ€“80s â€“ MUC (Message Understanding Conferences).  \n",
        "- 2000s â€“ TREC QA tracks.  \n",
        "- 2012 â€“ IBM Watson (Jeopardy!) â†’ Deep QA.  \n",
        "- 2018 â€“ BERT â†’ SQuAD benchmark breakthroughs.  \n",
        "- 2020s â€“ Open-domain QA with GPT-3/ChatGPT.  \n",
        "\n",
        "---\n",
        "\n",
        "## 8. ğŸ“ Text Classification & Sentiment Analysis\n",
        "\n",
        "- 1990s â€“ Naive Bayes, SVMs for classification.  \n",
        "- 2002 â€“ Pang et al.: sentiment analysis benchmark.  \n",
        "- 2010s â€“ RNNs/CNNs dominate sentiment analysis.  \n",
        "- 2018 â€“ BERT fine-tuning â†’ near-human classification accuracy.  \n",
        "- 2020s â€“ Zero-shot classification with LLMs.  \n",
        "\n",
        "---\n",
        "\n",
        "## 9. âœ‚ï¸ Summarization\n",
        "\n",
        "- 1950sâ€“70s â€“ Early extractive approaches.  \n",
        "- 2000s â€“ Statistical summarization (LexRank, TextRank).  \n",
        "- 2016 â€“ Seq2Seq + attention for abstractive summarization.  \n",
        "- 2019 â€“ BERTSum, PEGASUS (Google).  \n",
        "- 2020s â€“ ChatGPT-based summarization with controllable styles.  \n",
        "\n",
        "---\n",
        "\n",
        "## 10. ğŸ—£ï¸ Dialogue Systems & Chatbots\n",
        "\n",
        "- 1966 â€“ ELIZA.  \n",
        "- 1972 â€“ PARRY (simulated paranoia patient).  \n",
        "- 2000s â€“ ALICE (AIML-based).  \n",
        "- 2015 â€“ Sequence-to-sequence conversational models.  \n",
        "- 2019â€“2020 â€“ Meena, BlenderBot.  \n",
        "- 2022â€“2025 â€“ ChatGPT, Gemini, Claude â†’ general-purpose conversational AI.  \n",
        "\n",
        "---\n",
        "\n",
        "## 11. ğŸ§  Semantics & Pragmatics\n",
        "\n",
        "- 1970s â€“ Montague grammar.  \n",
        "- 1990s â€“ WordNet lexical database.  \n",
        "- 2000s â€“ Latent Semantic Analysis (LSA), Latent Dirichlet Allocation (LDA).  \n",
        "- 2019 â€“ Sentence-BERT: semantic embeddings.  \n",
        "- 2020s â€“ Embedding-based evaluation (BERTScore, COMET).  \n",
        "\n",
        "---\n",
        "\n",
        "## 12. ğŸ“‘ Evaluation Metrics in NLP\n",
        "\n",
        "- 1993 â€“ BLEU introduced.  \n",
        "- 2000s â€“ ROUGE (summarization).  \n",
        "- 2010s â€“ METEOR, TER, ChrF.  \n",
        "- 2019â€“2020s â€“ BERTScore, BLEURT, COMET, MoverScore.  \n",
        "- 2025 â€“ Hybrid metrics combining automatic + human preference models.  \n",
        "\n",
        "---\n",
        "\n",
        "## 13. âš¡ Recent Cutting-Edge Subfields (2020â€“2025)\n",
        "\n",
        "- Multilingual LLMs (mBERT, XLM-R).  \n",
        "- Low-resource NLP with transfer + few-shot learning.  \n",
        "- Code LLMs (Codex, AlphaCode, StarCoder).  \n",
        "- Multimodal NLP (CLIP, Flamingo, GPT-4V, Gemini).  \n",
        "- Responsible NLP â†’ bias, fairness, toxicity mitigation.  \n",
        "- Efficiency research â†’ distillation, quantization, retrieval-augmented LMs (RAG).  \n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“ Conclusion\n",
        "From rule-based linguistics in the 1950s to transformer-based LLMs in the 2020s, NLP has evolved across distinct subfields: MT, LM, QA, IR, summarization, dialogue, semantics, evaluation, and multimodality. Each milestone reflects a shift in paradigm:\n",
        "\n",
        "**Symbolic â†’ Statistical â†’ Neural RNNs â†’ Attention â†’ Transformers â†’ LLMs.**"
      ],
      "metadata": {
        "id": "mjco0MKmCOHE"
      }
    }
  ]
}