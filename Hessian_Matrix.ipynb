{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![Ludwig_Otto_Hesse.jpg](data:image/jpeg;base64,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)\n",
        "# Ludwig Otto Hesse"
      ],
      "metadata": {
        "id": "jxApVxQhV9Gh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What the Hessian Matrix Is (Core Idea)\n",
        "\n",
        "The **Hessian matrix** is a square matrix of **second-order partial derivatives** of a scalar-valued function\n",
        "\n",
        "$$\n",
        "f:\\mathbb{R}^n \\to \\mathbb{R}.\n",
        "$$\n",
        "\n",
        "It captures how the **gradient changes** as you move in input/parameter space, i.e., the **local curvature** of the function.\n",
        "\n",
        "Formally, for $$x = (x_1,\\dots,x_n),$$ the Hessian is\n",
        "\n",
        "$$\n",
        "H_f(x) \\;=\\; \\nabla^2 f(x)\n",
        "\\;=\\;\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial^2 f}{\\partial x_1^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "If all second partial derivatives are continuous (i.e., $$f \\in C^2$$), then **mixed partials commute**, and the Hessian is **symmetric**:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} \\;=\\; \\frac{\\partial^2 f}{\\partial x_j \\partial x_i}\n",
        "\\quad\\Rightarrow\\quad\n",
        "H_f(x) = H_f(x)^\\top.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "# 2. Geometric and Analytical Meaning\n",
        "\n",
        "The Hessian describes the **second-order shape** of $$f$$ around a point:\n",
        "\n",
        "- **Curvature**: how quickly the slope changes  \n",
        "- **Bowl-like vs ridge-like** geometry  \n",
        "- **Directional steepness**: curvature depends on direction  \n",
        "\n",
        "For a unit direction $$v$$, the **directional (second) curvature** is\n",
        "\n",
        "$$\n",
        "v^\\top H_f(x)\\, v.\n",
        "$$\n",
        "\n",
        "If $$H_f(x)$$ is symmetric, it has an eigen-decomposition\n",
        "\n",
        "$$\n",
        "H_f(x) = Q \\Lambda Q^\\top,\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- eigenvalues $$\\lambda_i$$ are **principal curvatures**\n",
        "- eigenvectors (columns of $$Q$$) are **principal directions** of curvature\n",
        "\n",
        "---\n",
        "\n",
        "# 3. Relation to Gradient and Jacobian\n",
        "\n",
        "The gradient is a vector field:\n",
        "\n",
        "$$\n",
        "\\nabla f(x) =\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial f}{\\partial x_1}\\\\\n",
        "\\vdots\\\\\n",
        "\\frac{\\partial f}{\\partial x_n}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "The Hessian is the **Jacobian of the gradient**:\n",
        "\n",
        "$$\n",
        "H_f(x) \\;=\\; J_{\\nabla f}(x).\n",
        "$$\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "- Gradient: **first-order behavior** (local slope)\n",
        "- Hessian: **second-order behavior** (local curvature)\n",
        "\n",
        "---\n",
        "\n",
        "# 4. Critical Points and the Second-Derivative Test\n",
        "\n",
        "At a **critical point** $$x^\\star$$ where\n",
        "\n",
        "$$\n",
        "\\nabla f(x^\\star) = 0,\n",
        "$$\n",
        "\n",
        "the Hessian determines the nature of the point:\n",
        "\n",
        "- **Positive definite Hessian** $$H_f(x^\\star) \\succ 0$$  $$\\Rightarrow$$ local minimum  \n",
        "- **Negative definite Hessian** $$H_f(x^\\star) \\prec 0$$  $$\\Rightarrow$$ local maximum  \n",
        "- **Indefinite Hessian** (mixed-sign eigenvalues) $$\\Rightarrow$$ saddle point  \n",
        "- **Singular / semidefinite Hessian** $$\\Rightarrow$$ inconclusive test  \n",
        "\n",
        "This is foundational in optimization and stability analysis.\n",
        "\n",
        "---\n",
        "\n",
        "# 5. Taylor Expansion Interpretation (Key for AI)\n",
        "\n",
        "The Hessian appears naturally in the **second-order Taylor expansion**:\n",
        "\n",
        "$$\n",
        "f(x+\\Delta x)\n",
        "\\;\\approx\\;\n",
        "f(x) + \\nabla f(x)^\\top \\Delta x\n",
        "+ \\frac{1}{2}\\Delta x^\\top H_f(x)\\Delta x.\n",
        "$$\n",
        "\n",
        "The term\n",
        "\n",
        "$$\n",
        "\\frac{1}{2}\\Delta x^\\top H_f(x)\\Delta x\n",
        "$$\n",
        "\n",
        "is the **quadratic correction** and directly encodes local curvature.  \n",
        "It controls how accurate a local quadratic approximation is.\n",
        "\n",
        "---\n",
        "\n",
        "# 6. How the Hessian Works in AI Models\n",
        "\n",
        "In machine learning, the function is typically a **loss**:\n",
        "\n",
        "$$\n",
        "L(\\theta),\n",
        "$$\n",
        "\n",
        "where $$\\theta \\in \\mathbb{R}^n$$ are model parameters. The Hessian\n",
        "\n",
        "$$\n",
        "H_L(\\theta) = \\nabla^2_\\theta L(\\theta)\n",
        "$$\n",
        "\n",
        "describes the **curvature of the loss landscape**.\n",
        "\n",
        "---\n",
        "\n",
        "## 6.1 Curvature of the Loss Landscape\n",
        "\n",
        "The Hessian characterizes local shape:\n",
        "\n",
        "- **Flat minima**: small curvature in many directions  \n",
        "- **Sharp minima**: large curvature in some directions  \n",
        "- Large positive eigenvalues $$\\Rightarrow$$ steep directions  \n",
        "- Small eigenvalues $$\\Rightarrow$$ flat directions  \n",
        "\n",
        "A standard scalar summary is the **condition number** (when SPD):\n",
        "\n",
        "$$\n",
        "\\kappa(H) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}.\n",
        "$$\n",
        "\n",
        "Large $$\\kappa(H)$$ indicates **ill-conditioning**, often linked to slow/unstable optimization.\n",
        "\n",
        "---\n",
        "\n",
        "## 6.2 Newton-Type Methods\n",
        "\n",
        "Newton updates use curvature to rescale the gradient:\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - H_L(\\theta_t)^{-1}\\nabla L(\\theta_t).\n",
        "$$\n",
        "\n",
        "Advantages:\n",
        "- fast local convergence near well-behaved minima\n",
        "\n",
        "Disadvantages:\n",
        "- Hessian storage is $$O(n^2)$$ and inversion is typically $$O(n^3)$$, infeasible for large deep nets\n",
        "\n",
        "---\n",
        "\n",
        "## 6.3 Quasi-Newton Methods (Practical AI)\n",
        "\n",
        "Methods like **BFGS** and **L-BFGS** construct an approximation to $$H^{-1}$$ using gradient differences:\n",
        "\n",
        "$$\n",
        "s_t = \\theta_{t+1}-\\theta_t,\\qquad\n",
        "y_t = \\nabla L(\\theta_{t+1})-\\nabla L(\\theta_t).\n",
        "$$\n",
        "\n",
        "They avoid explicit Hessians and are widely used in classical ML / medium-scale settings.\n",
        "\n",
        "---\n",
        "\n",
        "## 6.4 Hessian–Vector Products (Deep Learning Trick)\n",
        "\n",
        "Deep learning frameworks often avoid forming $$H$$ explicitly and compute\n",
        "\n",
        "$$\n",
        "H(\\theta)\\,v\n",
        "$$\n",
        "\n",
        "efficiently via automatic differentiation. This enables:\n",
        "\n",
        "- truncated Newton methods  \n",
        "- curvature-aware diagnostics  \n",
        "- estimating top eigenvalues / spectral properties  \n",
        "\n",
        "without storing the full matrix.\n",
        "\n",
        "---\n",
        "\n",
        "# 7. Stability, Conditioning, and Training Dynamics\n",
        "\n",
        "The Hessian governs training stability through its spectrum:\n",
        "\n",
        "- **Exploding gradients** often correlate with large curvature directions (large $$\\lambda_{\\max}$$)\n",
        "- **Vanishing gradients** often correlate with near-flat directions (small eigenvalues)\n",
        "\n",
        "Ill-conditioning appears when\n",
        "\n",
        "$$\n",
        "\\kappa(H) \\gg 1.\n",
        "$$\n",
        "\n",
        "This motivates stabilizers such as:\n",
        "- gradient clipping\n",
        "- learning-rate schedules\n",
        "- adaptive optimizers\n",
        "- curvature approximations / preconditioning\n",
        "\n",
        "---\n",
        "\n",
        "# 8. Generalization and Flatness\n",
        "\n",
        "Empirically, solutions in regions where the Hessian is “small” in aggregate often generalize better, e.g.:\n",
        "\n",
        "- small dominant eigenvalues  \n",
        "- small trace (sum of eigenvalues)\n",
        "\n",
        "$$\n",
        "\\mathrm{tr}(H) = \\sum_{i=1}^n \\lambda_i.\n",
        "$$\n",
        "\n",
        "These ideas connect the Hessian to **flat vs sharp minima** analyses and generalization behavior.\n",
        "\n",
        "---\n",
        "\n",
        "# 9. Constrained Optimization (Bordered Hessian)\n",
        "\n",
        "With constraints (e.g., equality constraints), optimality conditions involve a **bordered Hessian** from the Lagrangian\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\theta,\\lambda) = L(\\theta) + \\lambda^\\top c(\\theta),\n",
        "$$\n",
        "\n",
        "where $$c(\\theta)=0$$ encodes constraints. The bordered Hessian helps classify constrained extrema and appears in KKT-based analysis.\n",
        "\n",
        "---\n",
        "\n",
        "# 10. Beyond Scalar Losses\n",
        "\n",
        "If outputs are vector-valued, second derivatives form higher-order objects (tensors). In most AI training, this is reduced to a scalar loss $$L(\\theta)$$ before optimization, restoring the standard Hessian.\n",
        "\n",
        "---\n",
        "\n",
        "# 11. Other AI-Relevant Applications\n",
        "\n",
        "Computer vision:\n",
        "- blob / structure detection using determinant-based constructs (e.g., Hessian-based detectors)\n",
        "\n",
        "Optimization diagnostics:\n",
        "- curvature monitoring, sensitivity analysis, uncertainty-related approximations\n",
        "\n",
        "Evolution strategies:\n",
        "- covariance adaptation often aligns with inverse-curvature structure in quadratic regimes\n",
        "\n",
        "---\n",
        "\n",
        "# 12. One-Sentence Mental Model\n",
        "\n",
        "The gradient tells you where to go; the Hessian tells you how the terrain bends while you move.\n",
        "\n",
        "---\n",
        "\n",
        "# 13. Why the Hessian Is Fundamental in AI\n",
        "\n",
        "- defines curvature of the objective\n",
        "- controls convergence speed and step stability\n",
        "- explains instability via conditioning and spectrum\n",
        "- underpins sharp vs flat minima discussions\n",
        "- connects geometry, optimization, and generalization\n"
      ],
      "metadata": {
        "id": "3KL2Htd4VnMz"
      }
    }
  ]
}