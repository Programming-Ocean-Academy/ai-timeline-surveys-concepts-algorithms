{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Early Models Related to Markov/Neural Foundations (1900–1990)\n",
        "\n",
        "## 1. Ising Model (1925)\n",
        "- **Domain**: Statistical physics  \n",
        "- **Description**: Models binary spins (+1 / –1) with local interactions.  \n",
        "- **Relevance**: Inspired later probabilistic models in AI (Boltzmann Machines, Hopfield Nets).  \n",
        "- **Connection to Markov**: Markov Random Fields can be seen as generalizations of the Ising model.  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. McCulloch–Pitts Neuron (1943)\n",
        "- **Domain**: Neuroscience-inspired computing  \n",
        "- **Description**: Simplified model of a biological neuron, using binary threshold logic.  \n",
        "- **Relevance**: First formal model of neural networks.  \n",
        "- **Connection**: Like Markov chains, it formalized computation with discrete states.  \n",
        "\n",
        "---\n",
        "\n",
        "## 3. Hebbian Learning Rule (1949)\n",
        "- **Concept**: “Cells that fire together, wire together.”  \n",
        "- **Description**: Strengthens connections between co-activated neurons.  \n",
        "- **Relevance**: Early rule for updating weights in networks.  \n",
        "- **Connection**: Provided a probabilistic, local learning mechanism (like stochastic updates in Markov processes).  \n",
        "\n",
        "---\n",
        "\n",
        "## 4. Perceptron (Rosenblatt, 1958)\n",
        "- **Description**: Linear classifier using weighted sums of inputs + threshold.  \n",
        "- **Relevance**: First trainable machine-learning model inspired by biology.  \n",
        "- **Limitations**: Could not solve XOR (proved by Minsky & Papert, 1969).  \n",
        "- **Connection**: Simple state-to-output mapping, like transitions in a Markov model.  \n",
        "\n",
        "---\n",
        "\n",
        "## 5. Adaline & Delta Rule (Widrow & Hoff, 1960)\n",
        "- **Description**: Linear unit trained with gradient descent (LMS rule).  \n",
        "- **Relevance**: Introduced optimization for weight updates.  \n",
        "- **Connection**: Early continuous-valued extension of discrete Markov-like state models.  \n",
        "\n",
        "---\n",
        "\n",
        "## 6. Hopfield Network (1982)\n",
        "- **Description**: Recurrent neural network with symmetric weights, converging to stable attractor states (energy minimization).  \n",
        "- **Relevance**: Linked neural computation to statistical physics.  \n",
        "- **Connection**: Strongly connected to Ising models and Markov Random Fields.  \n",
        "\n",
        "---\n",
        "\n",
        "## 7. Boltzmann Machine (Hinton & Sejnowski, 1985)\n",
        "- **Description**: Stochastic recurrent network; hidden and visible nodes with probabilistic activations.  \n",
        "- **Relevance**: One of the first deep generative models.  \n",
        "- **Connection**: Uses Markov Chain Monte Carlo (MCMC) sampling → explicit link between Markov processes and neural networks.  \n",
        "\n",
        "---\n",
        "\n",
        "## 8. Backpropagation (Rumelhart, Hinton, Williams, 1986)\n",
        "- **Description**: Algorithm for training multi-layer networks via gradient descent and chain rule.  \n",
        "- **Relevance**: Opened the path for deep learning.  \n",
        "- **Connection**: Generalized learning beyond local updates like Hebbian rules — but still probabilistic at its core.  \n",
        "\n",
        "---\n",
        "\n",
        "## 9. Time-Dependent Models (1986–1989)\n",
        "- **Jordan Networks (1986)** and **Elman Networks (1989)**  \n",
        "- **Description**: Introduced recurrent feedback to model sequences.  \n",
        "- **Relevance**: Early inspiration for later RNNs and sequence models (GRUs, LSTMs, Transformers).  \n",
        "- **Connection**: Like Hidden Markov Models, these modeled sequential/temporal dependencies.  \n",
        "\n",
        "---\n",
        "\n",
        "##  Summary\n",
        "Between 1900 and 1990, the “Markov-like” predecessors in neural and deep learning were:\n",
        "\n",
        "- **Ising Model (1925)** → Probabilistic physics model, inspired energy-based networks.  \n",
        "- **McCulloch–Pitts Neuron (1943)** → First abstract neuron.  \n",
        "- **Hebbian Learning (1949)** → First biologically motivated weight update.  \n",
        "- **Perceptron (1958)** → First practical neural network.  \n",
        "- **Adaline (1960)** → Gradient-based training.  \n",
        "- **Hopfield Networks (1982)** → Energy minimization, attractor dynamics.  \n",
        "- **Boltzmann Machines (1985)** → Stochastic generative model with MCMC.  \n",
        "- **Backpropagation (1986)** → Training multi-layer nets.  \n",
        "- **Elman/Jordan Networks (1986–1989)** → Recurrent sequence modeling.  \n"
      ],
      "metadata": {
        "id": "qI2D9l6WQM6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Foundational Papers of Neural Network & Sequence Models (1925–2017)\n",
        "\n",
        "## Ising Model (1925)\n",
        "- Paper: *Beitrag zur Theorie des Ferromagnetismus*  \n",
        "- Author: Ernst Ising  \n",
        "- Venue: Zeitschrift für Physik, 1925  \n",
        "\n",
        "## Markov Model (1906)\n",
        "- Paper: *Extension of the Law of Large Numbers to Dependent Quantities*  \n",
        "- Author: Andrey Markov  \n",
        "- Venue: Proceedings of the Imperial Academy of Sciences of St. Petersburg, 1906  \n",
        "\n",
        "## Hidden Markov Model (1960s–1970s)\n",
        "- Paper: *Probabilistic Functions of a Markov Chain and Hidden Markov Models*  \n",
        "- Author: Leonard E. Baum and colleagues (Baum, Petrie, Soules, Weiss)  \n",
        "- Venue: Annals of Mathematical Statistics, 1966–1970  \n",
        "\n",
        "## McCulloch–Pitts Neuron (1943)\n",
        "- Paper: *A Logical Calculus of the Ideas Immanent in Nervous Activity*  \n",
        "- Authors: Warren McCulloch, Walter Pitts  \n",
        "- Venue: Bulletin of Mathematical Biophysics, 1943  \n",
        "\n",
        "## Hebbian Learning Rule (1949)\n",
        "- Book: *The Organization of Behavior: A Neuropsychological Theory*  \n",
        "- Author: Donald O. Hebb  \n",
        "- Publisher: Wiley, 1949  \n",
        "\n",
        "## Perceptron (1958)\n",
        "- Paper: *The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain*  \n",
        "- Author: Frank Rosenblatt  \n",
        "- Venue: Psychological Review, 1958  \n",
        "\n",
        "## Adaline & Delta Rule (1960)\n",
        "- Paper: *Adaptive Switching Circuits*  \n",
        "- Authors: Bernard Widrow, Marcian Hoff  \n",
        "- Venue: IRE WESCON Convention Record, 1960  \n",
        "\n",
        "## Hopfield Network (1982)\n",
        "- Paper: *Neural networks and physical systems with emergent collective computational abilities*  \n",
        "- Author: John J. Hopfield  \n",
        "- Venue: Proceedings of the National Academy of Sciences (PNAS), 1982  \n",
        "\n",
        "## Boltzmann Machine (1985)\n",
        "- Paper: *A Learning Algorithm for Boltzmann Machines*  \n",
        "- Authors: Geoffrey E. Hinton, Terrence J. Sejnowski  \n",
        "- Venue: Cognitive Science, 1985  \n",
        "\n",
        "## Backpropagation (1986)\n",
        "- Paper: *Learning representations by back-propagating errors*  \n",
        "- Authors: David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams  \n",
        "- Venue: Nature, 1986  \n",
        "\n",
        "## Jordan Network (1986)\n",
        "- Paper: *Attractor Dynamics and Parallelism in a Connectionist Sequential Machine*  \n",
        "- Author: Michael I. Jordan  \n",
        "- Venue: Proceedings of the Eighth Annual Conference of the Cognitive Science Society, 1986  \n",
        "\n",
        "## Elman Network (1989)\n",
        "- Paper: *Finding Structure in Time*  \n",
        "- Author: Jeffrey L. Elman  \n",
        "- Venue: Cognitive Science, 1989  \n",
        "\n",
        "## Vanilla RNN (1990s Formalization)\n",
        "- Paper: *Learning long-term dependencies with gradient descent is difficult*  \n",
        "- Authors: Yoshua Bengio, Patrice Simard, Paolo Frasconi  \n",
        "- Venue: IEEE Transactions on Neural Networks, 1994  \n",
        "\n",
        "While Elman and Jordan proposed the first practical RNNs, Bengio’s paper formalized the **vanishing gradient problem**.  \n",
        "\n",
        "## LSTM (1997)\n",
        "- Paper: *Long Short-Term Memory*  \n",
        "- Authors: Sepp Hochreiter, Jürgen Schmidhuber  \n",
        "- Venue: Neural Computation, 1997  \n",
        "\n",
        "## GRU (2014)\n",
        "- Paper: *Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation*  \n",
        "- Authors: Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bahdanau, Yoshua Bengio  \n",
        "- Venue: EMNLP, 2014  \n",
        "\n",
        "## Transformer (2017)\n",
        "- Paper: *Attention Is All You Need*  \n",
        "- Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin  \n",
        "- Venue: NeurIPS, 2017  \n"
      ],
      "metadata": {
        "id": "VNcd7CJczeOg"
      }
    }
  ]
}