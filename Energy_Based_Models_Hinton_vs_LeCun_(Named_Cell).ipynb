{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# https://web.stanford.edu/class/cs379c/archive/2012/suggested_reading_list/documents/LeCunetal06.pdf\n",
        "\n",
        "# https://www.cs.toronto.edu/~fritz/absps/cogscibm.pdf"
      ],
      "metadata": {
        "id": "k4zY_EIjrqYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Who First Used “Energy” in a Generative Neural Network?\n",
        "\n",
        "The first use of “energy” as a generative model in neural networks is:\n",
        "\n",
        "Geoffrey Hinton and Terrence Sejnowski (1985)  \n",
        "Paper: “A Learning Algorithm for Boltzmann Machines”\n",
        "\n",
        "Before this paper:\n",
        "\n",
        "Neural networks were discriminative only (perceptrons, early MLPs).\n",
        "\n",
        "No generative probabilistic interpretation existed.\n",
        "\n",
        "No link to physics (energy, temperature, Boltzmann distribution).\n",
        "\n",
        "Hinton introduced:\n",
        "\n",
        "Energy function E(x, h)\n",
        "\n",
        "Boltzmann distribution\n",
        "\n",
        "$$\n",
        "p(x,h)=\\frac{e^{-E(x,h)/T}}{Z}\n",
        "$$\n",
        "\n",
        "Temperature T (from statistical physics)\n",
        "\n",
        "Partition function Z\n",
        "\n",
        "Sampling via Gibbs sampling and simulated annealing\n",
        "\n",
        "Learning via minimizing energy of data vs. increasing energy of non-data states\n",
        "\n",
        "This is the birth of energy-based generative models.\n",
        "\n",
        "Conclusion:  \n",
        "Hinton is the pioneer of energy-based generative modeling.  \n",
        "All later EBMs—including LeCun’s—extend Hinton’s foundational idea.\n",
        "\n",
        "2. What Does the LeCun Paper Do?\n",
        "\n",
        "File: “A Tutorial on Energy-Based Learning” (2006)\n",
        "\n",
        "The LeCun tutorial is not introducing energy-based models.  \n",
        "It is doing something different:\n",
        "\n",
        "It generalizes the idea of “energy”\n",
        "\n",
        "from probabilistic generative physics-based models  \n",
        "to non-probabilistic, flexible, discriminative learning machines.\n",
        "\n",
        "LeCun removes the dependency on:\n",
        "\n",
        "Probability distributions  \n",
        "Temperature  \n",
        "Partition functions  \n",
        "Sampling  \n",
        "Normalization  \n",
        "Z (intractable integrals)\n",
        "\n",
        "LeCun expands energy to any model that assigns:\n",
        "\n",
        "Low energy = compatible  \n",
        "High energy = incompatible\n",
        "\n",
        "The tutorial introduces:\n",
        "\n",
        "Energy-based inference  \n",
        "Energy shaping  \n",
        "Margin-based losses  \n",
        "Contrastive losses  \n",
        "Perceptron and hinge losses  \n",
        "Structured prediction with energies  \n",
        "CRFs, max-margin Markov networks  \n",
        "Graph-transformer networks  \n",
        "Distances and compatibility functions  \n",
        "Sequence labeling  \n",
        "Factor graphs  \n",
        "Many architectures, not just generative ones\n",
        "\n",
        "Key point:  \n",
        "LeCun’s EBMs do not require probabilities.  \n",
        "They are a general framework.\n",
        "\n",
        "3. The Fundamental Differences Between the Two Papers\n",
        "\n",
        "Here is the precise scientific comparison.\n",
        "\n",
        "Difference 1: Purpose of the Model\n",
        "\n",
        "Hinton (1985)\n",
        "\n",
        "A probabilistic generative model.\n",
        "\n",
        "Goal: learn full joint distribution over visible and hidden units.\n",
        "\n",
        "Sampling: Gibbs and stochastic simulated annealing.\n",
        "\n",
        "Motivation: biology and statistical physics.\n",
        "\n",
        "LeCun (2006)\n",
        "\n",
        "A general mathematical framework for learning with energies.\n",
        "\n",
        "Does not need probabilities.\n",
        "\n",
        "Does not require tractable Z.\n",
        "\n",
        "Useful for classification, ranking, structured prediction, regression.\n",
        "\n",
        "Summary:  \n",
        "Hinton = generative probability model  \n",
        "LeCun = general energy framework (probabilistic and non-probabilistic)\n",
        "\n",
        "Difference 2: Role of Energy\n",
        "\n",
        "Hinton (1985): Energy defines a probability.\n",
        "\n",
        "$$\n",
        "p(x)=\\frac{e^{-E(x)/T}}{Z}\n",
        "$$\n",
        "\n",
        "Low energy → high probability.\n",
        "\n",
        "Energy is literally negative log probability times temperature.\n",
        "\n",
        "LeCun (2006): Energy is compatibility, not probability.\n",
        "\n",
        "low energy = good answer (no need for normalization)\n",
        "\n",
        "Energy is not tied to probability unless you choose to convert it.\n",
        "\n",
        "Summary:  \n",
        "Hinton = energy → probability  \n",
        "LeCun = energy → scoring function\n",
        "\n",
        "Difference 3: Temperature\n",
        "\n",
        "Hinton:\n",
        "\n",
        "Temperature T is critical:\n",
        "\n",
        "Used during annealing  \n",
        "Controls stochasticity  \n",
        "Comes from physics  \n",
        "Helps escape local minima  \n",
        "Governs the Gibbs distribution\n",
        "\n",
        "LeCun:\n",
        "\n",
        "Temperature is optional.  \n",
        "Only appears if converting energy → probability.  \n",
        "Not required in most EBMs.\n",
        "\n",
        "Summary:  \n",
        "Hinton = temperature is essential  \n",
        "LeCun = temperature is optional or irrelevant\n",
        "\n",
        "Difference 4: Training Method\n",
        "\n",
        "Hinton (1985):\n",
        "\n",
        "Training uses:\n",
        "\n",
        "Gradient of log-likelihood  \n",
        "Two expectations (positive phase and negative phase)  \n",
        "Requires Markov Chain Monte Carlo  \n",
        "Requires sampling (Gibbs)  \n",
        "Computes equilibrium statistics  \n",
        "Difficult to scale\n",
        "\n",
        "This is the ancestor of Contrastive Divergence, RBMs, DBNs.\n",
        "\n",
        "LeCun (2006):\n",
        "\n",
        "Training uses:\n",
        "\n",
        "Margin losses  \n",
        "Perceptron loss  \n",
        "Contrastive losses  \n",
        "Hinge loss  \n",
        "Log loss  \n",
        "Structured losses\n",
        "\n",
        "No sampling required\n",
        "\n",
        "Inference by minimization, not sampling\n",
        "\n",
        "Summary:  \n",
        "Hinton = generative training via sampling  \n",
        "LeCun = discriminative or general training via energy shaping\n",
        "\n",
        "Difference 5: Type of Tasks\n",
        "\n",
        "Hinton:\n",
        "\n",
        "Generative modeling  \n",
        "Density modeling  \n",
        "Missing data prediction  \n",
        "Sampling and reconstruction  \n",
        "Unsupervised learning\n",
        "\n",
        "LeCun:\n",
        "\n",
        "Classification  \n",
        "Ranking  \n",
        "Detection  \n",
        "Sequence labeling  \n",
        "Structured output  \n",
        "Optimization problems  \n",
        "CRFs, MMNs, graph models\n",
        "\n",
        "Summary:  \n",
        "Hinton = generative  \n",
        "LeCun = general-purpose machine learning\n",
        "\n",
        "Difference 6: Normalization\n",
        "\n",
        "Hinton:\n",
        "\n",
        "Requires computing or approximating Z, the partition function.\n",
        "\n",
        "LeCun:\n",
        "\n",
        "EBMs remove the normalization requirement.\n",
        "\n",
        "This is the largest conceptual shift.\n",
        "\n",
        "Final Answer\n",
        "\n",
        "Who first used energy in a generative neural network?  \n",
        "Geoffrey Hinton (1985), Boltzmann Machine.\n",
        "\n",
        "What is the difference between the two papers?\n",
        "\n",
        "| Topic | Hinton (1985) | LeCun (2006) |\n",
        "|-------|----------------|----------------|\n",
        "| Model type | Probabilistic generative model | General energy-based framework |\n",
        "| Energy meaning | Negative log-probability | Compatibility score |\n",
        "| Temperature | Essential | Optional |\n",
        "| Partition function Z | Required | Not required |\n",
        "| Learning | Stochastic sampling and likelihood | Direct energy shaping and loss functions |\n",
        "| Tasks | Generative modeling | Classification, ranking, structured prediction |\n",
        "| Architecture | Boltzmann Machine | Many architectures (CRFs, neural nets, GTN, etc.) |\n"
      ],
      "metadata": {
        "id": "avRxcBmfrkc1"
      }
    }
  ]
}