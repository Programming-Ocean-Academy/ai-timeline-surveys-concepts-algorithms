{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Complete Mathematical Equations for Major Deep Learning Models**\n",
        "\n",
        "---\n",
        "\n",
        "# **1. Feedforward Neural Network (FNN)**\n",
        "\n",
        "Given input:\n",
        "\n",
        "$$\n",
        "\\mathbf{x} \\in \\mathbb{R}^d\n",
        "$$\n",
        "\n",
        "Hidden layer:\n",
        "\n",
        "$$\n",
        "\\mathbf{h} = \\sigma(\\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1)\n",
        "$$\n",
        "\n",
        "Output:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(\\mathbf{W}_2 \\mathbf{h} + \\mathbf{b}_2)\n",
        "$$\n",
        "\n",
        "Activation:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "# **2. Convolutional Neural Network (CNN)**\n",
        "\n",
        "## **2.1 Convolution Operation**\n",
        "\n",
        "For image \\(X\\) and kernel \\(K\\):\n",
        "\n",
        "$$\n",
        "Y_{i,j} = \\sum_{m=1}^{k} \\sum_{n=1}^{k} K_{m,n}\\, X_{i+m, j+n}\n",
        "$$\n",
        "\n",
        "## **2.2 ReLU Activation**\n",
        "\n",
        "$$\n",
        "\\text{ReLU}(z) = \\max(0, z)\n",
        "$$\n",
        "\n",
        "## **2.3 Max Pooling**\n",
        "\n",
        "$$\n",
        "Y_{i,j} = \\max_{p,q \\in \\text{window}} X_{p,q}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "# **3. Recurrent Neural Network (RNN)**\n",
        "\n",
        "Hidden state update:\n",
        "\n",
        "$$\n",
        "h_t = \\tanh(W_h h_{t-1} + W_x x_t + b_h)\n",
        "$$\n",
        "\n",
        "Prediction:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(W_y h_T + b_y)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "# **4. Long Short-Term Memory (LSTM)**\n",
        "\n",
        "Given \\( x_t, h_{t-1}, c_{t-1} \\):\n",
        "\n",
        "### Forget gate:\n",
        "\n",
        "$$\n",
        "f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f)\n",
        "$$\n",
        "\n",
        "### Input gate:\n",
        "\n",
        "$$\n",
        "i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i)\n",
        "$$\n",
        "\n",
        "### Candidate cell:\n",
        "\n",
        "$$\n",
        "\\tilde{c}_t = \\tanh(W_c x_t + U_c h_{t-1} + b_c)\n",
        "$$\n",
        "\n",
        "### Cell state:\n",
        "\n",
        "$$\n",
        "c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t\n",
        "$$\n",
        "\n",
        "### Output gate:\n",
        "\n",
        "$$\n",
        "o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o)\n",
        "$$\n",
        "\n",
        "### Hidden state:\n",
        "\n",
        "$$\n",
        "h_t = o_t \\odot \\tanh(c_t)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "# **5. Gated Recurrent Unit (GRU)**\n",
        "\n",
        "Update gate:\n",
        "\n",
        "$$\n",
        "z_t = \\sigma(W_z x_t + U_z h_{t-1} + b_z)\n",
        "$$\n",
        "\n",
        "Reset gate:\n",
        "\n",
        "$$\n",
        "r_t = \\sigma(W_r x_t + U_r h_{t-1} + b_r)\n",
        "$$\n",
        "\n",
        "Candidate hidden:\n",
        "\n",
        "$$\n",
        "\\tilde{h}_t = \\tanh(W_h x_t + U_h (r_t \\odot h_{t-1}) + b_h)\n",
        "$$\n",
        "\n",
        "Final hidden:\n",
        "\n",
        "$$\n",
        "h_t = (1 - z_t)\\, h_{t-1} + z_t\\, \\tilde{h}_t\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "# **6. Transformer — Self-Attention**\n",
        "\n",
        "Input embeddings:\n",
        "\n",
        "$$\n",
        "X \\in \\mathbb{R}^{T \\times d}\n",
        "$$\n",
        "\n",
        "### Linear projections:\n",
        "\n",
        "$$\n",
        "Q = XW^Q,\\qquad K = XW^K,\\qquad V = XW^V\n",
        "$$\n",
        "\n",
        "### Scaled dot-product attention:\n",
        "\n",
        "Scores:\n",
        "\n",
        "$$\n",
        "S = \\frac{QK^\\top}{\\sqrt{d_k}}\n",
        "$$\n",
        "\n",
        "Softmax weights:\n",
        "\n",
        "$$\n",
        "A = \\text{softmax}(S)\n",
        "$$\n",
        "\n",
        "Output:\n",
        "\n",
        "$$\n",
        "O = A V\n",
        "$$\n",
        "\n",
        "### Multi-Head Attention:\n",
        "\n",
        "For heads \\(h = 1,\\dots,H\\):\n",
        "\n",
        "$$\n",
        "\\text{MHA}(X) =\n",
        "\\text{Concat}(O_1, O_2, \\dots, O_H)\\, W^O\n",
        "$$\n",
        "\n",
        "### Transformer Feed-Forward Network:\n",
        "\n",
        "$$\n",
        "\\text{FFN}(x) = W_2\\, \\text{ReLU}(W_1 x + b_1) + b_2\n",
        "$$\n",
        "\n",
        "### Residual + LayerNorm:\n",
        "\n",
        "$$\n",
        "x' = \\text{LayerNorm}(x + \\text{MHA}(x))\n",
        "$$\n",
        "\n",
        "$$\n",
        "x_{\\text{out}} = \\text{LayerNorm}(x' + \\text{FFN}(x'))\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "# **Summary Table**\n",
        "\n",
        "| Model | Key Equations |\n",
        "|-------|---------------|\n",
        "| **FNN** | $$h = \\sigma(W_1 x + b_1), \\qquad \\hat{y} = \\sigma(W_2 h + b_2)$$ |\n",
        "| **CNN** | $$\\text{Convolution}, \\quad \\text{ReLU}, \\quad \\text{MaxPool}$$ |\n",
        "| **RNN** | $$h_t = \\tanh(W_h h_{t-1} + W_x x_t)$$ |\n",
        "| **LSTM** | $$f_t,\\, i_t,\\, o_t,\\qquad c_t,\\qquad h_t$$ |\n",
        "| **GRU** | $$z_t,\\, r_t,\\qquad \\tilde{h}_t,\\qquad h_t$$ |\n",
        "| **Transformer** | $$Q = XW^Q,\\quad K = XW^K,\\quad V = XW^V$$ $$A = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right), \\qquad O = AV$$ |\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yY4TTx7d6Oal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ALL GENERATIVE MODELS**\n",
        "\n",
        "| **Model** | **Core Mathematical Equations** |\n",
        "|----------|----------------------------------|\n",
        "| **VAE** | $$\\text{ELBO} = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{\\mathrm{KL}}(q_\\phi(z|x)\\,\\|\\,p(z))$$ $$z = \\mu_\\phi(x) + \\sigma_\\phi(x)\\,\\epsilon,\\quad \\epsilon \\sim \\mathcal{N}(0,I)$$ |\n",
        "| **GAN** | $$\\min_G \\max_D\\; \\mathbb{E}_{x\\sim p_{\\text{data}}}[\\log D(x)] + \\mathbb{E}_{z\\sim p(z)}[\\log(1 - D(G(z)))]$$ $$L_G = -\\mathbb{E}_{z}[\\log D(G(z))]$$ |\n",
        "| **DDPM** | $$q(x_t|x_{t-1})=\\mathcal{N}\\!\\left(x_t;\\sqrt{1-\\beta_t}\\,x_{t-1},\\,\\beta_t I\\right)$$ $$q(x_t|x_0)=\\mathcal{N}\\!\\left(x_t;\\sqrt{\\bar{\\alpha}_t}\\,x_0,\\,(1-\\bar{\\alpha}_t)I\\right)$$ $$L = \\mathbb{E}\\left[\\lVert \\epsilon - \\epsilon_\\theta(x_t,t)\\rVert^2\\right]$$ |\n",
        "| **Normalizing Flow** | $$x = f_K\\circ\\cdots\\circ f_1(z_0),\\qquad z_0\\sim p_0(z_0)$$ $$\\log p(x)=\\log p_0(z_0)-\\sum_{k=1}^K \\log\\left|\\det\\frac{\\partial f_k}{\\partial z_{k-1}}\\right|$$ |\n",
        "| **Autoregressive Models** | $$p(x)=\\prod_{t=1}^T p(x_t \\mid x_{<t})$$ $$A=\\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}} + M\\right)$$ |\n",
        "| **Energy-Based Models (EBM)** | $$p_\\theta(x) = \\frac{e^{-E_\\theta(x)}}{Z_\\theta}$$ $$Z_\\theta = \\int e^{-E_\\theta(x)}\\,dx$$ |\n",
        "| **RBM** | $$E(v,h)=-v^\\top Wh - b^\\top v - c^\\top h$$ $$p(v,h)=\\frac{e^{-E(v,h)}}{Z}$$ $$\\Delta W \\propto \\langle vh^\\top\\rangle_{\\text{data}} - \\langle vh^\\top\\rangle_{\\text{model}}$$ |\n",
        "| **Score-Based Models** | $$s_\\theta(x)=\\nabla_x \\log p(x)$$ $$x_{t+1}=x_t + \\frac{\\epsilon}{2}s_\\theta(x_t) + \\sqrt{\\epsilon}\\,z_t$$ |\n",
        "| **GMM** | $$p(x)=\\sum_{k=1}^K \\pi_k\\,\\mathcal{N}(x;\\mu_k,\\Sigma_k)$$ $$\\gamma_{nk}=\\frac{\\pi_k\\,\\mathcal{N}(x_n;\\mu_k,\\Sigma_k)}{\\sum_j\\pi_j\\,\\mathcal{N}(x_n;\\mu_j,\\Sigma_j)}$$ $$\\mu_k=\\frac{\\sum_n\\gamma_{nk}x_n}{\\sum_n\\gamma_{nk}},\\quad \\Sigma_k=\\frac{\\sum_n\\gamma_{nk}(x_n-\\mu_k)(x_n-\\mu_k)^\\top}{\\sum_n\\gamma_{nk}},\\quad \\pi_k=\\frac{1}{N}\\sum_n\\gamma_{nk}$$ |\n",
        "| **Transformer Decoder (Generative)** | $$A=\\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}} + M\\right)$$ $$P(x_t|x_{<t})=\\text{softmax}(W h_t)$$ $$x_t \\sim P(x_t|x_{<t})$$ |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "EI1A7vCb7QDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Activation Summary Table (All Equations)**\n",
        "\n",
        "| **Activation** | **Mathematical Equation** |\n",
        "|----------------|----------------------------|\n",
        "| **Sigmoid** | $$\\sigma(x)=\\frac{1}{1+e^{-x}}$$ |\n",
        "| **Tanh** | $$\\tanh(x)=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$ |\n",
        "| **ReLU** | $$\\text{ReLU}(x)=\\max(0,x)$$ |\n",
        "| **Leaky ReLU** | $$\\text{LeakyReLU}(x)=\\begin{cases}\\alpha x & x<0 \\\\ x & x\\ge 0\\end{cases}$$ |\n",
        "| **PReLU** | $$\\text{PReLU}(x)=\\begin{cases}a x & x<0 \\\\ x & x\\ge 0\\end{cases}$$ |\n",
        "| **ELU** | $$\\text{ELU}(x)=\\begin{cases}x & x\\ge 0 \\\\ \\alpha(e^{x}-1) & x<0\\end{cases}$$ |\n",
        "| **SELU** | $$\\text{SELU}(x)=\\lambda\\begin{cases}x & x\\ge 0 \\\\ \\alpha(e^{x}-1) & x<0\\end{cases}$$ |\n",
        "| **GELU (exact)** | $$\\text{GELU}(x)=x\\,\\Phi(x)$$ |\n",
        "| **GELU (approx.)** | $$\\text{GELU}(x)\\approx 0.5x\\left(1+\\tanh\\!\\left[\\sqrt{\\frac{2}{\\pi}}\\,(x+0.044715x^3)\\right]\\right)$$ |\n",
        "| **Softplus** | $$\\text{Softplus}(x)=\\ln(1+e^{x})$$ |\n",
        "| **Mish** | $$\\text{Mish}(x)=x\\,\\tanh(\\ln(1+e^{x}))$$ |\n",
        "| **Swish** | $$\\text{Swish}(x)=x\\,\\sigma(x)$$ |\n",
        "| **Hard Sigmoid** | $$\\text{HardSigmoid}(x)=\\max(0,\\min(1,\\,0.2x+0.5))$$ |\n",
        "| **Hard Tanh** | $$\\text{HardTanh}(x)=\\begin{cases}-1 & x<-1 \\\\ x & -1\\le x\\le 1 \\\\ 1 & x>1\\end{cases}$$ |\n",
        "| **Softsign** | $$\\text{Softsign}(x)=\\frac{x}{1+|x|}$$ |\n",
        "| **Softmax** | $$\\text{Softmax}(x_i)=\\frac{e^{x_i}}{\\sum_j e^{x_j}}$$ |\n",
        "| **Maxout** | $$\\text{Maxout}(x)=\\max_k(W_k x + b_k)$$ |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vn50U-dU8HkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **All Optimizers**\n",
        "\n",
        "| **Optimizer** | **Core Mathematical Equation** |\n",
        "|---------------|--------------------------------|\n",
        "| **Gradient Descent (GD)** | $$\\theta_{t+1}=\\theta_t-\\eta\\,\\nabla_\\theta L(\\theta_t)$$ |\n",
        "| **SGD** | $$\\theta_{t+1}=\\theta_t-\\eta\\,\\nabla_\\theta L(\\theta_t;x_i)$$ |\n",
        "| **Momentum** | $$v_t=\\beta v_{t-1}+(1-\\beta)g_t,\\qquad \\theta_{t+1}=\\theta_t-\\eta v_t$$ |\n",
        "| **NAG** | $$v_t=\\beta v_{t-1}+\\nabla_\\theta L(\\theta_t-\\eta\\beta v_{t-1}),\\qquad \\theta_{t+1}=\\theta_t-\\eta v_t$$ |\n",
        "| **AdaGrad** | $$G_t=G_{t-1}+g_t^2,\\qquad \\theta_{t+1}=\\theta_t-\\frac{\\eta}{\\sqrt{G_t+\\epsilon}}g_t$$ |\n",
        "| **RMSProp** | $$E[g^2]_t=\\beta E[g^2]_{t-1}+(1-\\beta)g_t^2,\\qquad \\theta_{t+1}=\\theta_t-\\frac{\\eta}{\\sqrt{E[g^2]_t+\\epsilon}}g_t$$ |\n",
        "| **Adam** | $$m_t=\\beta_1m_{t-1}+(1-\\beta_1)g_t,\\quad v_t=\\beta_2v_{t-1}+(1-\\beta_2)g_t^2$$ $$\\hat m_t=\\frac{m_t}{1-\\beta_1^t},\\;\\hat v_t=\\frac{v_t}{1-\\beta_2^t}$$ $$\\theta_{t+1}=\\theta_t-\\eta\\frac{\\hat m_t}{\\sqrt{\\hat v_t}+\\epsilon}$$ |\n",
        "| **AdamW** | $$\\theta_{t+1}=\\theta_t-\\eta\\left(\\frac{\\hat m_t}{\\sqrt{\\hat v_t}+\\epsilon}+\\lambda\\theta_t\\right)$$ |\n",
        "| **AdaDelta** | $$E[g^2]_t=\\rho E[g^2]_{t-1}+(1-\\rho)g_t^2$$ $$\\Delta\\theta_t=-\\frac{\\sqrt{E[\\Delta\\theta^2]_{t-1}+\\epsilon}}{\\sqrt{E[g^2]_t+\\epsilon}}g_t$$ $$\\theta_{t+1}=\\theta_t+\\Delta\\theta_t$$ |\n",
        "| **AMSGrad** | $$v_t=\\beta_2v_{t-1}+(1-\\beta_2)g_t^2,\\qquad \\hat v_t=\\max(\\hat v_{t-1},v_t)$$ $$\\theta_{t+1}=\\theta_t-\\eta\\frac{m_t}{\\sqrt{\\hat v_t}+\\epsilon}$$ |\n",
        "| **Nadam** | $$\\theta_{t+1}=\\theta_t-\\eta\\frac{\\beta_1\\hat m_t+\\frac{(1-\\beta_1)g_t}{1-\\beta_1^t}}{\\sqrt{\\hat v_t}+\\epsilon}$$ |\n",
        "| **Lion** | $$m_t=\\beta_1m_{t-1}+(1-\\beta_1)g_t,\\qquad \\theta_{t+1}=\\theta_t-\\eta\\,\\text{sign}(m_t)$$ |\n",
        "| **LAMB** | $$r_t=\\frac{\\|\\theta_t\\|}{\\left\\|\\frac{m_t}{\\sqrt{v_t}+\\epsilon}\\right\\|}$$ $$\\theta_{t+1}=\\theta_t-\\eta\\,r_t\\,\\frac{m_t}{\\sqrt{v_t}+\\epsilon}$$ |\n",
        "| **Adafactor** | $$v_t \\approx r_t c_t,\\qquad \\theta_{t+1}=\\theta_t-\\eta\\frac{g_t}{\\sqrt{v_t}+\\epsilon}$$ |\n",
        "| **Lookahead** | $$\\theta^{(f)}_{t+1}=\\text{opt}(\\theta^{(f)}_t)$$ $$\\theta^{(s)}_{t+1}=\\theta^{(s)}_t+\\alpha(\\theta^{(f)}_{t+1}-\\theta^{(s)}_t)$$ $$\\theta^{(f)}_{t+1}=\\theta^{(s)}_{t+1}$$ |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "RlznFSYf8i1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ALL Backpropagation**\n",
        "\n",
        "| **Component** | **Key Backpropagation Equation** |\n",
        "|---------------|----------------------------------|\n",
        "| **Feedforward (Linear Layer)** | $$\\frac{\\partial L}{\\partial z}=\\delta\\odot f'(z)$$ $$\\frac{\\partial L}{\\partial W}=\\left(\\frac{\\partial L}{\\partial z}\\right)x^\\top$$ $$\\frac{\\partial L}{\\partial x}=W^\\top\\left(\\frac{\\partial L}{\\partial z}\\right)$$ |\n",
        "| **Binary Cross-Entropy** | $$\\frac{\\partial L}{\\partial \\hat y}=\\frac{\\hat y - y}{\\hat y(1-\\hat y)}$$ |\n",
        "| **Softmax + Cross-Entropy** | $$\\frac{\\partial L}{\\partial z_i}=\\hat y_i - y_i$$ |\n",
        "| **Convolution (CNN)** | $$\\frac{\\partial L}{\\partial K_{m,n}}=\\sum_{i,j}\\frac{\\partial L}{\\partial Y_{i,j}}X_{i+m,j+n}$$ $$\\frac{\\partial L}{\\partial X_{i,j}}=\\sum_{m,n}\\frac{\\partial L}{\\partial Y_{i-m,j-n}}K_{m,n}$$ |\n",
        "| **RNN** | $$\\frac{\\partial L}{\\partial h_t}=\\left.\\frac{\\partial L}{\\partial h_t}\\right|_{\\text{local}}+W_h^\\top\\left(\\frac{\\partial L}{\\partial h_{t+1}}\\odot(1-h_{t+1}^2)\\right)$$ |\n",
        "| **LSTM — Cell** | $$\\frac{\\partial L}{\\partial c_t}=\\frac{\\partial L}{\\partial h_t}\\odot o_t\\odot(1-\\tanh^2(c_t))+\\frac{\\partial L}{\\partial c_{t+1}}\\odot f_{t+1}$$ |\n",
        "| **LSTM — Gates** | $$\\frac{\\partial L}{\\partial f_t}=\\frac{\\partial L}{\\partial c_t}c_{t-1}$$ $$\\frac{\\partial L}{\\partial i_t}=\\frac{\\partial L}{\\partial c_t}\\tilde c_t$$ $$\\frac{\\partial L}{\\partial \\tilde c_t}=\\frac{\\partial L}{\\partial c_t}i_t$$ $$\\frac{\\partial L}{\\partial o_t}=\\frac{\\partial L}{\\partial h_t}\\tanh(c_t)$$ |\n",
        "| **GRU — Update Gate** | $$\\frac{\\partial L}{\\partial z_t}=(\\tilde h_t - h_{t-1})\\odot\\frac{\\partial L}{\\partial h_t}$$ |\n",
        "| **GRU — Candidate State** | $$\\frac{\\partial L}{\\partial \\tilde h_t}=z_t\\odot\\frac{\\partial L}{\\partial h_t}$$ |\n",
        "| **GRU — Previous Hidden** | $$\\frac{\\partial L}{\\partial h_{t-1}}=(1-z_t)\\odot\\frac{\\partial L}{\\partial h_t}+r_t U_h^\\top\\left((1-\\tilde h_t^2)\\odot\\frac{\\partial L}{\\partial \\tilde h_t}\\right)$$ |\n",
        "| **Self-Attention — Value** | $$\\frac{\\partial L}{\\partial V}=A^\\top\\frac{\\partial L}{\\partial O}$$ |\n",
        "| **Self-Attention — Attention Weights** | $$\\frac{\\partial L}{\\partial A}=\\frac{\\partial L}{\\partial O}V^\\top$$ |\n",
        "| **Softmax (Jacobian)** | $$\\frac{\\partial A_i}{\\partial S_i}=\\mathrm{diag}(A_i)-A_iA_i^\\top$$ |\n",
        "| **Self-Attention — Query** | $$\\frac{\\partial L}{\\partial Q}=\\left(\\frac{\\partial L}{\\partial A}\\cdot K\\right)\\frac{1}{\\sqrt{d_k}}$$ |\n",
        "| **Self-Attention — Key** | $$\\frac{\\partial L}{\\partial K}=\\left(\\frac{\\partial L}{\\partial A}\\right)^\\top Q\\frac{1}{\\sqrt{d_k}}$$ |\n",
        "| **LayerNorm** | $$\\frac{\\partial L}{\\partial x_i}=\\frac{\\gamma}{\\sqrt{\\sigma^2+\\epsilon}}\\left[\\delta_i-\\frac{1}{n}\\sum_j\\delta_j-\\frac{(x_i-\\mu)}{\\sigma^2+\\epsilon}\\sum_j\\delta_j(x_j-\\mu)\\right]$$ |\n",
        "| **Residual Connection** | $$\\frac{\\partial L}{\\partial x}=\\frac{\\partial L}{\\partial y}+\\frac{\\partial L}{\\partial F(x)}\\frac{\\partial F(x)}{\\partial x}$$ |\n",
        "| **Attention Mask** | $$\\frac{\\partial L}{\\partial A_{ij}}=0\\quad \\text{(if masked)}$$ |\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6Qf1wg5A9CEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Backpropagation for All Generative AI Models**\n",
        "\n",
        "| **Model** | **Core Backpropagation Equation** |\n",
        "|-----------|------------------------------------|\n",
        "| **VAE** | $$\\nabla_\\theta \\mathcal{L} = \\mathbb{E}_{q_\\phi(z|x)}\\left[\\nabla_\\theta \\log p_\\theta(x|z)\\right]$$ $$\\nabla_\\phi \\mathcal{L} = \\nabla_z \\mathcal{L}\\,\\frac{\\partial z}{\\partial \\phi} - \\nabla_\\phi D_{KL}(q_\\phi(z|x)\\,\\|\\,p(z))$$ |\n",
        "| **VAE – KL Gradients** | $$\\frac{\\partial D_{KL}}{\\partial \\mu_i}=\\mu_i,\\qquad \\frac{\\partial D_{KL}}{\\partial \\sigma_i}=\\sigma_i-\\frac{1}{\\sigma_i}$$ |\n",
        "| **GAN – Discriminator** | $$\\nabla_{\\theta_D}L_D = -\\frac{\\nabla D(x)}{D(x)} + \\frac{\\nabla D(G(z))}{1-D(G(z))}$$ |\n",
        "| **GAN – Generator** | $$\\nabla_{\\theta_G}L_G = -\\frac{1}{D(G(z))}\\nabla_{\\theta_G}D(G(z))$$ $$\\nabla_{\\theta_G}D(G(z)) = \\nabla_x D(x)\\big|_{x=G(z)}\\cdot \\nabla_{\\theta_G}G(z)$$ |\n",
        "| **DDPM / Diffusion Models** | $$\\nabla_\\theta L = \\mathbb{E}\\left[2(\\epsilon_\\theta - \\epsilon)\\,\\nabla_\\theta \\epsilon_\\theta\\right]$$ $$\\frac{\\partial L}{\\partial x_t}=2(\\epsilon_\\theta-\\epsilon)\\frac{\\partial \\epsilon_\\theta}{\\partial x_t}$$ |\n",
        "| **Normalizing Flows** | $$\\nabla_\\theta \\log p(x)=\\sum_k\\left[-\\nabla_\\theta \\log\\lvert \\det J_k\\rvert + \\frac{\\partial z_{k-1}}{\\partial \\theta}\\nabla_{z_0}\\log p(z_0)\\right]$$ |\n",
        "| **Autoregressive Models** | $$L=-\\sum_t\\log p(x_t|x_{<t}),\\qquad \\frac{\\partial L}{\\partial z_t}=\\hat y_t - y_t$$ $$\\frac{\\partial L}{\\partial Q}=\\frac{1}{\\sqrt{d_k}}\\left(\\frac{\\partial L}{\\partial A}\\right)K$$ $$\\frac{\\partial L}{\\partial K}=\\frac{1}{\\sqrt{d_k}}\\left(\\frac{\\partial L}{\\partial A}\\right)^\\top Q$$ $$\\frac{\\partial L}{\\partial V}=A^\\top\\frac{\\partial L}{\\partial O}$$ |\n",
        "| **Energy-Based Models (EBM)** | $$\\nabla_\\theta \\log p(x) = -\\nabla_\\theta E_\\theta(x) + \\mathbb{E}_{p_\\theta}[\\nabla_\\theta E_\\theta(x)]$$ |\n",
        "| **RBM (Contrastive Divergence)** | $$\\frac{\\partial L}{\\partial W}=\\langle vh^\\top\\rangle_{\\text{data}} - \\langle vh^\\top\\rangle_{\\text{model}}$$ $$\\frac{\\partial L}{\\partial b}=\\langle v\\rangle_{\\text{data}} - \\langle v\\rangle_{\\text{model}}$$ |\n",
        "| **GMM (EM Algorithm)** | $$\\gamma_{nk}=\\frac{\\pi_k\\mathcal{N}(x_n|\\mu_k,\\Sigma_k)}{\\sum_j\\pi_j\\mathcal{N}(x_n|\\mu_j,\\Sigma_j)}$$ $$\\frac{\\partial L}{\\partial \\mu_k}=\\sum_n\\gamma_{nk}(x_n-\\mu_k)$$ $$\\frac{\\partial L}{\\partial \\Sigma_k}=\\sum_n\\gamma_{nk}\\left[\\Sigma_k^{-1}(x_n-\\mu_k)(x_n-\\mu_k)^\\top\\Sigma_k^{-1}-\\Sigma_k^{-1}\\right]$$ |\n",
        "| **Transformer Decoders (LLMs)** | $$\\frac{\\partial L}{\\partial z_t}=\\hat y_t - y_t$$ $$\\nabla_Q L=\\frac{1}{\\sqrt{d_k}}\\left(\\frac{\\partial L}{\\partial A}\\right)K,\\qquad \\nabla_K L=\\frac{1}{\\sqrt{d_k}}\\left(\\frac{\\partial L}{\\partial A}\\right)^\\top Q$$ $$\\nabla_V L=A^\\top\\nabla_O L$$ |\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "puUc63Uq-f8b"
      }
    }
  ]
}