{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyODJem9/AUhXJSrzAUNPnTY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ğŸ“œ Generative Algorithms in Deep Learning\n","\n","---\n","\n","## ğŸ”¹ Energy-Based Models (Precursors)\n","- **Boltzmann Machines** â€“ Ackley, Hinton & Sejnowski (1985)  \n","  *â€œA Learning Algorithm for Boltzmann Machines.â€* Cognitive Science, 1985.  \n","\n","- **Restricted Boltzmann Machines (RBM)** â€“ Smolensky (1986)  \n","  *â€œInformation Processing in Dynamical Systems: Foundations of Harmony Theory.â€*  \n","\n","---\n","\n","## ğŸ”¹ Autoencoders & Variants\n","- **Autoencoder (basic)** â€“ Rumelhart, Hinton & Williams (1986)  \n","  *â€œLearning Representations by Back-Propagating Errors.â€*  \n","\n","- **Deep Autoencoder** â€“ Hinton & Salakhutdinov (2006)  \n","  *â€œReducing the Dimensionality of Data with Neural Networks.â€* Science, 2006.  \n","\n","- **Variational Autoencoder (VAE)** â€“ Kingma & Welling (2013)  \n","  *â€œAuto-Encoding Variational Bayes.â€* arXiv:1312.6114.  \n","\n","- **Î²-VAE** â€“ Higgins et al. (2017, DeepMind)  \n","  *â€œbeta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework.â€*  \n","\n","- **VQ-VAE** â€“ van den Oord, Vinyals & Kavukcuoglu (2017)  \n","  *â€œNeural Discrete Representation Learning.â€*  \n","\n","---\n","\n","## ğŸ”¹ Autoregressive Models\n","- **Neural Probabilistic Language Model** â€“ Bengio et al. (2003)  \n","  *â€œA Neural Probabilistic Language Model.â€* JMLR, 2003.  \n","\n","- **NADE (Neural Autoregressive Distribution Estimator)** â€“ Larochelle & Murray (2011)  \n","  *â€œThe Neural Autoregressive Distribution Estimator.â€* AISTATS 2011.  \n","\n","- **PixelRNN / PixelCNN** â€“ van den Oord et al. (2016, DeepMind)  \n","  *â€œPixel Recurrent Neural Networks.â€* ICML 2016.  \n","  *â€œConditional Image Generation with PixelCNN Decoders.â€* NeurIPS 2016.  \n","\n","---\n","\n","## ğŸ”¹ GAN Family\n","- **GAN (Generative Adversarial Network)** â€“ Goodfellow et al. (2014)  \n","  *â€œGenerative Adversarial Nets.â€* NeurIPS 2014.  \n","\n","- **DCGAN (Deep Convolutional GAN)** â€“ Radford, Metz & Chintala (2015)  \n","  *â€œUnsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.â€*  \n","\n","- **WGAN (Wasserstein GAN)** â€“ Arjovsky, Chintala & Bottou (2017)  \n","  *â€œWasserstein GAN.â€* ICML 2017.  \n","\n","- **StyleGAN** â€“ Karras et al. (2018, NVIDIA)  \n","  *â€œA Style-Based Generator Architecture for Generative Adversarial Networks.â€* CVPR 2019.  \n","\n","---\n","\n","## ğŸ”¹ Flow-Based Models\n","- **NICE (Nonlinear Independent Components Estimation)** â€“ Dinh, Krueger & Bengio (2014)  \n","  *â€œNICE: Non-linear Independent Components Estimation.â€* arXiv:1410.8516.  \n","\n","- **Real NVP** â€“ Dinh, Sohl-Dickstein & Bengio (2016)  \n","  *â€œDensity Estimation using Real NVP.â€* ICLR 2017.  \n","\n","- **Glow** â€“ Kingma & Dhariwal (2018, OpenAI)  \n","  *â€œGlow: Generative Flow with Invertible 1Ã—1 Convolutions.â€* NeurIPS 2018.  \n","\n","---\n","\n","## ğŸ”¹ Diffusion Models\n","- **Diffusion Probabilistic Models (DPMs)** â€“ Sohl-Dickstein et al. (2015)  \n","  *â€œDeep Unsupervised Learning using Nonequilibrium Thermodynamics.â€* ICML 2015.  \n","\n","- **DDPM (Denoising Diffusion Probabilistic Models)** â€“ Ho, Jain & Abbeel (2020, Google Brain)  \n","  *â€œDenoising Diffusion Probabilistic Models.â€* NeurIPS 2020.  \n","\n","- **DDIM** â€“ Song, Meng & Ermon (2020)  \n","  *â€œDenoising Diffusion Implicit Models.â€* ICLR 2021.  \n","\n","- **Latent Diffusion / Stable Diffusion** â€“ Rombach et al. (2022)  \n","  *â€œHigh-Resolution Image Synthesis with Latent Diffusion Models.â€* CVPR 2022.  \n","\n","---\n","\n","## âœ… Summary\n","- **Autoencoders:** AE (1986) â†’ Deep AE (2006) â†’ VAE (2013) â†’ VQ-VAE (2017).  \n","- **Autoregressive:** Bengioâ€™s NLM (2003) â†’ NADE (2011) â†’ PixelRNN/PixelCNN (2016).  \n","- **GANs:** GAN (2014) â†’ DCGAN (2015) â†’ WGAN (2017) â†’ StyleGAN (2018).  \n","- **Flows:** NICE (2014) â†’ RealNVP (2016) â†’ Glow (2018).  \n","- **Diffusions:** DPM (2015) â†’ DDPM (2020) â†’ DDIM (2021) â†’ Stable Diffusion (2022).  \n"],"metadata":{"id":"JLfKnQYh2GuD"}},{"cell_type":"markdown","source":["# ğŸ“œ Generative Algorithms in Deep Learning\n","\n","| **Family** | **Model** | **Year** | **Authors** | **Paper** |\n","|------------|-----------|----------|-------------|-----------|\n","| ğŸ”¹ Energy-Based | Boltzmann Machines | 1985 | Ackley, Hinton & Sejnowski | *A Learning Algorithm for Boltzmann Machines* (Cognitive Science) |\n","| | Restricted Boltzmann Machines (RBM) | 1986 | Smolensky | *Foundations of Harmony Theory* |\n","| ğŸ”¹ Autoencoders | Autoencoder (basic) | 1986 | Rumelhart, Hinton & Williams | *Learning Representations by Back-Propagating Errors* |\n","| | Deep Autoencoder | 2006 | Hinton & Salakhutdinov | *Reducing the Dimensionality of Data with Neural Networks* (Science) |\n","| | Variational Autoencoder (VAE) | 2013 | Kingma & Welling | *Auto-Encoding Variational Bayes* |\n","| | Î²-VAE | 2017 | Higgins et al. (DeepMind) | *beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework* |\n","| | VQ-VAE | 2017 | van den Oord, Vinyals & Kavukcuoglu | *Neural Discrete Representation Learning* |\n","| ğŸ”¹ Autoregressive | Neural Probabilistic Language Model | 2003 | Bengio et al. | *A Neural Probabilistic Language Model* (JMLR) |\n","| | NADE (Neural Autoregressive Distribution Estimator) | 2011 | Larochelle & Murray | *The Neural Autoregressive Distribution Estimator* (AISTATS) |\n","| | PixelRNN | 2016 | van den Oord et al. (DeepMind) | *Pixel Recurrent Neural Networks* (ICML) |\n","| | PixelCNN | 2016 | van den Oord et al. (DeepMind) | *Conditional Image Generation with PixelCNN Decoders* (NeurIPS) |\n","| ğŸ”¹ GANs | GAN | 2014 | Goodfellow et al. | *Generative Adversarial Nets* (NeurIPS) |\n","| | DCGAN | 2015 | Radford, Metz & Chintala | *Unsupervised Representation Learning with DCGANs* |\n","| | WGAN | 2017 | Arjovsky, Chintala & Bottou | *Wasserstein GAN* (ICML) |\n","| | StyleGAN | 2018 | Karras et al. (NVIDIA) | *A Style-Based Generator Architecture for GANs* (CVPR 2019) |\n","| ğŸ”¹ Flow-Based | NICE | 2014 | Dinh, Krueger & Bengio | *NICE: Non-linear Independent Components Estimation* |\n","| | Real NVP | 2016 | Dinh, Sohl-Dickstein & Bengio | *Density Estimation using Real NVP* (ICLR 2017) |\n","| | Glow | 2018 | Kingma & Dhariwal (OpenAI) | *Glow: Generative Flow with Invertible 1Ã—1 Convolutions* (NeurIPS) |\n","| ğŸ”¹ Diffusions | DPM | 2015 | Sohl-Dickstein et al. | *Deep Unsupervised Learning using Nonequilibrium Thermodynamics* (ICML) |\n","| | DDPM | 2020 | Ho, Jain & Abbeel (Google Brain) | *Denoising Diffusion Probabilistic Models* (NeurIPS) |\n","| | DDIM | 2020 | Song, Meng & Ermon | *Denoising Diffusion Implicit Models* (ICLR 2021) |\n","| | LDM / Stable Diffusion | 2022 | Rombach et al. | *High-Resolution Image Synthesis with Latent Diffusion Models* (CVPR) |\n","\n","---\n","\n","## âœ… Summary\n","- **Autoencoders:** AE (1986) â†’ Deep AE (2006) â†’ VAE (2013) â†’ VQ-VAE (2017).  \n","- **Autoregressive:** NLM (2003) â†’ NADE (2011) â†’ PixelRNN/PixelCNN (2016).  \n","- **GANs:** GAN (2014) â†’ DCGAN (2015) â†’ WGAN (2017) â†’ StyleGAN (2018).  \n","- **Flows:** NICE (2014) â†’ RealNVP (2016) â†’ Glow (2018).  \n","- **Diffusions:** DPM (2015) â†’ DDPM (2020) â†’ DDIM (2021) â†’ LDM (2022).  \n"],"metadata":{"id":"djlvMqrN2Xmv"}}]}