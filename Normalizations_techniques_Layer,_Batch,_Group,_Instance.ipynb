{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# https://arxiv.org/pdf/1607.06450\n",
        "\n",
        "# https://arxiv.org/pdf/1502.03167\n",
        "\n",
        "# https://arxiv.org/pdf/1803.08494\n",
        "\n",
        "# https://arxiv.org/pdf/1607.08022\n"
      ],
      "metadata": {
        "id": "Hpm-qrL5cKFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Normalization Techniques — Comprehensive Comparison\n",
        "\n",
        "## 1. Comparative Overview\n",
        "\n",
        "| Aspect | Batch Normalization (BN) | Layer Normalization (LN) | Group Normalization (GN) |\n",
        "|----------|--------------------------|---------------------------|----------------------------|\n",
        "| **Original Authors** | Sergey Ioffe, Christian Szegedy (2015) | Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey Hinton (2016) | Yuxin Wu, Kaiming He (2018) |\n",
        "| **Normalization Axis** | Across batch dimension (per channel) | Across feature dimension within one sample | Across groups of channels within one sample |\n",
        "| **Statistics Computed Over** | Mini-batch samples | All features of a layer (single example) | Subsets of channels (G groups) per example |\n",
        "| **Depends on Batch Size?** | Yes | No | No |\n",
        "| **Stable with Small Batches?** | Often unstable | Yes | Yes |\n",
        "| **Training vs Inference Behavior** | Different (uses running averages at inference) | Identical | Identical |\n",
        "| **Uses Running Statistics?** | Yes | No | No |\n",
        "| **Sensitivity to Batch Size** | High | None | None |\n",
        "| **Internal Covariate Shift Reduction** | Explicitly proposed to reduce it | Does not rely on batch statistics | Not framed as ICS reduction |\n",
        "| **Learning Rate Scaling** | Enables significantly higher LR | Moderately stabilizes | Stabilizes without LR dependence on batch |\n",
        "| **Regularization Effect** | Implicit regularization from batch noise | Minimal implicit regularization | Minimal implicit regularization |\n",
        "| **Works Well For** | Large-scale CNN image classification | RNNs, Transformers, sequence models | Vision tasks with small batches |\n",
        "| **Not Ideal For** | Small-batch training, variable batch sizes | Very deep CNNs without tuning | Extremely small channel counts |\n",
        "| **Computational Cost** | Slightly higher due to batch aggregation | Efficient | Efficient |\n",
        "| **Memory Efficiency** | Good | Good | Good |\n",
        "| **Parallelization Across Devices** | Requires sync (SyncBN) for multi-GPU | Naturally independent | Naturally independent |\n",
        "| **Sensitivity to Distributed Training** | High (needs synchronized stats) | None | None |\n",
        "| **Works with Variable Sequence Length?** | Difficult | Excellent | Good |\n",
        "| **Typical Placement** | After Conv / Linear, before activation | Inside residual blocks, before activation | Inside residual Conv blocks |\n",
        "| **Affine Parameters ($\\gamma$, $\\beta$)** | Yes | Yes | Yes |\n",
        "| **Effect on Gradient Flow** | Smooths optimization landscape | Stabilizes hidden state dynamics | Stabilizes channel-wise feature scaling |\n",
        "| **Effect on Feature Scaling** | Normalizes across samples | Normalizes across features | Normalizes structured channel subsets |\n",
        "| **Interpretation** | Dataset-level normalization per step | Instance-level normalization | Structured instance normalization |\n",
        "| **Relationship to InstanceNorm** | Different (uses batch stats) | Different | GN with $G=C$ equals InstanceNorm |\n",
        "| **Relationship to LN** | Different axis | GN with $G=1$ equals LN | Interpolates between LN and InstanceNorm |\n",
        "| **Best in CNNs?** | Yes (large batch) | Moderate | Yes (small batch) |\n",
        "| **Best in Transformers?** | No | Yes (standard choice) | Rare |\n",
        "| **Used in ResNet (original)?** | Yes | No | Modern variants |\n",
        "| **Used in GPT/BERT?** | No | Yes | No |\n",
        "| **Used in Diffusion U-Nets?** | Rare | Rare | Very common |\n",
        "| **Why Used in Diffusion Models?** | Batch instability issue | Less aligned with Conv channel grouping | Stable with small batch, natural for Conv residual blocks |\n",
        "| **Conditioning Compatibility (e.g., time embedding scaling)** | Possible but less standard | Possible | Very natural (scale/shift modulation after GN) |\n",
        "| **Behavior Under Batch Size = 1** | Fails (degenerates) | Stable | Stable |\n",
        "| **Stochasticity Source** | Mini-batch variance | None | None |\n",
        "| **Impact on Generalization** | Often improves due to noise | Neutral | Neutral |\n",
        "| **Mathematical Core** | $\\mu_B, \\sigma_B^2$ over batch | $\\mu_L, \\sigma_L^2$ over features | $\\mu_G, \\sigma_G^2$ over grouped channels |\n",
        "| **Robustness to Dataset Distribution Shift** | Can be sensitive | More stable | More stable |\n",
        "| **Primary Theoretical Motivation** | Reduce internal covariate shift | Stabilize hidden state transitions | Decouple normalization from batch size |\n",
        "| **Primary Practical Advantage** | Fast convergence | Works with sequence models | Stable small-batch vision training |\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Mathematical Formulation\n",
        "\n",
        "### Batch Normalization\n",
        "\n",
        "For a mini-batch of size $m$:\n",
        "\n",
        "$$\n",
        "\\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "y_i = \\gamma \\hat{x}_i + \\beta\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Layer Normalization\n",
        "\n",
        "For a single example with $d$ features:\n",
        "\n",
        "$$\n",
        "\\mu_L = \\frac{1}{d} \\sum_{j=1}^{d} x_j\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma_L^2 = \\frac{1}{d} \\sum_{j=1}^{d} (x_j - \\mu_L)^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{x}_j = \\frac{x_j - \\mu_L}{\\sqrt{\\sigma_L^2 + \\epsilon}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "y_j = \\gamma \\hat{x}_j + \\beta\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Group Normalization\n",
        "\n",
        "For each group containing $\\frac{C}{G}$ channels:\n",
        "\n",
        "$$\n",
        "\\mu_G = \\frac{1}{|S_G|} \\sum_{k \\in S_G} x_k\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma_G^2 = \\frac{1}{|S_G|} \\sum_{k \\in S_G} (x_k - \\mu_G)^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{x}_k = \\frac{x_k - \\mu_G}{\\sqrt{\\sigma_G^2 + \\epsilon}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "y_k = \\gamma \\hat{x}_k + \\beta\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Key Conceptual Differences\n",
        "\n",
        "### 3.1 Where the Statistics Come From\n",
        "\n",
        "- **BN** → across examples  \n",
        "- **LN** → across features  \n",
        "- **GN** → across feature groups  \n",
        "\n",
        "---\n",
        "\n",
        "### 3.2 Batch Size Dependency Spectrum\n",
        "\n",
        "$$\n",
        "\\text{BatchNorm} \\quad \\longrightarrow \\quad \\text{depends on batch}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{GroupNorm} \\quad \\longrightarrow \\quad \\text{independent}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{LayerNorm} \\quad \\longrightarrow \\quad \\text{independent}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 3.3 GroupNorm as a Continuum\n",
        "\n",
        "| Group Count ($G$) | Equivalent To |\n",
        "|-------------------|--------------|\n",
        "| $G = 1$ | LayerNorm |\n",
        "| $G = C$ | InstanceNorm |\n",
        "| $1 < G < C$ | GroupNorm (intermediate behavior) |\n",
        "\n",
        "Group Normalization forms a structural continuum between LayerNorm and InstanceNorm, making it particularly suitable for convolutional architectures.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Why Diffusion Models Prefer GroupNorm\n",
        "\n",
        "Diffusion U-Nets typically:\n",
        "\n",
        "- Use small per-GPU batch sizes  \n",
        "- Are memory intensive (high-resolution images)  \n",
        "- Use residual convolutional blocks  \n",
        "- Inject conditioning (time embeddings) via scale/shift after normalization  \n",
        "\n",
        "Group Normalization:\n",
        "\n",
        "- Does not depend on batch statistics  \n",
        "- Aligns naturally with convolutional channel grouping  \n",
        "- Remains stable under batch size $=1$  \n",
        "- Integrates cleanly with FiLM-style conditioning  \n",
        "\n",
        "This makes GroupNorm the most practical normalization choice in diffusion architectures.\n"
      ],
      "metadata": {
        "id": "F1nzkZ9vbqio"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalization Techniques — Concise Comparative Table\n",
        "\n",
        "## Comparative Summary\n",
        "\n",
        "| Aspect | Batch Norm (BN) | Layer Norm (LN) | Group Norm (GN) | Instance Norm (IN) |\n",
        "|----------|----------------|----------------|----------------|----------------|\n",
        "| **Core Definition** | Normalizes activations using mini-batch mean and variance per channel | Normalizes summed inputs using all units in a layer per sample | Normalizes features within channel groups per sample | Normalizes each channel independently per sample |\n",
        "| **How It Works** | Compute $\\mu_B, \\sigma_B$ over batch → normalize → apply learnable $\\gamma, \\beta$ | Compute $\\mu, \\sigma$ over hidden units of a layer (single example) → apply $\\gamma, \\beta$ | Split $C$ channels into $G$ groups → compute $\\mu, \\sigma$ per group → apply $\\gamma, \\beta$ | Compute $\\mu, \\sigma$ per channel per image (spatial only) → apply $\\gamma, \\beta$ |\n",
        "| **Normalization Axis** | $(N, H, W)$ per channel | $(C, H, W)$ per sample | $(C/G, H, W)$ per group per sample | $(H, W)$ per channel per sample |\n",
        "| **Batch Dependency** | Yes | No | No | No |\n",
        "| **Training vs Test** | Uses running averages at test | Same computation at train and test | Same computation at train and test | Applied at test time as well |\n",
        "| **Primary Motivation** | Reduce internal covariate shift | Stabilize hidden state dynamics (especially in RNNs) | Remove batch-size limitation of BN | Remove instance-specific contrast for stylization |\n",
        "| **Positive Effects** | Enables higher learning rates; faster convergence; acts as regularizer | Stable hidden dynamics; works for RNNs; no mini-batch constraint | Stable across small batches; transfers well to detection and segmentation | Improved stylization quality; removes contrast artifacts |\n",
        "| **Main Use Cases (per paper)** | Image classification CNNs | RNNs, sequence models | Vision tasks with small batch (detection, segmentation, video) | Fast neural style transfer |\n",
        "| **Key Limitation** | Performance degrades with small batch sizes | Less effective than BN for some CNN tasks | Requires choosing group number $G$ | Ignores channel dependency; tailored for generation |\n",
        "| **Invariance Properties** | Invariant to weight rescaling; partially to data shifts | Invariant to weight matrix rescaling and shifting | Independent of batch size | Instance-wise contrast invariance |\n",
        "| **Extremes Relationship** | — | Equivalent to GN when $G=1$ | $G=1$ → LN; $G=C$ → IN | Equivalent to GN when $G=C$ |\n",
        "\n",
        "---\n",
        "\n",
        "## Mathematical Formulations\n",
        "\n",
        "### Batch Normalization\n",
        "\n",
        "For mini-batch size $m$:\n",
        "\n",
        "$$\n",
        "\\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "y_i = \\gamma \\hat{x}_i + \\beta\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Layer Normalization\n",
        "\n",
        "For feature dimension $d$ within one sample:\n",
        "\n",
        "$$\n",
        "\\mu_L = \\frac{1}{d} \\sum_{j=1}^{d} x_j\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma_L^2 = \\frac{1}{d} \\sum_{j=1}^{d} (x_j - \\mu_L)^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{x}_j = \\frac{x_j - \\mu_L}{\\sqrt{\\sigma_L^2 + \\epsilon}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "y_j = \\gamma \\hat{x}_j + \\beta\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Group Normalization\n",
        "\n",
        "For each group containing $\\frac{C}{G}$ channels:\n",
        "\n",
        "$$\n",
        "\\mu_G = \\frac{1}{|S_G|} \\sum_{k \\in S_G} x_k\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma_G^2 = \\frac{1}{|S_G|} \\sum_{k \\in S_G} (x_k - \\mu_G)^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{x}_k = \\frac{x_k - \\mu_G}{\\sqrt{\\sigma_G^2 + \\epsilon}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "y_k = \\gamma \\hat{x}_k + \\beta\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Instance Normalization\n",
        "\n",
        "For each channel independently within one sample:\n",
        "\n",
        "$$\n",
        "\\mu_{IN} = \\frac{1}{HW} \\sum_{h,w} x_{h,w}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma_{IN}^2 = \\frac{1}{HW} \\sum_{h,w} (x_{h,w} - \\mu_{IN})^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{x}_{h,w} = \\frac{x_{h,w} - \\mu_{IN}}{\\sqrt{\\sigma_{IN}^2 + \\epsilon}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "y_{h,w} = \\gamma \\hat{x}_{h,w} + \\beta\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Conceptual Summary\n",
        "\n",
        "### Batch Normalization\n",
        "\n",
        "Stabilizes deep convolutional network training by normalizing batch statistics.  \n",
        "Introduces stochastic regularization through mini-batch noise.  \n",
        "Performance is sensitive to batch size.\n",
        "\n",
        "### Layer Normalization\n",
        "\n",
        "Removes batch dependence entirely.  \n",
        "Particularly effective for sequential and recurrent models.  \n",
        "Identical behavior during training and inference.\n",
        "\n",
        "### Group Normalization\n",
        "\n",
        "Generalizes Layer Normalization and Instance Normalization.  \n",
        "Designed to eliminate the small-batch instability of Batch Normalization.  \n",
        "Strong performance in detection, segmentation, and video tasks.\n",
        "\n",
        "### Instance Normalization\n",
        "\n",
        "Performs per-image contrast normalization.  \n",
        "Improves stylization quality in generative models.  \n",
        "Applied consistently during both training and inference.\n"
      ],
      "metadata": {
        "id": "Oz4rpxZydajI"
      }
    }
  ]
}