{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOchsgd27EmDEmWY7PfEDCz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["ğŸ“œ Unsupervised / Representation Learning Breakthroughs\n","\n","ğŸ”¹ Foundational Ideas\n","\n","Self-Organizing Maps (SOMs) â€“ Kohonen (1982)\n","â€œSelf-Organized Formation of Topologically Correct Feature Maps.â€\n","â Competitive learning algorithm mapping high-dimensional data onto a 2D grid; useful for clustering and visualization.\n","\n","Independent Component Analysis (ICA) / Sparse Coding â€“ Comon (1994), Olshausen & Field (1996)\n","â€œEmergence of Simple-Cell Receptive Field Properties by Learning a Sparse Code for Natural Images.â€ (Nature 1996).\n","â Learned sparse and independent features resembling visual cortex responses.\n","\n","ğŸ”¹ Neural Feature Learning (2000sâ€“2010s)\n","\n","Autoencoders â€“ Rumelhart et al. (1986); extended in deep learning era.\n","\n","Stacked Autoencoders & RBMs â€“ Hinton & Salakhutdinov (2006).\n","â First large-scale unsupervised pretraining, foundational to deep learning revival.\n","\n","Dictionary Learning / Sparse Representations â€“ used in image denoising, compression.\n","\n","ğŸ”¹ Modern Deep Representation Learning\n","\n","Contrastive Learning:\n","\n","SimCLR â€“ Chen et al. (2020, Google Brain)\n","â€œA Simple Framework for Contrastive Learning of Visual Representations.â€\n","â Positive vs. negative pair training with data augmentations.\n","\n","MoCo (Momentum Contrast) â€“ He et al. (2020, Facebook AI)\n","â€œMomentum Contrast for Unsupervised Visual Representation Learning.â€\n","â Memory bank + momentum encoder for scalable contrastive SSL.\n","\n","BYOL (Bootstrap Your Own Latent) â€“ Grill et al. (2020, DeepMind)\n","â Contrastive-like training without negatives; teacherâ€“student setup.\n","\n","SwAV (Swapping Assignments between Views) â€“ Caron et al. (2020, Facebook AI)\n","â Online clustering + contrastive SSL, competitive with supervised ImageNet training.\n","\n","âœ… Summary Families\n","\n","Neuro-inspired early methods: Self-Organizing Maps (1982), Sparse Coding/ICA (1990s).\n","\n","Deep unsupervised pretraining: Autoencoders, RBMs, DBNs (2006).\n","\n","Modern contrastive SSL: SimCLR, MoCo, BYOL, SwAV (2020).\n","\n","ğŸ‘‰ Why it matters:\n","Representation learning provides the latent features that power:\n","\n","Generative models (VAEs, GANs, Diffusion).\n","\n","Foundation models in NLP (BERT embeddings, GPT pretraining).\n","\n","Vision models (SimCLR, MAE, SwAV).\n","\n","Multimodal learning (CLIP, DALLÂ·E, PaLM-E)."],"metadata":{"id":"7qCIoU_VQbvE"}}]}