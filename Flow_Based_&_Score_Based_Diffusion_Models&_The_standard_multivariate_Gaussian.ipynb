{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Flow-Based Generative Models (Normalizing Flows)\n",
        "\n",
        "Flow-based models learn an **invertible mapping** between:\n",
        "\n",
        "- a simple base distribution (Gaussian)  \n",
        "- a complex data distribution  \n",
        "\n",
        "using invertible transformations.\n",
        "\n",
        "---\n",
        "\n",
        "## 1.1 Invertible Transformation\n",
        "\n",
        "$$\n",
        "x = f_{\\theta}(z), \\qquad z = f_{\\theta}^{-1}(x)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- \\(z\\) is the latent variable (e.g., \\(N(0,I)\\))  \n",
        "- \\(x\\) is data  \n",
        "- \\(f_{\\theta}\\) is invertible and differentiable  \n",
        "\n",
        "---\n",
        "\n",
        "## 1.2 Change of Variables Formula (Core of Flow Models)\n",
        "\n",
        "This is the central equation of flows:\n",
        "\n",
        "$$\n",
        "p_X(x) = p_Z(z)\\,\\left|\\det\\frac{\\partial f_{\\theta}^{-1}(x)}{\\partial x}\\right|\n",
        "$$\n",
        "\n",
        "Equivalently:\n",
        "\n",
        "$$\n",
        "p_X(x) = p_Z(f_{\\theta}^{-1}(x))\\,\n",
        "\\left|\\det J_{f_{\\theta}^{-1}}(x)\\right|\n",
        "$$\n",
        "\n",
        "Where \\(J\\) is the Jacobian matrix.\n",
        "\n",
        "---\n",
        "\n",
        "## 1.3 Log-Likelihood of Flow Models\n",
        "\n",
        "$$\n",
        "\\log p_X(x) = \\log p_Z(z) + \\log\\left|\\det J_{f_{\\theta}^{-1}}(x)\\right|\n",
        "$$\n",
        "\n",
        "Training is by maximum log-likelihood:\n",
        "\n",
        "$$\n",
        "\\max_{\\theta}\\sum_n \\log p_X(x^{(n)})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 1.4 Composition of Flows\n",
        "\n",
        "Flows compose multiple invertible transformations:\n",
        "\n",
        "$$\n",
        "f_{\\theta} = f_K \\circ f_{K-1} \\circ \\cdots \\circ f_1\n",
        "$$\n",
        "\n",
        "Jacobian log-determinant becomes:\n",
        "\n",
        "$$\n",
        "\\log\\left|\\det J_{f_{\\theta}^{-1}}\\right|\n",
        "= \\sum_{k=1}^{K} \\log\\left|\\det J_{f_k^{-1}}\\right|\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 1.5 Sampling\n",
        "\n",
        "Sample \\(z \\sim N(0,I)\\), then transform:\n",
        "\n",
        "$$\n",
        "x = f_{\\theta}(z)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "# 2. Score-Based Diffusion Models (Score Matching + SDEs)\n",
        "\n",
        "Score-based models learn the **score**:\n",
        "\n",
        "$$\n",
        "s_{\\theta}(x,t) = \\nabla_x \\log p_t(x)\n",
        "$$\n",
        "\n",
        "which is the gradient of the log-density.\n",
        "\n",
        "They combine:\n",
        "\n",
        "- forward diffusion (noise addition)  \n",
        "- reverse-time SDE (denoising)  \n",
        "\n",
        "---\n",
        "\n",
        "# 2.1 Forward Diffusion (Discrete)\n",
        "\n",
        "$$\n",
        "q(x_t \\mid x_{t-1}) =\n",
        "N(x_t; \\sqrt{1-\\beta_t}\\,x_{t-1},\\,\\beta_t I)\n",
        "$$\n",
        "\n",
        "Closed form:\n",
        "\n",
        "$$\n",
        "x_t = \\sqrt{\\bar{\\alpha}_t}\\,x_0 +\n",
        "\\sqrt{1-\\bar{\\alpha}_t}\\,\\epsilon,\\quad \\epsilon\\sim N(0,I)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$$\n",
        "\\bar{\\alpha}_t = \\prod_{s=1}^t (1 - \\beta_s)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "# 2.2 Forward SDE\n",
        "\n",
        "Variance-exploding:\n",
        "\n",
        "$$\n",
        "dx = g(t)\\,dw\n",
        "$$\n",
        "\n",
        "Variance-preserving:\n",
        "\n",
        "$$\n",
        "dx = -\\tfrac12 \\beta(t)\\,x\\,dt + \\beta(t)\\,dw\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "# 3. Score Function\n",
        "\n",
        "$$\n",
        "s(x,t)= \\nabla_x \\log p_t(x)\n",
        "$$\n",
        "\n",
        "The model learns:\n",
        "\n",
        "$$\n",
        "s_{\\theta}(x,t) \\approx \\nabla_x \\log p_t(x)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "# 4. Score-Matching Loss\n",
        "\n",
        "$$\n",
        "L(\\theta) =\n",
        "\\mathbb{E}_{t,x_t,x_0}\n",
        "\\left[\n",
        "\\lambda(t)\n",
        "\\left\\|\n",
        "s_{\\theta}(x_t,t) -\n",
        "\\nabla_x \\log q(x_t\\mid x_0)\n",
        "\\right\\|^2\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "For Gaussian forward noise:\n",
        "\n",
        "$$\n",
        "\\nabla_x \\log q(x_t\\mid x_0)\n",
        "=\n",
        "-\\frac{x_t - \\sqrt{\\bar{\\alpha}_t}\\,x_0}\n",
        "{\\sqrt{1-\\bar{\\alpha}_t}}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "# 5. Reverse-Time SDE (Key Sampling Equation)\n",
        "\n",
        "$$\n",
        "dx =\n",
        "\\left[\n",
        "f(x,t)\n",
        "-\n",
        "\\frac{g(t)^2}{2}\n",
        "\\nabla_x \\log p_t(x)\n",
        "\\right] dt\n",
        "+\n",
        "g(t)\\,d\\bar{w}\n",
        "$$\n",
        "\n",
        "Replace score with neural network:\n",
        "\n",
        "$$\n",
        "dx =\n",
        "\\left[\n",
        "f(x,t)\n",
        "-\n",
        "\\frac{g(t)^2}{2}\n",
        "\\,s_{\\theta}(x,t)\n",
        "\\right] dt\n",
        "+\n",
        "g(t)\\,d\\bar{w}\n",
        "$$\n",
        "\n",
        "This SDE generates data samples.\n",
        "\n",
        "---\n",
        "\n",
        "# 6. Probability-Flow ODE (Deterministic Sampling)\n",
        "\n",
        "$$\n",
        "\\frac{dx}{dt}\n",
        "=\n",
        "f(x,t)\n",
        "-\n",
        "\\frac{g(t)^2}{2}\n",
        "s_{\\theta}(x,t)\n",
        "$$\n",
        "\n",
        "Used in high-quality deterministic samplers.\n",
        "\n",
        "---\n",
        "\n",
        "# 7. DDPM Sampling (Discrete Reverse Diffusion)\n",
        "\n",
        "$$\n",
        "x_{t-1}\n",
        "=\n",
        "\\frac{1}{\\sqrt{\\alpha_t}}\n",
        "\\left(\n",
        "x_t\n",
        "-\n",
        "\\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\n",
        "\\epsilon_{\\theta}(x_t,t)\n",
        "\\right)\n",
        "+\n",
        "\\sigma_t z,\n",
        "\\qquad z\\sim N(0,I)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- \\(\\epsilon_{\\theta}(x_t,t)\\) is the noise predictor  \n",
        "- \\(\\sigma_t\\) controls sampling noise  \n",
        "\n",
        "---\n",
        "\n",
        "# Summary Table — Flow vs Score Models\n",
        "\n",
        "| Concept | Flow Models | Score/Diffusion Models |\n",
        "|--------|-------------|------------------------|\n",
        "| Main object | invertible mapping \\(f_{\\theta}(x)\\) | score \\(s(x,t)=\\nabla \\log p_t(x)\\) |\n",
        "| Training | max log-likelihood | score matching |\n",
        "| Generative process | invertible transform | reverse diffusion / SDE |\n",
        "| Probability | exact density via Jacobian | implicit density via score |\n",
        "| Sampling | one-shot \\(x=f(z)\\) | iterative denoising |\n",
        "| Base distribution | explicit \\(N(0,I)\\) | implicit noise schedule |\n",
        "| Key formula | change of variables | reverse SDE |\n",
        "\n"
      ],
      "metadata": {
        "id": "Adi8RsK1tuxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gaussian Distributions — Full Formulas Using Euler’s Number \\(e\\)\n",
        "\n",
        "This cell contains **all formulas exactly as written**, using **\\(e\\)** explicitly, with **no icons**.\n",
        "\n",
        "---\n",
        "\n",
        "# 1. Univariate Gaussian (1-D Normal Distribution)\n",
        "\n",
        "The probability density of a single real variable \\(x\\):\n",
        "\n",
        "$$\n",
        "p(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\; e^{-\\frac{(x-\\mu)^2}{2\\sigma^{2}}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- \\(\\mu\\) = mean  \n",
        "- \\(\\sigma^2\\) = variance  \n",
        "- \\(\\sigma\\) = standard deviation  \n",
        "- \\(e = 2.718281828459\\ldots\\) is Euler’s number  \n",
        "\n",
        "---\n",
        "\n",
        "# 2. Multivariate Gaussian (d-Dimensional Normal Distribution)\n",
        "\n",
        "Used extensively in deep learning (VAEs, diffusion models, flows):\n",
        "\n",
        "$$\n",
        "p(x)=\\frac{1}{(2\\pi)^{d/2}\\,|\\Sigma|^{1/2}}\\;\n",
        "e^{-\\frac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- \\(x \\in \\mathbb{R}^d\\)  \n",
        "- \\(\\mu \\in \\mathbb{R}^d\\) (mean vector)  \n",
        "- \\(\\Sigma \\in \\mathbb{R}^{d\\times d}\\) (covariance matrix)  \n",
        "- \\(|\\Sigma|\\) = determinant  \n",
        "- \\(\\Sigma^{-1}\\) = inverse covariance  \n",
        "\n",
        "---\n",
        "\n",
        "# 3. Standard Multivariate Gaussian — The AI Favorite\n",
        "\n",
        "This is the exact distribution used in:\n",
        "\n",
        "- diffusion models  \n",
        "- VAEs  \n",
        "- normalizing flows  \n",
        "- energy-based models  \n",
        "- weight initialization  \n",
        "\n",
        "Assume:\n",
        "\n",
        "- mean \\(= 0\\)  \n",
        "- covariance \\(= I\\)  \n",
        "\n",
        "Then:\n",
        "\n",
        "$$\n",
        "p(x)=\\frac{1}{(2\\pi)^{d/2}}\\;\n",
        "e^{-\\frac{1}{2}\\|x\\|^{2}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$$\n",
        "\\|x\\|^{2}=x^{T}x\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "# Optional Next Steps (You may ask for any of them)\n",
        "\n",
        "- Derivation of the Gaussian from first principles  \n",
        "- Why the exponent is quadratic  \n",
        "- How the Gaussian becomes the energy function in EBMs  \n",
        "- Why diffusion noise is always Gaussian  \n",
        "- Relationship to KL divergence and entropy  \n",
        "- Geometric meaning of \\(\\Sigma\\) and Mahalanobis distance  \n",
        "\n"
      ],
      "metadata": {
        "id": "GbIQqjWjt7yw"
      }
    }
  ]
}