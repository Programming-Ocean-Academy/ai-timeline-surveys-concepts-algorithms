{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOPBAcgGzcacW+haAvPyObx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#  Transformer Variants in Deep Learning\n","\n","---\n","\n","##  Foundation & Core Transformer\n","- **Transformer (original)** ‚Äì Vaswani et al. (2017)  \n","  *‚ÄúAttention Is All You Need.‚Äù* NeurIPS 2017.  \n","\n","---\n","\n","##  Transformer Families\n","\n","| **Category** | **Models** | **Authors & Year** | **Key Contributions** |\n","|--------------|------------|--------------------|-----------------------|\n","| **Encoder-only (Masked LM)** | **BERT** ‚Äì Devlin et al. (2018, Google AI) | *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.* | Bidirectional masked LM pretraining. |\n","| | **RoBERTa** ‚Äì Liu et al. (2019) | *Robustly Optimized BERT Pretraining Approach.* | More training, larger data, no NSP task. |\n","| | **ALBERT** ‚Äì Lan et al. (2019) | *A Lite BERT.* | Weight sharing + Sentence Order Prediction. |\n","| | **ELECTRA** ‚Äì Clark et al. (2020) | *Pre-training Text Encoders as Discriminators Rather Than Generators.* | Replaces MLM with Replaced Token Detection. |\n","| **Decoder-only (Autoregressive)** | **GPT** ‚Äì Radford et al. (2018, OpenAI) | *Improving Language Understanding by Generative Pre-Training.* | Left-to-right autoregressive Transformer. |\n","| | **GPT-2** ‚Äì Radford et al. (2019) | *Language Models are Unsupervised Multitask Learners.* | Large-scale autoregressive model, strong text generation. |\n","| | **GPT-3** ‚Äì Brown et al. (2020) | *Language Models are Few-Shot Learners.* | 175B parameters, few-shot learning ability. |\n","| **Encoder‚ÄìDecoder (‚ÄúText-to-Text‚Äù)** | **T5** ‚Äì Raffel et al. (2020, Google) | *Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.* | Unified text-to-text framework. |\n","| | **Switch Transformer, ByT5, UL2, Flan-T5, Pile-T5** (2021‚Äì2024) | Google & others | Scaling, byte-level processing, improved finetuning and transfer learning. |\n","| **Vision & Multimodal** | **ViT** ‚Äì Dosovitskiy et al. (2021) | *An Image is Worth 16√ó16 Words: Transformers for Image Recognition at Scale.* | First pure-Transformer vision model. |\n","| | **Swin Transformer** ‚Äì Liu et al. (2022, Microsoft) | *Swin Transformer V2.* | Hierarchical, scalable to high-res vision tasks. |\n","| | **Perceiver / Perceiver IO** ‚Äì Jaegle et al. (2021, DeepMind) | *General Perception with Iterative Attention.* | Multimodal, data-agnostic, bottleneck attention. |\n","\n","---\n","\n","##  Summary Table\n","\n","| **Category** | **Example Models** |\n","|--------------|---------------------|\n","| Encoder-only | BERT, RoBERTa, ALBERT, ELECTRA |\n","| Decoder-only (Autoregressive) | GPT, GPT-2, GPT-3 |\n","| Encoder‚ÄìDecoder | T5 (+ Switch, ByT5, UL2, Flan-T5, Pile-T5) |\n","| Vision / Multimodal | ViT, Swin Transformer, Perceiver (and IO) |\n","\n","---\n","\n","##  Research Sources\n","- Lin et al. (2021). *A Survey of Transformers.* arXiv.  \n","- **Vision Transformers (ViT, Swin)** ‚Äì arXiv, Wikipedia.  \n","- **Perceiver / Perceiver IO** ‚Äì DeepMind papers, Wikipedia.  \n"],"metadata":{"id":"FivtB5hV31B2"}},{"cell_type":"markdown","source":["#  Transformer-Based Models in Deep Learning (2017‚Äì2025)\n","\n","---\n","\n","## üîπ Foundation\n","\n","| **Model** | **Year** | **Authors / Org** | **Key Idea** |\n","|-----------|----------|-------------------|--------------|\n","| **Transformer** | 2017 | Vaswani et al., Google Brain | *Attention Is All You Need.* Introduced self-attention + encoder‚Äìdecoder architecture, eliminated recurrence. |\n","\n","---\n","\n","## üîπ Encoder-Only Models (Bidirectional, Masked LM)\n","\n","| **Model** | **Year** | **Authors / Org** | **Key Idea** |\n","|-----------|----------|-------------------|--------------|\n","| **BERT** | 2018 | Devlin et al., Google AI | Bidirectional masked LM, pre-training + fine-tuning paradigm. |\n","| **RoBERTa** | 2019 | Liu et al., Facebook AI | Optimized BERT training: more data, larger batches, no NSP task. |\n","| **ALBERT** | 2019 | Lan et al., Google & Toyota Research | Lightweight BERT with parameter sharing + SOP objective. |\n","| **ELECTRA** | 2020 | Clark et al., Google Research | Pre-training with Replaced Token Detection (discriminator-based). |\n","| **DeBERTa** | 2020 | He et al., Microsoft | Disentangled attention + improved position encoding. |\n","\n","---\n","\n","## üîπ Decoder-Only Models (Autoregressive, Generative)\n","\n","| **Model** | **Year** | **Authors / Org** | **Key Idea** |\n","|-----------|----------|-------------------|--------------|\n","| **GPT** | 2018 | Radford et al., OpenAI | Generative pretraining with left-to-right autoregressive Transformer. |\n","| **GPT-2** | 2019 | Radford et al., OpenAI | Larger GPT, strong unsupervised text generation. |\n","| **GPT-3** | 2020 | Brown et al., OpenAI | *Language Models are Few-Shot Learners.* 175B parameters. |\n","| **GPT-4** | 2023 | OpenAI | Multimodal Transformer, advanced reasoning, strong alignment. |\n","\n","---\n","\n","## üîπ Encoder‚ÄìDecoder Models (Text-to-Text Framework)\n","\n","| **Model** | **Year** | **Authors / Org** | **Key Idea** |\n","|-----------|----------|-------------------|--------------|\n","| **T5** | 2020 | Raffel et al., Google | Unified text-to-text framework across tasks. |\n","| **Switch Transformer** | 2021 | Fedus et al., Google | Mixture-of-Experts scaling ‚Üí trillion-parameter models. |\n","| **ByT5** | 2021 | Xue et al., Google | Byte-level input/output Transformer. |\n","| **UL2** | 2022 | Tay et al., Google | Unified pretraining across multiple objectives. |\n","| **Flan-T5 / Flan-PaLM** | 2022 | Chung et al., Google | Instruction tuning for better zero-shot generalization. |\n","\n","---\n","\n","## üîπ Vision & Multimodal Transformers\n","\n","| **Model** | **Year** | **Authors / Org** | **Key Idea** |\n","|-----------|----------|-------------------|--------------|\n","| **ViT (Vision Transformer)** | 2021 | Dosovitskiy et al., Google | First pure-Transformer vision model (*‚ÄúAn Image is Worth 16√ó16 Words‚Äù*). |\n","| **Swin Transformer** | 2021 | Liu et al., Microsoft | Hierarchical shifted-window design for scalable vision tasks. |\n","| **Perceiver / Perceiver IO** | 2021 | Jaegle et al., DeepMind | General, modality-agnostic attention bottleneck architecture. |\n","| **CLIP** | 2021 | Radford et al., OpenAI | Contrastive pretraining on paired image‚Äìtext data. |\n","| **PaLM-E** | 2023 | Google Research | Embodied multimodal Transformer (vision + language + robotics). |\n","\n","---\n","\n","## ‚úÖ Summary Families\n","\n","- **Encoder-only:** BERT ‚Üí RoBERTa ‚Üí ALBERT ‚Üí ELECTRA ‚Üí DeBERTa.  \n","- **Decoder-only:** GPT ‚Üí GPT-2 ‚Üí GPT-3 ‚Üí GPT-4.  \n","- **Encoder‚ÄìDecoder:** T5 ‚Üí Switch ‚Üí ByT5 ‚Üí UL2 ‚Üí Flan-T5/PaLM.  \n","- **Vision/Multimodal:** ViT ‚Üí Swin ‚Üí Perceiver ‚Üí CLIP ‚Üí PaLM-E.  \n"],"metadata":{"id":"KU1YmcyQ4M2p"}}]}
