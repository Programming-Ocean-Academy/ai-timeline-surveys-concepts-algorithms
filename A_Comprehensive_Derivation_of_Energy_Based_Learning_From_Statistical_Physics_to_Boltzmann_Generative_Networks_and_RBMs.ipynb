{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# for more: https://youtu.be/_bqa_I5hNAo?si=zc7ikfvusdMhW-N6"
      ],
      "metadata": {
        "id": "Cz0ta7meoddX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For most of the history, computers were seen as purely logical machines, mechanically crunching numbers to produce rigid, unambiguous solutions.\n",
        "\n",
        "There was no place for creativity or ambiguity.\n",
        "\n",
        "After all, when calculating a trajectory to launch a rocket into space, the last thing you want is your calculator dreaming up some funky, non-existing formula or improvising on the spot.\n",
        "\n",
        "50 years ago, if you asked anyone whether a computer program would sooner master driving a car versus composing a song, the answer would have been unanimous.\n",
        "\n",
        "Fast forward to 2024, however, we still haven't quite achieved autonomous driving, but the generative AI of all flavors is taken for granted at this point.\n",
        "\n",
        "So what sparked this shift?\n",
        "\n",
        "At what point do neural networks transcend mere deterministic computation and begin to create, synthesizing things that never existed before?\n",
        "\n",
        "Meet the Boltzmann machine, a type of a neural network that dared to embrace chaos and change the course of AI forever.\n",
        "\n",
        "Developed in 1980's, Boltzmann machines introduced a radical notion.\n",
        "\n",
        "What if we built uncertainty and randomness into the very fabric of machine learning?\n",
        "\n",
        "What if, instead of storing rigid facts and performing deterministic computations, our AI could grasp the underlying probabilistic rules that govern the world around us?\n",
        "\n",
        "In this video, we will build a Boltzmann machine from first principles and explore how concepts of probability and inherent uncertainty can be reconciled with the seemingly rigid nature of computer operations.\n",
        "\n",
        "If you're interested, stay tuned.\n",
        "\n",
        "To understand Boltzmann machines, we must first understand their simpler predecessors, associative memory networks, also known as Hopfield networks.\n",
        "\n",
        "We explored these in depth in the previous video.\n",
        "\n",
        "So if you haven't seen it, I highly recommend watching it before continuing with this one, as we'll be directly building on those ideas.\n",
        "\n",
        "But here's a quick refresher.\n",
        "\n",
        "A Hopfield network is a model of associative memory inspired by the brain's ability to recall complete patterns from partial or noisy inputs.\n",
        "\n",
        "It operates by assigning a specific energy value to each possible state, and then iteratively minimizing this energy by descending along the energy surface into the nearest well, thus recalling the best matching stored memory.\n",
        "\n",
        "This energy landscape is shaped by network weights, which are learned by observing data points, patterns we want to memorize, and adjusting the weights to lower the energy associated with those patterns.\n",
        "\n",
        "Given enough neurons, a Hopfield network has essentially perfect memory and excels at mechanical tasks like pattern completion.\n",
        "\n",
        "Think of it as a virtuoso classical musician who can recognize and flawlessly reproduce a well known masterpiece from just a few initial notes.\n",
        "\n",
        "However, while impressive, a Hopfield network's ability to recall and complete patterns is limited to reproducing what it has explicitly learned.\n",
        "\n",
        "It cannot create new patterns or understand the underlying structure of the data it has seen.\n",
        "\n",
        "This is where Boltzmann machines come in, offering a more flexible and creative approach to information processing.\n",
        "\n",
        "To illustrate the difference, let's extend our musical analogy.\n",
        "\n",
        "Imagine a jazz musician who has internalized not just specific songs, but also the fundamental rules and structures inherent to the music itself.\n",
        "\n",
        "When given a few opening notes, this musician doesn't simply recall and play an existing piece.\n",
        "\n",
        "Instead, they leverage a deep understanding of musical theory combined with creativity to improvise and produce something entirely new.\n",
        "\n",
        "This jazz musician represents a Boltzmann machine.\n",
        "\n",
        "Unlike an associative network, it doesn't just memorize data points.\n",
        "\n",
        "Instead, it learns the underlying probability distribution of the data, capturing the essence of what makes a pattern belong to a particular category or style, while incorporating inherent uncertainty into its computations.\n",
        "\n",
        "At first glance, these two systems might seem fundamentally different, with little in common algorithmically.\n",
        "\n",
        "However, in fact, they are very closely related.\n",
        "\n",
        "Just two key technical modifications can transform any Hopfield network into a Boltzmann machine, namely stochasticity and hidden units.\n",
        "\n",
        "Let's explore each of them in detail.\n",
        "\n",
        "We will first sprinkle in a dash of randomness and talk about how Boltzmann machines earned their name.\n",
        "\n",
        "We begin in Austria, 19th century, where a young physicist, Ludwig Boltzmann, is grappling with a fundamental problem.\n",
        "\n",
        "Imagine a system of particles, like a gas.\n",
        "\n",
        "Each particle has its own energy, determined by factors such as its velocity.\n",
        "\n",
        "We can measure the average energy of particles on a macroscopic scale by measuring the temperature.\n",
        "\n",
        "But what happens at the individual particle level?\n",
        "\n",
        "We might imagine that particles probably differ in terms of exact energy values.\n",
        "\n",
        "Indeed, collisions can cause some particles to move faster than others, resulting in a range of energies.\n",
        "\n",
        "Boltzmann's quest was to understand this energy distribution.\n",
        "\n",
        "In other words, if we randomly select a particle, what is the probability that it will have a specific energy value?\n",
        "\n",
        "Boltzmann's insight was to link a state's probability to its energy through an exponential relationship.\n",
        "\n",
        "Specifically, the probability of a state S with energy E is proportional to the exponent of the negative energy divided by temperature.\n",
        "\n",
        "Intuitively, lower energy states are more probable than higher energy states and this fundamental relationship quantifies exactly how much more probable.\n",
        "\n",
        "To understand why the exponent arises here, imagine energy levels as steps on a staircase with particles jumping between them.\n",
        "\n",
        "Each step represents a small energy increment, Є (epsilon).\n",
        "\n",
        "For a particle to move up one step, it must gain epsilon units of energy, perhaps through a collision with another particle.\n",
        "\n",
        "Let's call the probability of such a collision p.\n",
        "\n",
        "Given a large number of particles, this probability is essentially constant and depends only on the average particle velocity or temperature.\n",
        "\n",
        "If a particle jumps up one level with a probability p, it might immediately jump again with the same probability.\n",
        "\n",
        "Since probabilities multiply for independent events, the chance of jumping two levels is p-square, three levels is p-cubed, and so on.\n",
        "\n",
        "We see a pattern.\n",
        "\n",
        "The probability of jumping n levels is p to the power of n.\n",
        "\n",
        "Now, consider a particle increasing its energy by ΔE (delta E).\n",
        "\n",
        "How many steps must it climb?\n",
        "\n",
        "Since the gap between the steps is constant, the number of steps is ΔE over Є.\n",
        "\n",
        "Thus, the probability of making this transition to a higher energy state is \\(p^{\\Delta E / \\varepsilon}\\).\n",
        "\n",
        "To bring it into a more familiar form, we can repackage different constants.\n",
        "\n",
        "We can move the temperature dependency of p into the exponent and change the base to e or Euler's number, conventionally used in exponential.\n",
        "\n",
        "Since p is less than one, while e is greater than one, this necessitates a minus sign before the energy in the exponent, since the temperature is always positive.\n",
        "\n",
        "Consequently, the probability of an energy increase ΔE is equal to the exponent of minus ΔE over temperature.\n",
        "\n",
        "Oh, and by the way, in textbooks you will usually find a version of it with a Boltzmann constant k in front of the temperature.\n",
        "\n",
        "But this constant is used to convert the units of temperature measured in Kelvin to energy measured in joules.\n",
        "\n",
        "But in this video we absorb the Boltzmann constant into temperature.\n",
        "\n",
        "This equation gives us the relative probability of transitioning from one state to another as a function of the energy difference between them.\n",
        "\n",
        "But how can we find the absolute probability of a particular energy state?\n",
        "\n",
        "Consider a toy example.\n",
        "\n",
        "Suppose there are only three states with energy values of one, two and three respectively.\n",
        "\n",
        "Temperature = 1.\n",
        "\n",
        "The equation tells us that state two is 1/e as likely as state one.\n",
        "\n",
        "State three is 1/e² as likely as state one.\n",
        "\n",
        "But what about absolute values?\n",
        "\n",
        "We don't know the baseline probability of state one.\n",
        "\n",
        "Absolute probabilities must sum to one.\n",
        "\n",
        "Let probability of state one = x.\n",
        "\n",
        "Use ratios to express probabilities of others.\n",
        "\n",
        "Use total probability rule to solve for x.\n",
        "\n",
        "This demonstrates how we move from relative probabilities to absolute probabilities.\n",
        "\n",
        "Now plug the absolute energy values into the exponential formula.\n",
        "\n",
        "Plot relative probabilities vs energy.\n",
        "\n",
        "Plot absolute probabilities.\n",
        "\n",
        "Notice: one shape is a vertically rescaled version of the other.\n",
        "\n",
        "Therefore absolute probability of a state with energy e is proportional to exp(−e).\n",
        "\n",
        "Divide by Z, where Z is the partition function.\n",
        "\n",
        "Z ensures the probabilities sum to one.\n",
        "\n",
        "This is the final Boltzmann distribution linking energy to probability.\n",
        "\n",
        "Compute Z by summing exp(−energy) across all states.\n",
        "\n",
        "Probability of a given state = exp(−E) / Z.\n",
        "\n",
        "Now apply this to Hopfield networks to make them more stochastic.\n",
        "\n",
        "Recall: Hopfield neurons update deterministically.\n",
        "\n",
        "Boltzmann machines embrace randomness.\n",
        "\n",
        "Instead of always choosing lowest energy, they choose probabilistically using Boltzmann distribution.\n",
        "\n",
        "Consider a single neuron i.\n",
        "\n",
        "At a given update step, two candidate states: on or off.\n",
        "\n",
        "Compute energy of each.\n",
        "\n",
        "First term: edges of neuron i.\n",
        "\n",
        "Second term: energy of rest of network.\n",
        "\n",
        "Energy term from rest of network cancels.\n",
        "\n",
        "Probability depends only on local connections.\n",
        "\n",
        "Probability of switching on is a function of energy difference.\n",
        "\n",
        "Energy difference = 2 × weighted input.\n",
        "\n",
        "Substituting gives sigmoid(weighted input).\n",
        "\n",
        "When input positive: high probability of switching on.\n",
        "\n",
        "When input negative: low probability.\n",
        "\n",
        "Stochastic update rule:\n",
        "\n",
        "Compute weighted input.\n",
        "\n",
        "Compute sigmoid probability.\n",
        "\n",
        "Generate random number 0–1.\n",
        "\n",
        "If < probability → neuron = 1. Else neuron = −1.\n",
        "\n",
        "Stochasticity allows moves to higher energy states.\n",
        "\n",
        "High temp → more random.\n",
        "\n",
        "Low temp → more deterministic.\n",
        "\n",
        "Temperature is a hyper-parameter controlling creativity.\n",
        "\n",
        "This stochastic rule is essential.\n",
        "\n",
        "It allows the network to escape local minima.\n",
        "\n",
        "It explores many states.\n",
        "\n",
        "Enables learning complex probability distributions.\n",
        "\n",
        "Random update rule is the key modification for inference.\n",
        "\n",
        "But does stochasticity change learning?\n",
        "\n",
        "Yes → leads to contrastive learning rule.\n",
        "\n",
        "In Hopfield networks, learning = lowering energy of stored patterns.\n",
        "\n",
        "In Boltzmann machines, goal = learn probability distribution of data.\n",
        "\n",
        "Ideally, network should spend more time in states corresponding to training data.\n",
        "\n",
        "These states must have higher probability.\n",
        "\n",
        "From Boltzmann distribution → higher probability = lower energy relative to others.\n",
        "\n",
        "But lowering one energy affects Z.\n",
        "\n",
        "Z depends on energies of all states.\n",
        "\n",
        "Learning goal: maximize probability of training data while accounting for entire energy landscape.\n",
        "\n",
        "Objective: maximize log probability of data.\n",
        "\n",
        "Take log of joint probability.\n",
        "\n",
        "Substitute Boltzmann distribution.\n",
        "\n",
        "Insight: maximize log probability =\n",
        "\n",
        "Minimize energy of training patterns.\n",
        "\n",
        "Minimize partition function.\n",
        "\n",
        "Minimize Z means raising energy of non-training states.\n",
        "\n",
        "Two forces:\n",
        "\n",
        "Digging wells around data.\n",
        "\n",
        "Raising surface for unrealistic states.\n",
        "\n",
        "Weight update rule = contrastive Hebbian learning.\n",
        "\n",
        "First term: average product xi xj when clamped to data (Hebbian).\n",
        "\n",
        "Second term: average product xi xj during free-running (anti-Hebbian).\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "Strengthen correlations present in data.\n",
        "\n",
        "Weaken correlations present only in hallucinated states.\n",
        "\n",
        "Called contrastive because it contrasts constrained vs free phases.\n",
        "\n",
        "Positive phase: clamp visible neurons to data.\n",
        "\n",
        "Negative phase: allow free-running sampling.\n",
        "\n",
        "In practice:\n",
        "\n",
        "For Hebbian term: iterate over training examples.\n",
        "\n",
        "For anti-Hebbian term: run network freely from random states.\n",
        "\n",
        "Repeat many times → iterative learning.\n",
        "\n",
        "Learning alternates between positive and negative phases.\n",
        "\n",
        "This gradually shapes the energy landscape so valleys = data, peaks = unrealistic states.\n",
        "\n",
        "So far: only visible units.\n",
        "\n",
        "Final modification: hidden units.\n",
        "\n",
        "Hidden units encode internal representations.\n",
        "\n",
        "Capture abstract features.\n",
        "\n",
        "Visible units = observation.\n",
        "\n",
        "Hidden units = latent structure.\n",
        "\n",
        "Implementation:\n",
        "\n",
        "Add neurons.\n",
        "\n",
        "Some = visible, some = hidden.\n",
        "\n",
        "Hidden units update same as visible.\n",
        "\n",
        "In learning:\n",
        "\n",
        "Positive phase: clamp visible, let hidden update.\n",
        "\n",
        "Negative phase: let all run freely.\n",
        "\n",
        "Apply contrastive learning to all weights.\n",
        "\n",
        "Hidden units learn to represent structure without being explicitly taught.\n",
        "\n",
        "Over time they learn abstract features ∼ early deep learning.\n",
        "\n",
        "Restricted Boltzmann Machines (RBM):\n",
        "\n",
        "No visible–visible connections.\n",
        "\n",
        "No hidden–hidden connections.\n",
        "\n",
        "Only visible ↔ hidden.\n",
        "\n",
        "Allows parallel unit updates.\n",
        "\n",
        "Much more efficient.\n",
        "\n",
        "Despite restrictions, RBMs are expressive and practical.\n",
        "\n",
        "Summary:\n",
        "\n",
        "Hopfield networks → store/recall.\n",
        "\n",
        "Add randomness + probability → Boltzmann machines.\n",
        "\n",
        "Add hidden units → abstract representation + generation.\n",
        "\n",
        "Contrastive learning → captures distributions.\n",
        "\n",
        "Foundation for modern generative AI.\n",
        "\n",
        "Closing remarks + sponsor message (Shortform).\n",
        "\n",
        "Guides, summaries, references, broader context.\n",
        "\n",
        "Encouragement to subscribe and continue learning.\n"
      ],
      "metadata": {
        "id": "JN6Aay3onyqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I. Physics of Energy & Probability (Boltzmann Distribution Foundations)\n",
        "\n",
        "1. Energy increment per step  \n",
        "$$\\Delta E = n\\,\\varepsilon$$\n",
        "\n",
        "2. Probability of climbing one step  \n",
        "$$p$$\n",
        "\n",
        "3. Probability of climbing \\(n\\) steps  \n",
        "$$p^n$$\n",
        "\n",
        "4. Probability of energy increase by \\(\\Delta E\\)  \n",
        "$$p^{\\Delta E / \\varepsilon}$$\n",
        "\n",
        "5. Exponential form of transition probability  \n",
        "$$\\Pr(\\Delta E)=e^{-\\Delta E/T}$$\n",
        "\n",
        "(Where temperature absorbs Boltzmann constant: \\(kT \\rightarrow T\\))\n",
        "\n",
        "---\n",
        "\n",
        "## II. From Relative Probabilities to Absolute Probabilities\n",
        "\n",
        "6. Relative probability of a state with energy \\(E\\)  \n",
        "$$\\tilde{p}(E)=e^{-E}$$\n",
        "\n",
        "7. Partition function  \n",
        "$$Z=\\sum_{s} e^{-E(s)}$$\n",
        "\n",
        "8. Boltzmann distribution (final form)  \n",
        "$$p(s)=\\frac{e^{-E(s)}}{Z}$$\n",
        "\n",
        "---\n",
        "\n",
        "## III. Energy in Hopfield / Boltzmann Networks\n",
        "\n",
        "9. Energy of a network configuration  \n",
        "$$E(x)= -\\sum_{i<j} w_{ij} x_i x_j$$\n",
        "\n",
        "---\n",
        "\n",
        "## IV. Local Neuron Update (Boltzmann Machine Stochastic Rule)\n",
        "\n",
        "10. Energy when neuron \\(i\\) is ON  \n",
        "$$E(x_i=1) = -\\sum_j w_{ij} x_j + C$$\n",
        "\n",
        "11. Energy when neuron \\(i\\) is OFF  \n",
        "$$E(x_i=-1) = +\\sum_j w_{ij} x_j + C$$\n",
        "\n",
        "12. Energy difference between states  \n",
        "$$\\Delta E = -2\\sum_j w_{ij} x_j$$\n",
        "\n",
        "13. Boltzmann probability of neuron \\(i\\) turning ON  \n",
        "$$p(x_i = 1)=\\frac{1}{1 + e^{\\Delta E/T}}$$\n",
        "\n",
        "14. Sigmoid form  \n",
        "$$p(x_i = 1)=\\sigma\\left( \\frac{2}{T}\\sum_j w_{ij} x_j \\right)$$\n",
        "\n",
        "---\n",
        "\n",
        "## V. Learning Objective (Maximum Likelihood)\n",
        "\n",
        "15. Log-probability of dataset \\(\\{x^{(1)},\\dots,x^{(N)}\\}\\)  \n",
        "$$\\log P = \\sum_{n=1}^{N} \\log p(x^{(n)})$$\n",
        "\n",
        "16. Substitute Boltzmann distribution  \n",
        "$$\\log P = \\sum_{n=1}^{N} \\left( -E(x^{(n)}) - \\log Z \\right)$$\n",
        "\n",
        "17. Derivative (core learning gradient)  \n",
        "$${\\partial \\log P \\over \\partial w_{ij}}\n",
        "= -\\langle x_i x_j \\rangle_{\\text{data}}\n",
        "+ \\langle x_i x_j \\rangle_{\\text{model}}$$\n",
        "\n",
        "---\n",
        "\n",
        "## VI. Contrastive Hebbian Learning Rule\n",
        "\n",
        "18. Final weight update rule  \n",
        "$$\\Delta w_{ij} = \\eta\\left( \\langle x_i x_j \\rangle_{\\text{data}} - \\langle x_i x_j \\rangle_{\\text{model}} \\right)$$\n",
        "\n",
        "Positive phase:  \n",
        "$$\\langle x_i x_j \\rangle_{\\text{data}}$$\n",
        "\n",
        "Negative phase:  \n",
        "$$\\langle x_i x_j \\rangle_{\\text{model}}$$\n",
        "\n",
        "---\n",
        "\n",
        "## VII. Hidden Units (same rules, extended)\n",
        "\n",
        "19. Full Boltzmann machine energy  \n",
        "$$\n",
        "E(v,h)=\n",
        "-\\sum_{i,j} w_{ij} v_i h_j\n",
        "-\\sum_{i<j} a_{ij} v_i v_j\n",
        "-\\sum_{i<j} b_{ij} h_i h_j\n",
        "$$\n",
        "\n",
        "20. Restricted Boltzmann Machine (RBM) energy  \n",
        "$$E(v,h) = -\\sum_{i,j} w_{ij} v_i h_j$$\n",
        "\n",
        "---\n",
        "\n",
        "## VIII. Inference in RBM (Parallel Updates)\n",
        "\n",
        "21. Hidden unit activation  \n",
        "$$p(h_j = 1 \\mid v) = \\sigma\\left( \\sum_i w_{ij} v_i \\right)$$\n",
        "\n",
        "22. Visible unit activation  \n",
        "$$p(v_i = 1 \\mid h) = \\sigma\\left( \\sum_j w_{ij} h_j \\right)$$\n",
        "\n",
        "---\n",
        "\n",
        "## Complete Equation List (Quick Index)\n",
        "\n",
        "$$\\Delta E = n\\varepsilon$$  \n",
        "$$p, \\; p^n, \\; p^{\\Delta E/\\varepsilon}$$  \n",
        "$$e^{-\\Delta E/T}$$  \n",
        "$$e^{-E}$$  \n",
        "$$Z=\\sum_s e^{-E(s)}$$  \n",
        "$$p(s)=e^{-E(s)}/Z$$  \n",
        "$$E(x)= -\\sum_{i<j} w_{ij} x_i x_j$$  \n",
        "$$E(x_i=1)= -\\sum_j w_{ij} x_j + C$$  \n",
        "$$E(x_i=-1)= +\\sum_j w_{ij} x_j + C$$  \n",
        "$$\\Delta E = -2\\sum_j w_{ij} x_j$$  \n",
        "$$p(x_i=1)=\\frac{1}{1+e^{\\Delta E/T}}$$  \n",
        "$$\\sigma\\left( \\frac{2}{T}\\sum_j w_{ij} x_j \\right)$$  \n",
        "$$\\log P = \\sum_n \\log p(x^{(n)})$$  \n",
        "$$\\log P = \\sum_n (-E(x^{(n)}) - \\log Z)$$  \n",
        "$${\\partial \\log P \\over \\partial w_{ij}}\n",
        "= -\\langle x_i x_j \\rangle_{\\text{data}}\n",
        "+ \\langle x_i x_j \\rangle_{\\text{model}}$$  \n",
        "$$\\Delta w_{ij}\n",
        "= \\eta(\\langle x_i x_j \\rangle_{\\text{data}}\n",
        "- \\langle x_i x_j \\rangle_{\\text{model}})$$  \n",
        "$$\n",
        "E(v,h)=\n",
        "-\\sum_{i,j} w_{ij} v_i h_j\n",
        "-\\sum_{i<j} a_{ij} v_i v_j\n",
        "-\\sum_{i<j} b_{ij} h_i h_j\n",
        "$$  \n",
        "$$E(v,h)= -\\sum_{i,j} w_{ij} v_i h_j$$  \n",
        "$$p(h_j=1|v)=\\sigma\\left(\\sum_i w_{ij} v_i\\right)$$  \n",
        "$$p(v_i=1|h)=\\sigma\\left(\\sum_j w_{ij} h_j\\right)$$\n"
      ],
      "metadata": {
        "id": "L25eGOlan7rO"
      }
    }
  ]
}