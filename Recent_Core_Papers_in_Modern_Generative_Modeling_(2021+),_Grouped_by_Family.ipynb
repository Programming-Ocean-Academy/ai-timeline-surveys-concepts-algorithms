{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Recent Core Papers in Modern Generative Modeling (2021+), Grouped by Family\n",
        "\n",
        "## 1) Transformers (Generative Models & Foundation Models)\n",
        "\n",
        "### Text/Foundation Models\n",
        "- **Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity** (2021)  \n",
        "- **Training Compute-Optimal Large Language Models (Chinchilla)** (2022)  \n",
        "- **PaLM: Scaling Language Modeling with Pathways** (2022)  \n",
        "- **FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness** (2022)  \n",
        "- **LLaMA: Open and Efficient Foundation Language Models** (2023)  \n",
        "- **GPT-4 Technical Report** (2023)\n",
        "\n",
        "### Transformer-based Visual Generation\n",
        "- **MaskGIT: Masked Generative Image Transformer** (2022)  \n",
        "- **Scaling Autoregressive Models for Content-Rich Text-to-Image Generation (Parti)** (2022)\n",
        "\n",
        "Key primitive (scaled dot-product attention):\n",
        "$$\n",
        "\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V.\n",
        "$$\n",
        "\n",
        "Autoregressive likelihood factorization:\n",
        "$$\n",
        "p(x)=\\prod_{t=1}^{T} p(x_t \\mid x_{<t}).\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 2) Diffusion / Score-Based Models\n",
        "\n",
        "- **Improved Denoising Diffusion Probabilistic Models** (2021)  \n",
        "- **Diffusion Models Beat GANs on Image Synthesis** (2021)  \n",
        "- **High-Resolution Image Synthesis with Latent Diffusion Models (LDM)** (2021/2022)  \n",
        "- **Classifier-Free Diffusion Guidance** (2022)  \n",
        "- **Elucidating the Design Space of Diffusion-Based Generative Models (EDM)** (2022)  \n",
        "- **Consistency Models** (2023)\n",
        "\n",
        "Forward noising (DDPM-style):\n",
        "$$\n",
        "q(x_t \\mid x_{t-1})=\\mathcal{N}\\!\\left(\\sqrt{1-\\beta_t}\\,x_{t-1},\\,\\beta_t I\\right),\n",
        "\\qquad\n",
        "q(x_t \\mid x_0)=\\mathcal{N}\\!\\left(\\sqrt{\\bar{\\alpha}_t}\\,x_0,\\,(1-\\bar{\\alpha}_t)I\\right),\n",
        "$$\n",
        "where $\\alpha_t=1-\\beta_t$ and $\\bar{\\alpha}_t=\\prod_{s=1}^{t}\\alpha_s$.\n",
        "\n",
        "Denoising/score learning objective (one common form):\n",
        "$$\n",
        "\\min_{\\theta}\\; \\mathbb{E}_{t,x_0,\\varepsilon}\\left[\\left\\|\\varepsilon-\\varepsilon_\\theta(x_t,t)\\right\\|_2^2\\right],\n",
        "\\qquad\n",
        "x_t=\\sqrt{\\bar{\\alpha}_t}\\,x_0+\\sqrt{1-\\bar{\\alpha}_t}\\,\\varepsilon,\\;\\varepsilon\\sim\\mathcal{N}(0,I).\n",
        "$$\n",
        "\n",
        "Classifier-free guidance (schematic):\n",
        "$$\n",
        "\\hat{\\varepsilon}(x_t,t,c)=\\varepsilon_\\theta(x_t,t,\\varnothing)+w\\left(\\varepsilon_\\theta(x_t,t,c)-\\varepsilon_\\theta(x_t,t,\\varnothing)\\right).\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 3) Flow-Based Models / Continuous Normalizing Flows\n",
        "\n",
        "- **Flow Matching for Generative Modeling** (2022 / ICLR 2023)  \n",
        "- **Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow** (2022)  \n",
        "- **Improving Rectified Flow with Boundary Conditions** (2025)\n",
        "\n",
        "Continuous normalizing flow (CNF) dynamics:\n",
        "$$\n",
        "\\frac{dx}{dt}=f_\\theta(x,t).\n",
        "$$\n",
        "\n",
        "Instantaneous change of variables (log-density evolution):\n",
        "$$\n",
        "\\frac{d}{dt}\\log p(x(t))=-\\mathrm{Tr}\\!\\left(\\frac{\\partial f_\\theta}{\\partial x}\\right).\n",
        "$$\n",
        "\n",
        "Flow matching (high-level target: match a velocity field along an interpolation path):\n",
        "$$\n",
        "\\min_{\\theta}\\; \\mathbb{E}_{t}\\,\\mathbb{E}_{x(t)}\\left[\\left\\|f_\\theta(x(t),t)-v^\\star(x(t),t)\\right\\|_2^2\\right].\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 4) GANs / Adversarial Generative Models\n",
        "\n",
        "- **Alias-Free Generative Adversarial Networks (StyleGAN3)** (2021)  \n",
        "- **StyleGAN-XL: Scaling StyleGAN to Large and Diverse Datasets** (2022)  \n",
        "- **StyleSwin: Transformer-Based GAN for High-Resolution Image Generation** (2022)  \n",
        "- **Designing an Encoder for StyleGAN Image Manipulation (e4e)** (2021)\n",
        "\n",
        "Standard GAN minimax objective:\n",
        "$$\n",
        "\\min_{G}\\max_{D}\\;\n",
        "\\mathbb{E}_{x\\sim p_{\\text{data}}}\\left[\\log D(x)\\right]\n",
        "+\\mathbb{E}_{z\\sim p(z)}\\left[\\log\\left(1-D(G(z))\\right)\\right].\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 5) Adversarial Neural Networks (Robustness & Adversarial Training â€” not GANs)\n",
        "\n",
        "- **Fast Is Better Than Free: Revisiting Adversarial Training** (2020/2021)\n",
        "\n",
        "Robust optimization perspective:\n",
        "$$\n",
        "\\min_{\\theta}\\;\\mathbb{E}_{(x,y)}\\left[\\max_{\\|\\delta\\|\\le \\varepsilon}\\;\\mathcal{L}\\big(f_\\theta(x+\\delta),y\\big)\\right].\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 6) Energy-Based Models (EBMs)\n",
        "\n",
        "- **Energy-Based Models for Anomaly Detection: A Manifold Diffusion Recovery Approach** (NeurIPS 2023)  \n",
        "- **Energy-Based Diffusion Language Model (EDLM)** (2024)\n",
        "\n",
        "Energy-based density:\n",
        "$$\n",
        "p_\\theta(x)=\\frac{\\exp(-E_\\theta(x))}{Z_\\theta},\n",
        "\\qquad\n",
        "Z_\\theta=\\int \\exp(-E_\\theta(x))\\,dx.\n",
        "$$\n",
        "\n",
        "Negative log-likelihood (up to an additive constant):\n",
        "$$\n",
        "-\\log p_\\theta(x)=E_\\theta(x)+\\log Z_\\theta.\n",
        "$$\n"
      ],
      "metadata": {
        "id": "j4zFK0bkbuym"
      }
    }
  ]
}