{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Master Taxonomy of Deep Model Optimization and Acceleration Techniques (2014–2025)\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Model Compression\n",
        "\n",
        "### 1.1 Pruning & Sparsity\n",
        "\n",
        "- **Unstructured magnitude pruning**  \n",
        "  *Han et al., “Learning both Weights and Connections...”, 2015; “Deep Compression”, ICLR 2016*  \n",
        "  Iteratively remove small weights and retrain to maintain accuracy.\n",
        "\n",
        "- **Structured / channel / filter pruning**  \n",
        "  *Li et al., 2017; He et al., 2017*  \n",
        "  Remove whole filters, channels, or residual blocks for real-world acceleration.\n",
        "\n",
        "- **Automated / importance-based pruning**  \n",
        "  *AMC, MorphNet, MetaPruning, Movement Pruning (2020)*  \n",
        "  Learn what to prune using sensitivity or meta-learning.\n",
        "\n",
        "- **Dynamic sparse training (RigL, SET, GMP)**  \n",
        "  *Evci et al., 2020; Mostafa & Wang, 2019*  \n",
        "  Maintain fixed sparsity and regrow useful connections during training.\n",
        "\n",
        "- **Semi-structured N:M sparsity**  \n",
        "  *NVIDIA Ampere–Blackwell (2:4, 4:8 patterns)*  \n",
        "  Hardware-aligned sparsity for Tensor Core acceleration.\n",
        "\n",
        "- **One-shot LLM pruning**  \n",
        "  *SparseGPT (2023); Wanda (2023)*  \n",
        "  Directly prune LLMs without full retraining.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.2 Quantization\n",
        "\n",
        "- **Classic quantization (INT8/INT4)**  \n",
        "  *Jacob et al., 2018*  \n",
        "  Post-training (PTQ) or quantization-aware (QAT) for CNNs/Transformers.\n",
        "\n",
        "- **LLM-grade quantization (SmoothQuant)**  \n",
        "  *Xiao et al., ICML 2023*  \n",
        "  Handles outliers by shifting activation scales for all matmuls (W8A8).\n",
        "\n",
        "- **Activation-aware quantization (AWQ, GPTQ, AQLM)**  \n",
        "  *Lin et al., 2023*  \n",
        "  Quantize weights (4-bit) while preserving activation precision (16-bit).\n",
        "\n",
        "- **Ultra-low precision (FP8, FP4, NF4, NVFP4)**  \n",
        "  *NVIDIA Blackwell, 2024–2025*  \n",
        "  Native low-bit tensor formats balancing efficiency and stability.\n",
        "\n",
        "---\n",
        "\n",
        "### 1.3 Low-Rank and Knowledge Distillation\n",
        "\n",
        "- **Low-rank factorization**  \n",
        "  *SVD, Tucker, CP, Tensor-Train, LoRA, DoRA, AdapterFusion*  \n",
        "  Decompose or adapt model layers to reduce parameter count.\n",
        "\n",
        "- **Knowledge Distillation (KD)**  \n",
        "  *Hinton et al., 2015; DistilBERT (2019); TinyBERT (2020); MiniLM (2020)*  \n",
        "  Teach smaller “student” models using teacher outputs.\n",
        "\n",
        "- **Dataset distillation / pruning**  \n",
        "  *\"Less Is More for LLM Training\", 2024–2025*  \n",
        "  Optimize training data via quality filtering, scoring, and curricula.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Efficient Attention and Architecture\n",
        "\n",
        "### 2.1 Attention Optimizations\n",
        "\n",
        "- **FlashAttention 1–3**  \n",
        "  *Dao et al., 2022–2025*  \n",
        "  IO-aware kernels that minimize memory reads and writes.\n",
        "\n",
        "- **Paged / chunked attention (vLLM)**  \n",
        "  Enables long context windows with efficient KV-cache management.\n",
        "\n",
        "- **Linear and sparse Transformers**  \n",
        "  *Linformer, Performer, Reformer, Longformer, BigBird, Nyströmformer*  \n",
        "  Reduce quadratic complexity of self-attention.\n",
        "\n",
        "- **MQA / GQA**  \n",
        "  *Shazeer, 2019; PaLM/LLaMA3 (2023–2024)*  \n",
        "  Share K/V tensors per head or group to cut KV memory.\n",
        "\n",
        "- **Mixture-of-Experts (MoE)**  \n",
        "  *Switch Transformer (2021); GLaM; Mixtral; DeepSeek-V3 (2024–2025)*  \n",
        "  Sparse activation of expert layers for high capacity with lower cost.\n",
        "\n",
        "- **Early-exit / conditional compute**  \n",
        "  *DeeBERT, PABEE, BEExformer (2024)*  \n",
        "  Dynamically stop inference when confidence is high.\n",
        "\n",
        "- **Neural Architecture Search (NAS)**  \n",
        "  *MnasNet, EfficientNet, ProxylessNAS*  \n",
        "  Automated discovery of efficient architectures.\n",
        "\n",
        "- **CNN efficiency**  \n",
        "  *MobileNet, Xception, ShuffleNet*  \n",
        "  Depthwise-separable and pointwise convolutions for edge devices.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Training-Time Efficiency\n",
        "\n",
        "- **Mixed precision (FP16/BF16/FP8)**  \n",
        "  *Micikevicius et al., ICLR 2018*  \n",
        "  Lower precision with master FP32 copy for stable gradients.\n",
        "\n",
        "- **Activation / gradient checkpointing**  \n",
        "  *Chen et al., 2016*  \n",
        "  Recompute activations during backprop to save memory.\n",
        "\n",
        "- **Parallelism & sharding**  \n",
        "  *FSDP, ZeRO, Ulysses, Sequence Parallelism (2021–2025)*  \n",
        "  Distribute parameters, gradients, and optimizer states.\n",
        "\n",
        "- **Dynamic sparse training**  \n",
        "  Compute only on active subgraphs with topology updates.\n",
        "\n",
        "- **Progressive layer dropping / growing**  \n",
        "  LayerDrop, DropPath for adaptive capacity.\n",
        "\n",
        "- **Curriculum learning / data pruning**  \n",
        "  Present simpler or higher-value samples earlier.\n",
        "\n",
        "- **Optimizer compression (8-bit Adam)**  \n",
        "  *Dettmers et al., 2022; bitsandbytes*  \n",
        "  Quantize optimizer states for memory efficiency.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Inference-Time Serving Optimizations\n",
        "\n",
        "- **Paged KV-cache (vLLM)**  \n",
        "  Efficiently manage long contexts using paged attention structures.\n",
        "\n",
        "- **Continuous batching (vLLM, SGLang)**  \n",
        "  Aggregate requests dynamically to improve GPU utilization.\n",
        "\n",
        "- **Speculative decoding**  \n",
        "  *Leviathan et al., 2023 → Medusa/EAGLE (2024–2025)*  \n",
        "  Draft-then-verify token generation to accelerate autoregressive inference.\n",
        "\n",
        "- **Kernel and graph fusion**  \n",
        "  *TorchInductor, TensorRT-LLM, XLA/IREE, Triton*  \n",
        "  Fuse linear, normalization, and activation kernels.\n",
        "\n",
        "- **Quantized serving kernels**  \n",
        "  *Marlin/AWQ kernels for INT4 + FP16 decoding.*\n",
        "\n",
        "- **Parallelism (pipeline/tensor/expert)**  \n",
        "  *Megatron-Core, DeepSpeed inference*  \n",
        "  Split model computation across devices for scalability.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Hardware and System-Level Optimizations\n",
        "\n",
        "- **Quantized formats**  \n",
        "  *INT8/INT4/PTQ/QAT, SmoothQuant, AWQ, GPTQ, AQLM.*\n",
        "\n",
        "- **Low-bit floating formats**  \n",
        "  *FP8/BF16/FP4/NVFP4 kernels (Hopper/Blackwell).*\n",
        "\n",
        "- **N:M structured sparsity**  \n",
        "  Tensor Core acceleration for semi-structured pruning.\n",
        "\n",
        "- **AOT compilers**  \n",
        "  *XLA, TensorRT-LLM, Triton, OpenXLA/IREE.*  \n",
        "  Optimize computational graphs and runtime execution.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Decision Guidelines\n",
        "\n",
        "### Training Optimization\n",
        "Use:\n",
        "- **Mixed precision (BF16/FP16)**  \n",
        "- **Activation checkpointing**  \n",
        "- **FSDP / ZeRO**  \n",
        "to improve memory and training speed.\n",
        "\n",
        "### Inference Optimization\n",
        "Use:\n",
        "- **GQA/MQA**, **FlashAttention-3**, **Paged KV-cache (vLLM)**  \n",
        "for high throughput without accuracy degradation.\n",
        "\n",
        "### Memory-Constrained Serving\n",
        "Use:\n",
        "- **SmoothQuant (W8A8)** or **AWQ (W4A16)**  \n",
        "on **vLLM/TRT-LLM** runtimes.\n",
        "\n",
        "### Scaling Capacity\n",
        "Use:\n",
        "- **Sparse MoE models** (e.g., Mixtral, DeepSeek-V3)  \n",
        "if expert-parallel frameworks are available.\n",
        "\n",
        "### Latency-Sensitive Applications\n",
        "Use:\n",
        "- **Speculative decoding**, **Continuous batching**, **TensorRT fusion**  \n",
        "to minimize per-token latency.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Cross-Technique Playbooks\n",
        "\n",
        "### Long-Context LLM Serving (2025 GPUs)\n",
        "$$\n",
        "\\text{GQA + FlashAttention-3 + SmoothQuant (W8A8) + vLLM + Continuous Batching + Speculative Decoding}\n",
        "$$\n",
        "Optionally export to TensorRT-LLM for fused kernels.\n",
        "\n",
        "### High-Capacity MoE Training\n",
        "$$\n",
        "\\text{Expert Parallel (Megatron-Core) + FlashAttention + INT8 Weights + SmoothQuant + vLLM Serving}\n",
        "$$\n",
        "\n",
        "### Efficient LLM Training on Small Clusters\n",
        "$$\n",
        "\\text{BF16 + ZeRO-3 + Checkpointing + Sequence Parallel + 8-bit Adam}\n",
        "$$\n",
        "\n",
        "### Edge Vision or On-Device Models\n",
        "$$\n",
        "\\text{Structured Pruning + INT8 PTQ + Early-Exit + TensorRT Deployment}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Key References\n",
        "\n",
        "- Han et al., *Deep Compression*, ICLR 2016.  \n",
        "- Li et al., *Pruning Filters for Efficient ConvNets*, ICLR 2017.  \n",
        "- Hinton et al., *Distilling the Knowledge*, 2015.  \n",
        "- Frankle & Carbin, *Lottery Ticket Hypothesis*, ICLR 2019.  \n",
        "- Sanh et al., *Movement Pruning*, NeurIPS 2020.  \n",
        "- Micikevicius et al., *Mixed Precision Training*, ICLR 2018.  \n",
        "- Dao et al., *FlashAttention*, 2022–2025.  \n",
        "- Xiao et al., *SmoothQuant*, ICML 2023–2024.  \n",
        "- Lin et al., *AWQ*, 2023.  \n",
        "- DeepSeek-V2/V3, Mixtral (2024–2025).  \n",
        "- NVIDIA *TensorRT-LLM / Blackwell NVFP4* (2025).  \n",
        "- DeepSpeed *ZeRO / Ulysses / SP* (2021–2025).\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Notes and Emerging Directions\n",
        "\n",
        "- Reasoning-aware quantization (2025) is under active development.  \n",
        "- NVFP4 kernels are still maturing for general vLLM integration.  \n",
        "- Domain-specific accelerations (e.g., diffusion, 3D vision) are expanding.  \n",
        "- Gradient-free “forward-forward” training is experimental but promising.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "NfPkkrfQ70wh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCHjMlMw7V8l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Master Taxonomy (2014 → 2025)\n",
        "\n",
        "---\n",
        "\n",
        "## A) Model Compression\n",
        "\n",
        "### Pruning & Sparsity\n",
        "\n",
        "- **Unstructured / magnitude pruning** — iterative pruning & retraining.  \n",
        "  *arxiv.org*\n",
        "\n",
        "- **Structured pruning (channels/filters/heads)**; **movement-based pruning**; **N:M structured (e.g., 2:4)**.  \n",
        "  *arxiv.org*  \n",
        "  +1\n",
        "\n",
        "- **One-shot & post-training pruning for LLMs (e.g., SparseGPT)**.  \n",
        "  *arxiv.org*\n",
        "\n",
        "---\n",
        "\n",
        "### Quantization\n",
        "\n",
        "- **PTQ/QAT for CNNs/Transformers; W8A8 (SmoothQuant), INT8/INT4 weight-only (LLM.int8, GPTQ, AWQ), NF4/NF8 (QLoRA)**.  \n",
        "  *arxiv.org*  \n",
        "  +4  \n",
        "  *arxiv.org*  \n",
        "  +4  \n",
        "  *arxiv.org*  \n",
        "  +4\n",
        "\n",
        "---\n",
        "\n",
        "### Weight Sharing / Tying\n",
        "\n",
        "- **Hashed weights; tied input–output embeddings; parameter sharing across layers (e.g., ALBERT)**.  \n",
        "  *ar5iv*  \n",
        "  +1\n",
        "\n",
        "---\n",
        "\n",
        "### Low-Rank & Tensor Decompositions\n",
        "\n",
        "- **SVD/CP/Tucker for conv/linear layers; low-rank adapters (LoRA)**.  \n",
        "  *arxiv.org*  \n",
        "  +1\n",
        "\n",
        "---\n",
        "\n",
        "### Knowledge Distillation (KD)\n",
        "\n",
        "- **Vanilla KD; task-agnostic KD (DistilBERT, TinyBERT); hint-based (FitNets)**.  \n",
        "  *arxiv.org*  \n",
        "  +2  \n",
        "  *arxiv.org*  \n",
        "  +2\n",
        "\n",
        "---\n",
        "\n",
        "## B) Efficient Attention & Architectures\n",
        "\n",
        "### Efficient Transformers (long context / linearized attention)\n",
        "\n",
        "- **Reformer (LSH), Linformer (low-rank), Longformer/BigBird (sparse patterns), Performer (FAVOR+)**.  \n",
        "  *arxiv.org*  \n",
        "  +4  \n",
        "  *arxiv.org*  \n",
        "  +4  \n",
        "  *arxiv.org*  \n",
        "  +4\n",
        "\n",
        "- **MQA/GQA (share K/V across heads or groups for fast decoding)**.  \n",
        "  *arxiv.org*  \n",
        "  +1\n",
        "\n",
        "---\n",
        "\n",
        "### MoE (Sparsely Activated)\n",
        "\n",
        "- **Sparsely-gated layers; Switch Transformer; GShard-style routing/parallelism**.  \n",
        "  *arxiv.org*\n",
        "\n",
        "---\n",
        "\n",
        "### Conditional Compute & Early Exit\n",
        "\n",
        "- **Token/patch pruning/merging; adaptive depth; early-exit Transformers**.  \n",
        "  *arxiv.org*\n",
        "\n",
        "---\n",
        "\n",
        "### NAS & Compound Scaling\n",
        "\n",
        "- **Search-based / rule-based scaling (e.g., EfficientNet)**.  \n",
        "  *arxiv.org*\n",
        "\n",
        "---\n",
        "\n",
        "## C) Training-Time Efficiency\n",
        "\n",
        "### Numerical & Memory\n",
        "\n",
        "- **Mixed-precision (FP16/bfloat16/FP8); gradient/activation checkpointing; ZeRO-style partitioning**.  \n",
        "  *arxiv.org*  \n",
        "  +1\n",
        "\n",
        "---\n",
        "\n",
        "### Sparse & Progressive Training\n",
        "\n",
        "- **Dynamic sparse training (RigL/SET), LayerDrop/progressive depth**.  \n",
        "  *arxiv.org*\n",
        "\n",
        "---\n",
        "\n",
        "### Data-Level\n",
        "\n",
        "- **Curriculum/self-paced learning; data pruning (EL2N/forgetting events); dataset distillation**.  \n",
        "  *arxiv.org*\n",
        "\n",
        "---\n",
        "\n",
        "## D) Inference-Time Serving Optimizations\n",
        "\n",
        "### KV-Cache & Batching\n",
        "\n",
        "- **PagedAttention (vLLM), continuous batching, cache sharing/eviction policies**.  \n",
        "  *arxiv.org*\n",
        "\n",
        "---\n",
        "\n",
        "### Speculative & Multi-Draft Decoding\n",
        "\n",
        "- **Draft-model / tree-based / Medusa-style generation**.  \n",
        "  *arxiv.org*\n",
        "\n",
        "---\n",
        "\n",
        "### Kernel / Graph\n",
        "\n",
        "- **FlashAttention kernels; graph/tensor fusion (XLA/TensorRT-LLM/Triton)**.  \n",
        "  *arxiv.org*  \n",
        "  +1\n",
        "\n",
        "---\n",
        "\n",
        "### Memory & IO\n",
        "\n",
        "- **Chunked prefill, CUDA graphs, paged/prefix KV, streaming attention variants (serving)**.  \n",
        "  *GitHub*\n",
        "\n",
        "---\n",
        "\n",
        "## E) Hardware & Formats\n",
        "\n",
        "### Low-Precision Formats\n",
        "\n",
        "- **INT8/INT4/W8A8; FP8 training/inference; NormalFloat NF4/NF8 for fine-tuning**.  \n",
        "  *arxiv.org*  \n",
        "  +2  \n",
        "  *arxiv.org*  \n",
        "  +2\n",
        "\n",
        "---\n",
        "\n",
        "### Structured Sparsity for Accelerators\n",
        "\n",
        "- **N:M (e.g., 2:4) exploiting hardware sparse-MM support**.  \n",
        "  *arxiv.org*\n",
        "\n",
        "---\n",
        "\n",
        "### AOT/Compilers & Runtimes\n",
        "\n",
        "- **XLA, TensorRT-LLM, Triton custom kernels**.  \n",
        "  *IBM Research*\n",
        "\n",
        "---\n",
        "\n",
        "## Evidence Table\n",
        "\n",
        "| Technique | Core Goal | Key Idea (1–2 lines) | Canonical Paper(s) | 2023–2025 Follow-ups | Typical Wins* | Accuracy & Trade-offs |\n",
        "|------------|------------|----------------------|--------------------|----------------------|----------------|-----------------------|\n",
        "| **Magnitude / iterative pruning** | Both | Remove small-magnitude weights; retrain to recover accuracy. | Chen et al., gradient checkpointing enabled deep nets; used widely alongside pruning. *arxiv.org* | One-shot LLM pruning (SparseGPT). *arxiv.org* | 2–10× params ↓; 1.2–2× speed on sparse-aware hw | Dense GPUs need structured sparsity or kernels; retraining usually required. |\n",
        "| **Structured filter/channel pruning** | Both | Prune channels/heads/filters to fit dense kernels. | BigBird/Longformer show structured sparsity patterns viable at attention level. *arxiv.org* +1 | Movement pruning (for Transformers). *arxiv.org* | 1.3–2× speed; 20–70% params ↓ | Larger drops if too aggressive; pick saliency-guided criteria. |\n",
        "| **N:M structured sparsity (2:4)** | Infer | Enforce hardware-friendly pattern for GEMM speedups. | BigBird/structured patterns; vendor docs (NVIDIA Ampere). *arxiv.org* | Wider N:M support in recent libs. | Up to ~1.5× GEMM on supported GPUs | Requires retraining/fine-tune to preserve accuracy. |\n",
        "| **PTQ W8A8 (SmoothQuant)** | Infer | Shift activation outliers into weights offline; enable W8A8. | SmoothQuant (2022/23). *arxiv.org* +1 | Broad adoption in LLM serving stacks. | ~1.3–1.6× speed; ~2× memory ↓ | Usually near-lossless on common LLMs; tune per-layer scaling. |\n",
        "| **LLM.int8** | Infer | Mixed outlier handling + int8 matmuls for proj/FFN. | Dettmers et al., 2022. *arxiv.org* +1 | — | ~2× memory ↓; modest throughput ↑ | Near-baseline accuracy; small quality drift on some tasks. |\n",
        "| **GPTQ (3–4-bit PTQ)** | Infer | Second-order, one-shot weight quantization for GPTs. | Frantar et al., 2022/ICLR’23. *arxiv.org* +1 | Many toolchains (2023–25). | 2–4× mem ↓; 1.2–1.8× speed | Per-layer bit-allocation and calibration critical. |\n",
        "| **AWQ (weight-only)** | Infer | Protect ~1% salient weights to cut quant error. | Lin et al., 2023/MLSys’24. *arxiv.org* +1 | Integrated in vLLM/TensorRT-LLM. | 2–4× mem ↓; small speed ↑ | Robust on edge; activations kept FP16/FP8. |\n",
        "| **QLoRA (NF4)** | Train | 4-bit base weights + LoRA adapters + paged optimizers. | Dettmers et al., 2023. *arxiv.org* +1 | NF8 variants, broader HF support. | 4–6× train mem ↓ | Quality near FT when data/task aligned. |\n",
        "| **Weight tying / ALBERT** | Both | Share embeddings/weights across layers to cut params. | Press & Wolf 2017; ALBERT 2019. *semanticscholar.org* | — | 1.5–3× params ↓ | Minimal loss; may affect capacity for heterogenous features. |\n",
        "| **Low-rank factorization (SVD/CP/Tucker)** | Infer | Factor conv/linear tensors into low-rank products. | Denton et al., 2014; Lebedev et al., 2014. *arxiv.org* | Modern SVD on attention/FFN blocks. | 1.3–2× speed; mem ↓ | Need layer-wise rank search; small quality drop typical. |\n",
        "| **LoRA adapters** | Train | Low-rank updates on frozen weights. | Hu et al., 2021 (LoRA). *arxiv.org* | QLoRA (2023). *arxiv.org* | 10–100× fewer trainable params | Inference cost ~unchanged; combine with quant/PTQ. |\n",
        "| **Knowledge Distillation (KD)** | Both | Match student to teacher logits/hidden states. | Hinton et al., 2015; DistilBERT (2019); TinyBERT (2020). *arxiv.org* +2 | Cross-modal KD for V+L; task-agnostic KD. | 1.5–10× smaller; 1.2–1.8× faster | Careful temperature/loss balancing; may underperform OOD. |\n",
        "| **Efficient long-seq attention** | Both | Approximate/sparse/low-rank attention to reduce O(L²). | Reformer, Linformer, Longformer, BigBird, Performer (2020). *arxiv.org* +4 | Kernel-fused FlashAttention for exact softmax. | >2–10× mem↓/speed↑ on long L | May change inductive bias; benchmark task-wise. |\n",
        "| **MQA / GQA** | Infer | Share K/V across heads or small groups to cut KV size; faster decode. | Shazeer 2019 (MQA); Ainslie et al., 2023 (GQA). *arxiv.org* +1 | Widely used in modern LLMs. | 1.3–1.8× tok/s ↑; KV mem ↓ | Minor quality drop for MQA; GQA narrows gap. |\n",
        "| **MoE / Switch** | Both | Route tokens to few experts (top-1/2) → sparse activation. | Fedus et al., 2021 (Switch). *arxiv.org* | Production MoE in 2024–25 families. | Pretrain up to 4–7× faster at iso-FLOPs | Requires careful routing/stability; inference sharding. |\n",
        "| **Early-exit / conditional depth** | Infer | Exit when confidence high; or drop layers adaptively. | Used in DeeBERT/PABEE lines. | — | Latency ↓ 20–50% on easy inputs | Must guard against biased exits; calibrate. |\n",
        "| **Mixed precision (FP16/bf16/FP8)** | Train | FP16/bf16 math with loss scaling; FP8 on Hopper-class GPUs. | Micikevicius et al., 2017. *arxiv.org* | FP8 serving/training in 2023–25 stacks. | 1.3–2× throughput; ~2× mem ↓ | Keep master weights FP32/bf16; watch overflow. |\n",
        "| **Activation/gradient checkpointing** | Train | Recompute activations on backward to save memory. | Chen et al., 2016. *arxiv.org* | Widely adopted in LLM training. | up to ~3–5× larger batch/ctx at same VRAM | Extra fwd compute (~20–30%). |\n",
        "| **Data curriculum / pruning / distillation** | Train | Order, select, or synthesize data to reduce steps. | Classic curriculum/self-paced; dataset distillation. *arxiv.org* | EL2N/forgetting-based selection (2021–24). | 1.2–3× wall-clock ↓ | Risk of overfitting to curated subsets; maintain diversity. |\n",
        "| **FlashAttention (kernels)** | Both | IO-aware tiled exact attention; v2/v3 improve throughput. | Tri Dao et al., 2022–23. *arxiv.org* | Integrated in vLLM / PyTorch. | 1.5–3× speed; big mem ↓ | Exact softmax; needs matching layouts. |\n",
        "| **vLLM: PagedAttention + continuous batching** | Infer | Page-based KV memory, near-zero waste; flexible sharing; continuous batching. | Kwon et al., 2023. *arxiv.org* | vTensor/Jenga/PagedEviction families. *arxiv.org* +2 | 2–4× throughput vs prior, esp. long ctx | Requires engine integration; page size tuning. |\n",
        "| **Speculative decoding** | Infer | Draft with small model/multi-heads; verify with target LM. | General technique; widely used since 2023. *arxiv.org* | Medusa / multi-draft variants. | 1.5–3× tok/s ↑ at similar quality | Gains shrink with high-quality targets/strict sampling. |\n",
        "| **Compiler/graph fusion (XLA/TensorRT-LLM/Triton)** | Both | Fuse ops; autotune kernels; AOT scheduling; memory planning. | Triton (Tillet et al., 2019). *IBM Research* | TensorRT-LLM 2023–25 releases. | 1.2–2× speed; latency tail ↓ | Engine-specific constraints; kernel portability. |\n",
        "\n",
        "---\n",
        "\n",
        "*Ballparks from cited papers and benchmarks; gains vary by model, context length, batch, hardware, and kernel maturity.*\n",
        "\n",
        "---\n",
        "\n",
        "## “When to Use” Decision Guide\n",
        "\n",
        "- **Long context throughput** → FlashAttention + MQA/GQA + PagedAttention (vLLM); add speculative decoding if draft model is cheap enough.  \n",
        "  *arxiv.org* +2\n",
        "\n",
        "- **VRAM bound at training** → mixed precision (bf16/FP16) + activation checkpointing + gradient accumulation; if still tight, use LoRA/QLoRA for fine-tuning.  \n",
        "  *arxiv.org* +2\n",
        "\n",
        "- **Edge/device inference** → AWQ/GPTQ weight-only or W8A8 (SmoothQuant); avoid activation quantization on extreme outlier layers.  \n",
        "  *arxiv.org* +2\n",
        "\n",
        "- **Latency spikes on easy inputs** → early-exit / adaptive depth + continuous batching. Monitor drift on hard tails.  \n",
        "\n",
        "- **Serving many small requests** → continuous batching + CUDA graphs + paged KV (vLLM/TensorRT-LLM).  \n",
        "  *arxiv.org*\n",
        "\n",
        "- **Model too big to fine-tune** → KD → smaller student or LoRA on frozen base; for fastest iteration, use QLoRA with 4-bit base.  \n",
        "  *arxiv.org* +1\n",
        "\n",
        "- **Compute budget fixed; need scale** → MoE/Switch (sparse activation) for larger capacity at constant FLOPs.  \n",
        "  *arxiv.org*\n",
        "\n",
        "---\n",
        "\n",
        "## Cross-Technique Playbooks\n",
        "\n",
        "### LLM Serving (Long Context, High Throughput)\n",
        "**GQA + FlashAttention + W8A8 (SmoothQuant) + vLLM (PagedAttention, continuous batching) + speculative decoding**  \n",
        "*arxiv.org* +3\n",
        "\n",
        "---\n",
        "\n",
        "### Memory-Tight Fine-Tuning (Single GPU)\n",
        "**QLoRA (NF4) + bf16/FP16 AMP + activation checkpointing; batch via gradient accumulation.**  \n",
        "Switch to **LoRA + AWQ (int4)** for ablations.  \n",
        "*arxiv.org* +2\n",
        "\n",
        "---\n",
        "\n",
        "### Edge / Device CNN–ViT Deployment\n",
        "**Weight-only PTQ (AWQ/GPTQ) + structured pruning + compiler fusion (TensorRT/Triton)**.  \n",
        "Validate calibration carefully.  \n",
        "*arxiv.org* +2\n",
        "\n",
        "---\n",
        "\n",
        "### Throughput-Oriented Cluster Training\n",
        "**MoE/Switch + bf16 AMP + checkpointing + efficient attention + ZeRO partitioning + curriculum pruning**.  \n",
        "*arxiv.org* +2\n",
        "\n",
        "---\n",
        "\n",
        "### Low-Latency Chat (Short Prompts)\n",
        "**MQA/GQA + FlashAttention + CUDA Graphs + int8/FP8 kernels + speculative decoding.**  \n",
        "*arxiv.org* +2\n",
        "\n",
        "---\n",
        "\n",
        "## Coverage Check\n",
        "\n",
        "- **Quantization:** LLM.int8 (Dettmers 2022), QLoRA (Dettmers 2023), SmoothQuant (Xiao 2022/23), GPTQ (Frantar 2022), AWQ (Lin 2023/24).  \n",
        "  *arxiv.org* +4\n",
        "\n",
        "- **Attention / Architectures:** Reformer, Linformer, Longformer, BigBird, Performer (2020); FlashAttention (2022–23); MQA (Shazeer 2019); GQA (Ainslie 2023); Switch Transformer (Fedus 2021).  \n",
        "  *arxiv.org* +8\n",
        "\n",
        "- **Serving:** vLLM + PagedAttention (Kwon 2023); vTensor, Jenga (2024–25).  \n",
        "  *arxiv.org* +2\n",
        "\n",
        "- **Compression:** Low-rank (Denton 2014), tensor decompositions (Lebedev 2014); weight tying/ALBERT; HashedNets.  \n",
        "  *arxiv.org* +2  \n",
        "  *semanticscholar.org* +2\n",
        "\n",
        "- **Training Efficiency:** Mixed precision (Micikevicius 2017); checkpointing (Chen 2016).  \n",
        "  *arxiv.org* +1\n",
        "\n",
        "- **Distillation:** Hinton 2015; DistilBERT 2019; TinyBERT 2020.  \n",
        "  *arxiv.org* +2\n",
        "\n",
        "---\n",
        "\n",
        "## Possible Gaps / Families to Add\n",
        "\n",
        "- Token-/head-level pruning for LLMs (2023–25)  \n",
        "- Prefix-caching & attention sinks for streaming LLMs  \n",
        "- Multi-token prediction (MTP)  \n",
        "- NAS variants (MnasNet)  \n",
        "- Vendor-specific FP8/FP4 details and 2:4 compiler passes (TensorRT-LLM/XLA releases)\n",
        "\n",
        "---\n",
        "\n",
        "## Notes on Scope Guard\n",
        "\n",
        "Ambiguous terms like “shuffling” are treated under **curriculum/data ordering and selection**; practical implementations rely on **curriculum/self-paced learning** and **example-selection metrics** (loss, gradient norms, forgetting events).  \n",
        "*arxiv.org*\n"
      ],
      "metadata": {
        "id": "0JUCl6y-8C4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Master Taxonomy (2014 → 2025)\n",
        "\n",
        "---\n",
        "\n",
        "## Model Compression\n",
        "\n",
        "### Pruning & Sparsity\n",
        "- **Magnitude & iterative pruning**; *Deep Compression* (Han et al., 2015) — iterative prune–retrain cycle.  \n",
        "- **Lottery Ticket Hypothesis** (Frankle & Carbin, 2019) — find sparse subnetworks that can train to full accuracy.  \n",
        "- **Movement pruning** (Sanh et al., 2020) — identify unimportant weights via gradient movement during fine-tuning.  \n",
        "- **Structured N:M (2:4) sparsity** — GPU-accelerated sparsity patterns (Ampere/Hopper).  \n",
        "- **Post-training sparse LLMs (SparseGPT)** and **activation-aware pruning (Wanda)** for one-shot LLM compression.  \n",
        "\n",
        "---\n",
        "\n",
        "### Quantization\n",
        "- **Post-training quantization** (LLM.int8, GPTQ, AWQ, SmoothQuant) — activation-aware or layerwise scaling.  \n",
        "- **Quantization-aware training (QAT)** — retrain with quant noise simulation.  \n",
        "- **Weight-only 4-bit quantization** — GPTQ, AWQ.  \n",
        "- **FP8 training/inference** — Transformer Engine (E4M3/E5M2).  \n",
        "- **NF4/NF8 (QLoRA)** — NormalFloat low-bit quantization for fine-tuning.  \n",
        "  *arxiv.org*  \n",
        "  +2  \n",
        "  *arxiv.org*  \n",
        "  +2  \n",
        "\n",
        "---\n",
        "\n",
        "### Weight Sharing / Tying\n",
        "- **HashedNets** (Chen et al., 2015) — hash multiple connections into shared parameters.  \n",
        "- **ALBERT** (Lan et al., 2019 / ICLR 2020) — cross-layer parameter sharing & factorized embeddings.  \n",
        "  *Proceedings of Machine Learning Research*  \n",
        "  +1  \n",
        "\n",
        "---\n",
        "\n",
        "### Low-Rank & Adapters\n",
        "- **LoRA / DoRA** — train low-rank update matrices for frozen models; efficient fine-tuning.  \n",
        "- **QLoRA (4-bit base + LoRA)** — combine 4-bit NF4 weights with low-rank adapters for low-memory finetuning.  \n",
        "  *arxiv.org*  \n",
        "\n",
        "---\n",
        "\n",
        "### Tensor Decompositions\n",
        "- **CP / Tucker / Tensor-Train (TT)** — compress convolutional and linear layers via low-rank factorization.  \n",
        "  *Note:* Classic pre-LLM techniques, now rarely used in Transformer compression.\n",
        "\n",
        "---\n",
        "\n",
        "### Distillation (KD)\n",
        "- **Hinton KD** — transfer soft labels from teacher to student.  \n",
        "- **DistilBERT**, **TinyBERT** — task-specific transformer distillation.  \n",
        "- **Modern LLM self- and speculative-aided distillation** — use the model itself or speculative drafts for supervision.\n",
        "\n",
        "---\n",
        "\n",
        "## Efficient Attention & Architecture\n",
        "\n",
        "### Kernel-Efficient Attention\n",
        "- **FlashAttention v1/v2** — IO-aware, tiled, exact attention computation.  \n",
        "- **Long-context attention** — Reformer (LSH), Linformer (low-rank), Longformer/BigBird (sparse), Performer (FAVOR+).\n",
        "\n",
        "---\n",
        "\n",
        "### MQA / GQA\n",
        "- **Multi-Query Attention (MQA)** — one KV per head.  \n",
        "- **Grouped-Query Attention (GQA)** — shared K/V across head groups for efficiency.\n",
        "\n",
        "---\n",
        "\n",
        "### Sparsely-Activated (MoE)\n",
        "- **Mixture-of-Experts (MoE)** — sparse activation of submodules; *GShard (2020)*, *Switch Transformer (2021)*, modern MoE LLMs.\n",
        "\n",
        "---\n",
        "\n",
        "### Early-Exit / Conditional Compute\n",
        "- **BranchyNet (2016)** — dynamic inference depth.  \n",
        "- **DeeBERT / PABEE (2020)** — confidence-based early exit during Transformer inference.\n",
        "\n",
        "---\n",
        "\n",
        "### NAS & Compound Scaling\n",
        "- **ENAS** — efficient neural architecture search.  \n",
        "- **EfficientNet scaling** — depth, width, and resolution compound scaling rules.\n",
        "\n",
        "---\n",
        "\n",
        "### State-Space / Hybrid Alternatives\n",
        "- **Mamba** — linear-time selective state-space models (SSMs).  \n",
        "- **Hybrid memory-augmented Transformers** for long-context tasks.  \n",
        "  *arxiv.org*  \n",
        "  +1  \n",
        "\n",
        "---\n",
        "\n",
        "## Training-Time Efficiency\n",
        "\n",
        "### Mixed Precision\n",
        "- **FP16/BF16 precision** — reduced-precision arithmetic with loss scaling.  \n",
        "- **FP8 Transformer Engine** — emerging standard for NVIDIA Hopper/Blackwell.  \n",
        "  *arxiv.org*  \n",
        "  +1  \n",
        "\n",
        "---\n",
        "\n",
        "### Checkpointing / Rematerialization\n",
        "- **Sublinear-memory training (O(√n))** — recompute activations during backprop to reduce memory footprint.  \n",
        "  *arxiv.org*  \n",
        "\n",
        "---\n",
        "\n",
        "### Dynamic Sparse Training\n",
        "- **RigL**, **Sparse Evolutionary Training (SET)** — dynamically evolve sparse connectivity during training.\n",
        "\n",
        "---\n",
        "\n",
        "### Depth Scheduling\n",
        "- **Stochastic Depth**, **LayerDrop** — probabilistically drop residual paths or layers for efficiency and regularization.\n",
        "\n",
        "---\n",
        "\n",
        "### Curriculum / Data Pruning / Distillation\n",
        "- **Curriculum learning** — schedule samples from easy to hard.  \n",
        "- **Influence/DataMap pruning** — remove low-utility data points.  \n",
        "- **Dataset distillation** — synthesize representative samples for faster training.\n",
        "\n",
        "---\n",
        "\n",
        "### Parallelism & Sharding (Systems)\n",
        "- **ZeRO / FSDP** — optimizer, gradient, and parameter partitioning.  \n",
        "- **4-D / compiler-aided parallelism (TorchTitan)** — advanced distributed model sharding.  \n",
        "  *arxiv.org*  \n",
        "  +1  \n",
        "\n",
        "---\n",
        "\n",
        "## Inference-Time Serving Optimizations\n",
        "\n",
        "### KV-Cache Management\n",
        "- **PagedAttention (vLLM)** — virtual-memory-like KV cache.  \n",
        "- **Continuous batching** — dynamic batch aggregation for concurrent requests.  \n",
        "- **GQA/MQA** — shrink KV cache footprint.  \n",
        "  *arxiv.org*  \n",
        "  +1  \n",
        "\n",
        "---\n",
        "\n",
        "### Speculative Decoding\n",
        "- **Draft-and-verify decoding** — use a smaller model to propose tokens and verify with a larger one.  \n",
        "- **Staged/block verification** and **self-speculative decoding** — modern latency-optimized variants.  \n",
        "  *arxiv.org*  \n",
        "  +3  \n",
        "  *arxiv.org*  \n",
        "  +3  \n",
        "  *arxiv.org*  \n",
        "  +3  \n",
        "\n",
        "---\n",
        "\n",
        "### Kernel / Graph Fusion\n",
        "- **TorchInductor (PyTorch 2)** → **Triton kernels** — fuse linear operations and reduce GPU kernel launches.  \n",
        "- **CUDA Graphs** — reduce CPU overhead for repeated operations.  \n",
        "  *dl.acm.org*  \n",
        "  +1  \n",
        "\n",
        "---\n",
        "\n",
        "### Tensor & Graph Optimizers\n",
        "- **TensorRT-LLM / ONNX / Flash-kernels** — optimize fused kernels with quantization-aware compilation.\n",
        "\n",
        "---\n",
        "\n",
        "### Scheduling\n",
        "- **Continuous batching, chunked prefill, beam/prompt KV sharing** — throughput scaling and reduced fragmentation in *vLLM*.  \n",
        "  *GitHub*\n",
        "\n",
        "---\n",
        "\n",
        "## Hardware / Format\n",
        "\n",
        "### Low-Precision Formats\n",
        "- **INT8 / INT4 weight-only quantization; W8A8 full quantization.**  \n",
        "- **FP8 (E4M3/E5M2)** — new numeric formats for mixed-precision compute.  \n",
        "  *arxiv.org*\n",
        "\n",
        "---\n",
        "\n",
        "### Structured Sparsity\n",
        "- **2:4 N:M sparsity** — hardware-accelerated structured sparsity on Ampere/Hopper GPUs.\n",
        "\n",
        "---\n",
        "\n",
        "### Ahead-of-Time (AOT) Compilers\n",
        "- **TensorRT-LLM, XLA/OpenXLA, Triton, PyTorch 2 compile path** — optimize static graphs and fusions for performance.  \n",
        "  *dl.acm.org*\n",
        "\n",
        "---\n",
        "\n",
        "## Evidence Table\n",
        "\n",
        "| Technique | Core Goal (train / infer / both) | Key Idea | Canonical Paper(s) | Notable 2023–2025 Follow-ups | Typical Speedup / Memory Wins* | Accuracy & Trade-offs |\n",
        "|------------|----------------------------------|-----------|--------------------|------------------------------|--------------------------------|-----------------------|\n",
        "| **Magnitude / Iterative Pruning** | both | Remove small weights and retrain | *Deep Compression* (Han et al., 2015) | Movement Pruning (Sanh et al., 2020) | 3–10× params ↓; speedup needs sparse kernels | Accuracy recoverable; requires fine-tuning |\n",
        "| **Lottery Ticket Hypothesis** | train | Sparse subnetworks train to full accuracy | Frankle & Carbin, 2019 | RigL integration | Param↓; variable training savings | Not plug-and-play for LLMs |\n",
        "| **2:4 Structured Sparsity** | both | N:M pattern for GPU sparse matmul | NVIDIA Ampere (2020) | Post-A100 adoption | 1.5–2× kernel speed | Retraining needed |\n",
        "| **LLM.int8** | infer | Outlier-aware 8-bit matmul w/ FP16 acts | Dettmers et al., 2022 | W8A8 variants | 1.2–1.6× speed; 50% mem↓ | Minor drift if outliers mishandled |\n",
        "| **GPTQ (PTQ)** | infer | 4-bit per-channel quantization | Frantar et al., 2022 | AutoRound | 2–4× mem↓; 1.3–2× speed | Depends on calibration data |\n",
        "| **SmoothQuant (W8A8)** | infer | Offline smoothing for 8-bit activations | Xiao et al., 2022 | TensorRT integration | 1.3–2× speed; mem↓ | Nearly lossless |\n",
        "| **AWQ (4-bit)** | infer | Protect salient channels via scaling | Lin et al., 2023 | Integrated in vLLM/TensorRT | 2–4× mem↓ | Robust; needs calibration |\n",
        "| **QLoRA (NF4)** | train | 4-bit base + LoRA adapters | Dettmers et al., 2023 | Instruction-tuning pipelines | 4–8× train mem↓ | Matches full-precision fine-tune |\n",
        "| **FP8 Training/Inference** | both | Low-bit FP8 arithmetic | Micikevicius et al., 2022 | Transformer Engine | 1.2–1.6× speed; mem↓ | Near BF16 if tuned |\n",
        "| **Hashed Weight Sharing** | both | Parameter hashing | Chen et al., 2015 | Balanced sharing (2023) | Large param↓ | Noise; best in over-param nets |\n",
        "| **ALBERT Tying** | train | Cross-layer weight sharing | Lan et al., 2019 | — | 2–3× param↓ | Slight capacity loss |\n",
        "| **LoRA / DoRA** | train | Low-rank finetuning | Hu et al., 2021 | DoRA (2024) | 10–100× fewer trainable params | Accuracy near full finetune |\n",
        "| **FlashAttention v1/v2** | both | IO-aware exact attention | Dao et al., 2022–2023 | TensorRT integration | 1.5–2.5× faster attention | Exact results |\n",
        "| **MQA / GQA** | infer | Shared K/V among heads | Shazeer 2019; Ainslie 2023 | Default in LLMs | KV mem↓ up to 8× | Minor quality drop |\n",
        "| **MoE (Switch / GShard)** | both | Token routing to few experts | Fedus 2021; Lepikhin 2020 | Production MoE | 2–4× training throughput | Router stability issues |\n",
        "| **Mamba (SSMs)** | both | Linear-time selective SSMs | Gu & Dao 2023 | Mamba-2 (2024–25) | O(L) inference | Maturing ecosystem |\n",
        "| **Checkpointing** | train | Activation recomputation | Chen et al., 2016 | Hybrid checkpointing (2024–25) | >50% mem↓ | 20–40% compute↑ |\n",
        "| **PagedAttention + Continuous Batching** | infer | Virtual KV cache, flexible batching | Kwon et al., 2023 | Jenga (2025) | 2–4× throughput | Scheduler tuning critical |\n",
        "| **Speculative Decoding** | infer | Draft small model, verify outputs | Chen et al., 2023 | Block/self variants | 1.5–2.5× latency↓ | Gains vary with temperature |\n",
        "| **TorchInductor → Triton Fusion** | both | Graph fusion & kernel optimization | Ansel et al., 2024 | CUDA Graphs integration | 1.2–2× speed | Limited dynamic support |\n",
        "\n",
        "---\n",
        "\n",
        "*Speedups from literature and practice; depend on hardware, sequence length, and kernel maturity.*\n",
        "\n",
        "---\n",
        "\n",
        "## “When to Use” Decision Guide\n",
        "\n",
        "- **Long contexts / high concurrency** → FlashAttention + vLLM (PagedAttention + continuous batching) + GQA/MQA; add speculative decoding if latency dominates.  \n",
        "  *arxiv.org* +1  \n",
        "\n",
        "- **Memory-bound serving** → Weight-only 4-bit (GPTQ/AWQ) or W8A8 (SmoothQuant) with FlashAttention kernels.  \n",
        "\n",
        "- **Finetuning on limited VRAM** → QLoRA (NF4) + LoRA + checkpointing + BF16/FP16 precision.  \n",
        "  *arxiv.org* +2  \n",
        "\n",
        "- **Training large dense models** → Mixed precision (BF16/FP8) + checkpointing + compiler fusion + stochastic depth.  \n",
        "  *arxiv.org* +2  \n",
        "\n",
        "- **Aggressive cost reduction** → Structured 2:4 pruning + brief KD refresh; fine-tune per-layer.\n",
        "\n",
        "- **Beyond Transformers (linear-time)** → Mamba or hybrid SSM-Transformer when quadratic attention is the bottleneck.  \n",
        "  *arxiv.org*\n",
        "\n",
        "---\n",
        "\n",
        "## Cross-Technique Playbooks\n",
        "\n",
        "### LLM Serving (Long Context)\n",
        "**GQA + FlashAttention-2 + W8A8 (SmoothQuant) + vLLM (PagedAttention + continuous batching) + speculative decoding.**  \n",
        "*arxiv.org* +1  \n",
        "\n",
        "### Cost-Efficient Instruction-Tuning (24–48 GB)\n",
        "**QLoRA (NF4) + LoRA + checkpointing + BF16 + torch.compile (Inductor→Triton).**  \n",
        "*dl.acm.org* +3  \n",
        "*arxiv.org* +3  \n",
        "\n",
        "### Multi-Tenant API Throughput\n",
        "**vLLM (continuous batching, chunked prefill) + FlashAttention + CUDA Graphs + TensorRT-LLM fusion.**  \n",
        "*GitHub* +1  \n",
        "\n",
        "### Sparse-Aware Acceleration\n",
        "**2:4 pruning + distillation + FP8 kernels; keep critical layers dense.**  \n",
        "*arxiv.org*\n",
        "\n",
        "### Long-Sequence Research\n",
        "**Mamba backbone / Reformer / Performer + LoRA adapters; evaluate against exact-attention baselines.**  \n",
        "*arxiv.org*\n",
        "\n",
        "---\n",
        "\n",
        "## Coverage Check (Primary Sources)\n",
        "\n",
        "- **Pruning & Sparsity:** Han (2015); Frankle & Carbin (2019); Movement Pruning (2020); NVIDIA 2:4 (2020).  \n",
        "- **Quantization:** LLM.int8 (2022); GPTQ (2022); SmoothQuant (2022); AWQ (2023); FP8 (2022); QLoRA/NF4 (2023).  \n",
        "  *arxiv.org* +2  \n",
        "- **Weight Sharing:** HashedNets (2015); ALBERT (2019).  \n",
        "  *Proceedings of Machine Learning Research* +1  \n",
        "- **Adapters:** LoRA (2021).  \n",
        "- **Attention & Architectures:** FlashAttention (2022–23); Reformer, Linformer, Longformer, Performer (2020); MQA (2019); GQA (2023); GShard (2020); Switch (2021); Mamba (2023).  \n",
        "  *arxiv.org*  \n",
        "- **Training Efficiency:** Mixed Precision (2017); Checkpointing (2016); Stochastic Depth (2016); LayerDrop (2019).  \n",
        "  *arxiv.org* +1  \n",
        "- **Serving Systems:** vLLM (2023); TensorRT-LLM; PyTorch 2 / TorchInductor (2024); CUDA Graphs.  \n",
        "  *arxiv.org* +2  \n",
        "  *dl.acm.org* +2  \n",
        "- **Decoding:** Speculative Decoding (2023–24).  \n",
        "  *arxiv.org* +2  \n",
        "\n",
        "---\n",
        "\n",
        "## Possible Gaps\n",
        "- Recent tensor-decomposition benchmarks for LLMs.  \n",
        "- Structured sparsity beyond 2:4 (Hopper+).  \n",
        "- Memory schedulers beyond vLLM (e.g., FlashInfer).  \n",
        "\n",
        "---\n",
        "\n",
        "## Notes\n",
        "Speedups are indicative; depend on sequence length, batch, and hardware.  \n",
        "“Lossless” denotes mathematically exact attention (FlashAttention) or identical output distribution (speculative decoding).  \n",
        "Quantization and approximate attention may introduce minor accuracy drift.\n"
      ],
      "metadata": {
        "id": "Xgux0zT38T0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Master Taxonomy (2014 → 2025)\n",
        "\n",
        "---\n",
        "\n",
        "## 1) Pruning & Sparsification\n",
        "\n",
        "- **Second-order (early) pruning:** *Optimal Brain Damage (OBD)*, *Optimal Brain Surgeon (OBS).*  \n",
        "  *ResearchGate*  \n",
        "  +1  \n",
        "\n",
        "- **Magnitude / iterative pruning (“learn-prune-retrain”):** *Learning both Weights and Connections* (Han et al.).  \n",
        "  *arxiv.org*  \n",
        "  +2  \n",
        "  *papers.nips.cc*  \n",
        "  +2  \n",
        "\n",
        "- **One-shot “at initialization” pruning:** *SNIP.*  \n",
        "  *arxiv.org*  \n",
        "  +1  \n",
        "\n",
        "- **Winning subnets / trainability:** *Lottery Ticket Hypothesis.*  \n",
        "  *arxiv.org*  \n",
        "  +2  \n",
        "  *openreview.net*  \n",
        "  +2  \n",
        "\n",
        "- **Movement-aware pruning for transfer learning / NLP:** *Movement Pruning.*  \n",
        "  *arxiv.org*  \n",
        "  +1  \n",
        "\n",
        "- **Dynamic sparse training:** *RigL* — sparse connectivity grown / pruned during training.  \n",
        "  *GitHub*  \n",
        "  +1  \n",
        "\n",
        "---\n",
        "\n",
        "## 2) Quantization (Weights / Activations / Compute)\n",
        "\n",
        "- **Int8 for Transformers (mixed outlier handling):** *LLM.int8().*  \n",
        "  *arxiv.org*  \n",
        "  +1  \n",
        "\n",
        "- **Post-training LLM quantization (second-order):** *GPTQ.*  \n",
        "  *arxiv.org*  \n",
        "  +1  \n",
        "\n",
        "- **Activation-aware weight quantization (INT3/4) for LLMs:** *AWQ.*  \n",
        "  *arxiv.org*  \n",
        "  +2  \n",
        "  *proceedings.mlsys.org*  \n",
        "  +2  \n",
        "\n",
        "- **Training-free W8A8 quant (smooth activations):** *SmoothQuant.*  \n",
        "  *arxiv.org*  \n",
        "  +2  \n",
        "  *arxiv.org*  \n",
        "  +2  \n",
        "\n",
        "- **Zero-cost PTQ pipeline for large Transformers (INT8/INT4):** *ZeroQuant.*  \n",
        "  *arxiv.org*  \n",
        "  +2  \n",
        "  *proceedings.neurips.cc*  \n",
        "  +2  \n",
        "\n",
        "- **Ultra-low precision BERT (Hessian-guided):** *Q-BERT.*  \n",
        "  *arxiv.org*  \n",
        "  +2  \n",
        "  *ojs.aaai.org*  \n",
        "  +2  \n",
        "\n",
        "- **Binary / ternary / extreme low-bit CNNs:** *BinaryNet / BNN, XNOR-Net, DoReFa-Net.*  \n",
        "  *arxiv.org*  \n",
        "  +5  \n",
        "  *arxiv.org*  \n",
        "  +5  \n",
        "  *papers.neurips.cc*  \n",
        "  +5  \n",
        "\n",
        "- **Quantized fine-tuning (parameter-efficient):** *QLoRA* (4-bit base + LoRA).  \n",
        "  *arxiv.org*  \n",
        "  +1  \n",
        "\n",
        "---\n",
        "\n",
        "## 3) Knowledge Distillation (Teacher → Student)\n",
        "\n",
        "- **Classical response-based KD:** *Hinton et al.* (2015).  \n",
        "  *robots.ox.ac.uk*  \n",
        "\n",
        "- **Intermediate hints / feature KD:** *FitNets.*  \n",
        "  *arxiv.org*  \n",
        "\n",
        "- **Self-distillation / Born-Again Nets:** *BAN.*  \n",
        "  *papers.neurips.cc*  \n",
        "\n",
        "- **Compact Transformer students:** *DistilBERT, TinyBERT, MobileBERT; Patient KD for BERT.*  \n",
        "  *ResearchGate*  \n",
        "  +3  \n",
        "  *arxiv.org*  \n",
        "  +3  \n",
        "  *arxiv.org*  \n",
        "  +3  \n",
        "\n",
        "---\n",
        "\n",
        "## 4) Low-Rank & Tensor Decomposition\n",
        "\n",
        "- **Low-rank factorization of conv/fc layers:** *Jaderberg et al.* (rank-1 conv), *Denton et al.* (linear structure).  \n",
        "  *arxiv.org*  \n",
        "\n",
        "- **Tensor (CP/Tucker) decompositions:** *Lebedev et al.; Tai et al.* (low-rank regularization).  \n",
        "  *openaccess.thecvf.com*  \n",
        "  *arxiv.org*  \n",
        "\n",
        "- **Low-rank adapters (efficient updates):** *LoRA* — often speeds fine-tuning and can reduce inference compute.  \n",
        "  *arxiv.org*  \n",
        "\n",
        "---\n",
        "\n",
        "## 5) Weight Sharing, Clustering & Entropy Coding\n",
        "\n",
        "- **Deep Compression pipeline:** *Prune → Quantize → Huffman coding* (*Han et al.*).  \n",
        "  *ResearchGate*  \n",
        "\n",
        "- **Hashed parameter sharing:** *HashedNets.*  \n",
        "\n",
        "---\n",
        "\n",
        "## 6) Efficient Network Operators & Architectures\n",
        "\n",
        "- **Depthwise-separable convs & inverted residuals:** *MobileNetV1*, *MobileNetV2.*  \n",
        "  *arxiv.org*  \n",
        "  +1  \n",
        "\n",
        "- **Group conv + channel shuffle:** *ShuffleNet.*  \n",
        "  *arxiv.org*  \n",
        "\n",
        "- **Parameter-lean CNN:** *SqueezeNet* (+Deep Compression).  \n",
        "  *arxiv.org*  \n",
        "\n",
        "- **Ghost modules (cheap operations):** *GhostNet.*  \n",
        "  *arxiv.org*  \n",
        "  +1  \n",
        "\n",
        "- **Compound scaling & NAS:** *EfficientNet; Once-for-All (OFANet).*  \n",
        "  *proceedings.neurips.cc*  \n",
        "  +1  \n",
        "\n",
        "---\n",
        "\n",
        "## 7) Attention / Transformer Efficiency (Sub-Quadratic or Memory-Aware)\n",
        "\n",
        "- **Reversible / efficient Transformers:** *Reformer* (LSH attention, reversible layers).  \n",
        "  *arxiv.org*  \n",
        "\n",
        "- **Low-rank projection attention:** *Linformer.*  \n",
        "  *arxiv.org*  \n",
        "\n",
        "- **Kernelized attention:** *Performer* (FAVOR+).  \n",
        "  *openaccess.thecvf.com*  \n",
        "\n",
        "- **Sparse / long-sequence patterns:** *Longformer*, *BigBird.*  \n",
        "  *arxiv.org*  \n",
        "  +1  \n",
        "\n",
        "- **Memory-optimal kernels:** *FlashAttention* (IO-aware exact attention).  \n",
        "  *bradmcdanel.com*  \n",
        "\n",
        "---\n",
        "\n",
        "## 8) Conditional Computation & Sparse Experts\n",
        "\n",
        "- **GShard:** Conditional computation and automatic sharding — foundational for MoE scaling.  \n",
        "  *arxiv.org*  \n",
        "\n",
        "- **Switch Transformer:** Simple MoE routing, faster pretraining.  \n",
        "  *arxiv.org*  \n",
        "  +1  \n",
        "\n",
        "- **Vision-MoE:** Sparse experts for ViT.  \n",
        "  *proceedings.neurips.cc*  \n",
        "\n",
        "---\n",
        "\n",
        "## 9) Dynamic Inference & Early Exiting / Layer Skipping\n",
        "\n",
        "- **Early-exit heads:** *BranchyNet; MSDNet (multi-exit DenseNets).*  \n",
        "  *ResearchGate*  \n",
        "\n",
        "- **Dynamic layer routing / skipping:** *SkipNet, BlockDrop.*  \n",
        "  *arxiv.org*  \n",
        "  +1  \n",
        "\n",
        "- **Early-exit Transformers for NLP:** *DeeBERT* (and related).  \n",
        "  *ResearchGate*  \n",
        "\n",
        "---\n",
        "\n",
        "## 10) Algorithmic / Kernel-Level Speedups\n",
        "\n",
        "- **Fast convolutions:** FFT-based convolution; *Winograd minimal filtering.*  \n",
        "  *arxiv.org*  \n",
        "  +1  \n",
        "\n",
        "---\n",
        "\n",
        "## 11) Architecture Growth / Sharing for Faster Training\n",
        "\n",
        "- **Function-preserving transforms:** *Net2Net* and later extensions for rapid model growth.  \n",
        "  *arxiv.org*  \n",
        "  +1  \n",
        "\n",
        "- **Parameter tying / sharing to reduce memory:** *ALBERT.*  \n",
        "  *arxiv.org*  \n",
        "  +1  \n",
        "\n",
        "---\n",
        "\n",
        "## 12) Automated, Hardware-Aware Compression / Search\n",
        "\n",
        "- **RL-driven compression:** *AMC (AutoML for Model Compression).*  \n",
        "  *arxiv.org*  \n",
        "  +1  \n",
        "\n",
        "- **Latency-measured simplification:** *NetAdapt.*  \n",
        "  *arxiv.org*  \n",
        "  +1  \n",
        "\n",
        "- **Meta-learned channel pruning:** *MetaPruning.*  \n",
        "  *arxiv.org*  \n",
        "  +1  \n",
        "\n",
        "---\n",
        "\n",
        "## 13) Fast Decoding for LLMs (Quality-Preserving)\n",
        "\n",
        "- **Speculative decoding:** Exact-output-distribution acceleration via draft-and-verify.  \n",
        "  *arxiv.org*  \n",
        "  +2  \n",
        "  *openreview.net*  \n",
        "  +2  \n",
        "\n",
        "- **Multi-token prediction heads (e.g., MEDUSA):** Predict multiple tokens per step.  \n",
        "  *arxiv.org*  \n",
        "  +2  \n",
        "  *dl.acm.org*  \n",
        "  +2  \n",
        "\n",
        "---\n",
        "\n",
        "## Notes\n",
        "\n",
        "- This taxonomy highlights **canonical**, **high-impact**, and **widely adopted** acceleration and compression methods from 2014–2025.  \n",
        "- **Emerging directions:** token pruning/merging for ViTs and LLMs, FP8/BF16 mixed-precision training, KV-cache optimizations, hybrid state-space architectures.  \n",
        "- **Composability:** Many techniques can be layered for multiplicative benefits — e.g., *(prune + quantize + distill)*, *(NAS + depthwise ops)*, *(MoE + FlashAttention).*  \n"
      ],
      "metadata": {
        "id": "O7YCL8PN8doA"
      }
    }
  ]
}
