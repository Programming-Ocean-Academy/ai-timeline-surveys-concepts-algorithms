{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Master Comparison Table Across Generative Model Families  \n",
        "\n",
        "\n",
        "| Aspect | Hopfield Network | Boltzmann Machine (BM) | Restricted Boltzmann Machine (RBM) | Deep Belief Network (DBN) | Diffusion Models (DDPM, Score Models) |\n",
        "|-------|------------------|-------------------------|-------------------------------------|-----------------------------|-----------------------------------------|\n",
        "| **Year / Pioneers** | 1982 – Hopfield | 1983–85 – Hinton, Ackley, Sejnowski | 1986 – Smolensky, Hinton | 2006 – Hinton, Osindero, Teh | 2019–2021 – Sohl-Dickstein → Song → Ho |\n",
        "| **Core Purpose** | Memory retrieval (associative memory) | Full probabilistic generative model | Efficient generative model; feature learning | Deep hierarchical generative model | High-quality generative modeling (images, audio, video) |\n",
        "| **State Type** | Deterministic binary neurons (+1/−1) | Stochastic binary neurons | Stochastic binary hidden units | Stacked stochastic layers | Continuous real-valued signals (Gaussian noise) |\n",
        "| **Energy Function** | Quadratic symmetric energy | Full Boltzmann energy with hidden units | Bipartite energy: $$E(v,h)$$ | Sum of RBM layer energies | Continuous energy gradient (score-based) |\n",
        "| **Stochasticity Level** | None (deterministic) | Fully stochastic Gibbs sampling | Stochastic but simplified | Layer-wise stochastic sampling | Continuous stochastic differential equations (SDEs) |\n",
        "| **Temperature Control** | Not used | Yes (controls randomness) | Typically fixed at \\(T=1\\) | Implicit via RBMs | Central idea via noise schedule \\(\\beta_t\\) |\n",
        "| **Input–Output Relation** | Attractor memory; converges to stored pattern | Joint distribution \\(P(v,h)\\) | Restricted joint distribution | Hierarchical generative mapping | No direct mapping; diffusion trajectory |\n",
        "| **Inference Mechanism** | Energy minimization | Long MCMC chains | Single-step conditional sampling | Downward sampling using trained RBMs | Reverse SDE or denoising ODE |\n",
        "| **Training Difficulty** | Very easy (Hebbian learning) | Extremely difficult (intractable partition function \\(Z\\)) | Moderate (Contrastive Divergence) | Hard but solvable via greedy layer-wise CD | Requires large nets but stable and scalable |\n",
        "| **Partition Function \\(Z\\)** | Closed form | Intractable | Approximate but easier | Inherited from RBMs | Not required (score-based models) |\n",
        "| **Sampling Process** | Deterministic convergence | Gibbs sampling; slow | Efficient Gibbs sampling | Layer-wise top-down sampling | Start from noise → iterative denoising |\n",
        "| **Memory vs. Generation** | Memory only | Generative | Generative + feature extraction | Generative hierarchical model | High-quality generative modeling |\n",
        "| **Architecture** | Fully connected symmetric | Fully connected symmetric | Bipartite graph (no intra-layer edges) | Stack of RBMs | U-Net backbone + noise schedule |\n",
        "| **Mathematical Philosophy** | Energy minimization | Gibbs distribution; statistical physics | Structured energy modeling | Deep latent generative process | Probabilistic diffusion in continuous space |\n",
        "| **Convergence Target** | Energy minima | Low-energy samples | Data distribution | Hierarchical latent representation | Data manifold via denoising |\n",
        "| **Best Known Uses** | Associative memory | Theoretical generative modeling | Pretraining and feature discovery | Pre-2012 deep learning foundation | Stable Diffusion, Imagen, DALLE-3 |\n",
        "| **Strengths** | Simple and interpretable | Elegant probabilistic theory | Efficient learning, strong features | Enabled deep learning revival | State-of-the-art generative quality |\n",
        "| **Weaknesses** | Limited capacity | Impossible to scale | Limited expressiveness | Outperformed by VAEs/Transformers | Heavy compute but improving |\n",
        "| **Relation to Physics** | Static energy surface | Full Boltzmann law | Simplified Boltzmann form | Composed Boltzmann layers | Stochastic thermodynamics, SDEs |\n",
        "| **Concept of Randomness** | None | Controlled via temperature | Conditional stochasticity | Layer sampling | Gaussian noise schedule |\n",
        "| **Does it start from randomness?** | No | Yes | Yes | Yes | Yes — explicitly from pure Gaussian noise |\n",
        "| **Noise Removal / Annealing** | Not applicable | Simulated annealing | CD negative phase | Greedy RBM annealing | Reverse diffusion = gradual noise removal |\n",
        "| **Connection to Modern Diffusion Models** | Conceptually related (energy minima) | Directly related (Boltzmann energy) | Related via energy modeling | Historical foundation | The modern extension of stochastic generative modeling |\n",
        "\n"
      ],
      "metadata": {
        "id": "XvhBCWHyJk3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deeper Conceptual Comparison (Narrative)\n",
        "\n",
        "## 1. Hopfield Network — Deterministic Memory  \n",
        "A Hopfield network behaves like an energy landscape full of valleys.  \n",
        "When you present a partial or noisy pattern, the system always slides **deterministically** into the nearest valley, retrieving the closest stored memory.  \n",
        "There is **no randomness**; only energy minimization.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Boltzmann Machine — Randomness and Statistical Physics  \n",
        "Imagine a marble rolling on a heated metal surface.  \n",
        "At **high temperature**, the marble jumps everywhere (high randomness).  \n",
        "As you **lower the temperature**, it settles into deeper and deeper valleys.  \n",
        "This is a **true probabilistic generative model**, sampling from the Boltzmann distribution.  \n",
        "Its mathematical foundation connects directly to modern diffusion and stochastic dynamics.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Restricted Boltzmann Machine (RBM) — Simplified Boltzmann  \n",
        "The RBM removes lateral connections within layers, creating a **bipartite structure**.  \n",
        "This single structural change makes sampling much faster and learning feasible.  \n",
        "Hinton introduced **Contrastive Divergence** to train RBMs efficiently.  \n",
        "This breakthrough reopened the path to deep learning around **2006**.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Deep Belief Networks — Layers of RBMs  \n",
        "A DBN stacks RBMs on top of each other.  \n",
        "Each RBM learns a distribution over the layer below it.  \n",
        "Through this stacking, a **deep hierarchy of increasingly abstract representations** emerges.  \n",
        "This was one of the earliest successes of deep learning before the rise of CNNs and Transformers.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Diffusion Models — Continuous Noise Removal  \n",
        "A diffusion model begins with **pure Gaussian noise**.  \n",
        "Then, it gradually removes noise using reverse diffusion or stochastic differential equations (SDEs).  \n",
        "This process eventually moves toward a **high-probability data sample**.  \n",
        "The mathematics connects directly to:  \n",
        "- Boltzmann energy  \n",
        "- Langevin dynamics  \n",
        "- Stochastic thermodynamics  \n",
        "\n",
        "Diffusion is the **modern rebirth** of stochastic generative modeling.\n",
        "\n",
        "---\n",
        "\n",
        "# Final Summary (One Sentence Each)\n",
        "\n",
        "**Hopfield:** deterministic memory retrieval through energy minimization.  \n",
        "**Boltzmann Machine:** a probabilistic neural network rooted in physics and randomness.  \n",
        "**RBM:** a simplified Boltzmann model enabling efficient training through a bipartite structure.  \n",
        "**DBN:** a deep generative hierarchy created by stacking RBMs.  \n",
        "**Diffusion Models:** generate samples by transforming pure noise into structure through iterative denoising.\n"
      ],
      "metadata": {
        "id": "9lqFy6v5Jvkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complete Mathematical Summary of Hopfield, Boltzmann, RBM, DBN, and Diffusion Models  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 1. Hopfield Network — Principal Equations\n",
        "\n",
        "### Energy Function\n",
        "$$\n",
        "E(s)= -\\frac12 \\sum_i \\sum_j w_{ij} s_i s_j - \\sum_i b_i s_i\n",
        "$$\n",
        "\n",
        "### Local Field\n",
        "$$\n",
        "h_i = \\sum_j w_{ij} s_j + b_i\n",
        "$$\n",
        "\n",
        "### Deterministic Update Rule\n",
        "$$\n",
        "s_i \\leftarrow \\text{sign}(h_i)\n",
        "$$\n",
        "\n",
        "### Hebbian Learning (Store \\(P\\) patterns)\n",
        "$$\n",
        "w_{ij}=\\frac{1}{P} \\sum_{\\mu=1}^{P} \\xi_i^{\\mu}\\,\\xi_j^{\\mu}\n",
        "$$\n",
        "\n",
        "### Energy Decrease Condition\n",
        "$$\n",
        "\\Delta E_i = -s_i h_i < 0\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "# 2. Boltzmann Machine — Principal Equations\n",
        "\n",
        "### Energy Function\n",
        "$$\n",
        "E(v,h)= -\\sum_{i<j} w_{ij} s_i s_j - \\sum_i b_i s_i\n",
        "$$\n",
        "\n",
        "### Boltzmann (Gibbs) Distribution\n",
        "$$\n",
        "P(s)=\\frac{1}{Z} e^{-E(s)/T},\n",
        "\\qquad\n",
        "Z=\\sum_s e^{-E(s)/T}\n",
        "$$\n",
        "\n",
        "### Neuron Activation Probability\n",
        "$$\n",
        "P(s_i=1\\mid \\text{rest})=\\sigma\\!\\left(\\frac{1}{T}\\sum_j w_{ij}s_j + b_i\\right)\n",
        "$$\n",
        "\n",
        "Where  \n",
        "$$\n",
        "\\sigma(x)=\\frac{1}{1+e^{-x}}.\n",
        "$$\n",
        "\n",
        "### Learning Rule\n",
        "$$\n",
        "\\Delta w_{ij}=\\eta\\left(\\langle s_i s_j\\rangle_{\\text{data}}-\\langle s_i s_j\\rangle_{\\text{model}}\\right)\n",
        "$$\n",
        "\n",
        "### Gibbs Sampling Update\n",
        "$$\n",
        "s_i^{t+1}\\sim \\text{Bernoulli}\\left(\\sigma\\!\\left(\\sum_j w_{ij}s_j^{t}+b_i\\right)\\right)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "# 3. Restricted Boltzmann Machine (RBM) — Principal Equations\n",
        "\n",
        "### Energy Function\n",
        "$$\n",
        "E(v,h)= -a^{\\top}v - b^{\\top}h - v^{\\top}Wh\n",
        "$$\n",
        "\n",
        "### Joint Distribution\n",
        "$$\n",
        "P(v,h)=\\frac{1}{Z} e^{-E(v,h)}\n",
        "$$\n",
        "\n",
        "### Conditional Distributions  \n",
        "(No intra-layer connections)\n",
        "\n",
        "**Visible given hidden**\n",
        "$$\n",
        "P(v_i=1\\mid h)=\\sigma\\left(a_i + \\sum_j w_{ij}h_j\\right)\n",
        "$$\n",
        "\n",
        "**Hidden given visible**\n",
        "$$\n",
        "P(h_j=1\\mid v)=\\sigma\\left(b_j + \\sum_i w_{ij}v_i\\right)\n",
        "$$\n",
        "\n",
        "### Contrastive Divergence Learning Rule\n",
        "$$\n",
        "\\Delta w_{ij}=\\eta\\left(\\langle v_i h_j\\rangle_{\\text{data}}\n",
        "-\n",
        "\\langle v_i h_j\\rangle_{\\text{model}}\\right)\n",
        "$$\n",
        "\n",
        "### CD-k Chain\n",
        "$$\n",
        "v^{(0)} \\rightarrow h^{(0)} \\rightarrow v^{(1)} \\rightarrow h^{(1)} \\rightarrow \\cdots \\rightarrow v^{(k)}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "# 4. Deep Belief Networks (DBN) — Principal Equations\n",
        "\n",
        "### Layer-wise RBM Energy (Layer \\(l\\))\n",
        "$$\n",
        "E^{(l)}(v^{(l)},h^{(l)})=\n",
        "-a^{(l)\\top}v^{(l)}-b^{(l)\\top}h^{(l)}-v^{(l)\\top}W^{(l)}h^{(l)}\n",
        "$$\n",
        "\n",
        "### Greedy Layer-wise Learning\n",
        "$$\n",
        "\\Delta W^{(l)}=\n",
        "\\eta\\left(\n",
        "\\langle v^{(l)}h^{(l)}\\rangle_{\\text{data}}\n",
        "-\n",
        "\\langle v^{(l)}h^{(l)}\\rangle_{\\text{model}}\n",
        "\\right)\n",
        "$$\n",
        "\n",
        "### DBN Joint Distribution (Two-Layer Example)\n",
        "$$\n",
        "P(v,h^{(1)},h^{(2)})\n",
        "=\n",
        "P(h^{(1)},h^{(2)})\n",
        "\\prod_i P(v_i\\mid h^{(1)})\n",
        "$$\n",
        "\n",
        "Top layer behaves like a Boltzmann Machine; lower layers like conditional RBMs.\n",
        "\n",
        "### Downward Generative Process\n",
        "$$\n",
        "h^{(2)} \\rightarrow h^{(1)} \\rightarrow v\n",
        "$$\n",
        "\n",
        "### Upward Inference\n",
        "$$\n",
        "v \\rightarrow h^{(1)} \\rightarrow h^{(2)}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "# 5. Diffusion Models — Principal Equations\n",
        "\n",
        "### Forward Diffusion (Add Noise)\n",
        "$$\n",
        "q(x_t\\mid x_{t-1}) =\n",
        "\\mathcal{N}\\!\\Big(x_t;\\, \\sqrt{1-\\beta_t}\\,x_{t-1}, \\beta_t I\\Big)\n",
        "$$\n",
        "\n",
        "Closed-form:\n",
        "$$\n",
        "q(x_t\\mid x_0)=\\mathcal{N}\\!\\Big(x_t;\\, \\sqrt{\\bar{\\alpha}_t}\\,x_0,\\,(1-\\bar{\\alpha}_t)I\\Big)\n",
        "$$\n",
        "\n",
        "Where  \n",
        "$$\n",
        "\\alpha_t = 1-\\beta_t,\n",
        "\\qquad\n",
        "\\bar{\\alpha}_t=\\prod_{s=1}^t \\alpha_s.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Reverse Diffusion\n",
        "$$\n",
        "p_\\theta(x_{t-1}\\mid x_t)=\n",
        "\\mathcal{N}\\!\\left(x_{t-1};\\, \\mu_\\theta(x_t,t),\\,\\Sigma_\\theta(x_t,t)\\right)\n",
        "$$\n",
        "\n",
        "### Mean Prediction (Using Noise Predictor)\n",
        "$$\n",
        "\\mu_\\theta(x_t,t)\n",
        "=\n",
        "\\frac{1}{\\alpha_t}\n",
        "\\left(\n",
        "x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\,\n",
        "\\epsilon_\\theta(x_t,t)\n",
        "\\right)\n",
        "$$\n",
        "\n",
        "### Noise Prediction Loss\n",
        "$$\n",
        "L_{\\text{simple}}\n",
        "=\n",
        "\\mathbb{E}_{x_0,t,\\epsilon}\\left[ \\|\\epsilon - \\epsilon_\\theta(x_t,t)\\|^2 \\right]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Reverse SDE (Score-Based Diffusion)\n",
        "$$\n",
        "dx=\n",
        "\\Big[\n",
        "f(x,t)\n",
        "-\n",
        "\\frac{g(t)^2}{2}\\nabla_x \\log p_t(x)\n",
        "\\Big]dt\n",
        "+\n",
        "g(t)\\, d\\bar{w}\n",
        "$$\n",
        "\n",
        "Where the learned **score** is:\n",
        "$$\n",
        "s_\\theta(x,t)=\\nabla_x \\log p_t(x)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Sampling by Reverse SDE (Langevin-Type)\n",
        "$$\n",
        "x_{t-\\Delta t}\n",
        "=\n",
        "x_t\n",
        "+\n",
        "\\Big[\n",
        "f(x_t,t)\n",
        "-\n",
        "\\frac{g(t)^2}{2}\\,s_\\theta(x_t,t)\n",
        "\\Big]\\Delta t\n",
        "+\n",
        "g(t)\\sqrt{\\Delta t}\\,z\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "# Ultra-Short Summary of Key Equations\n",
        "\n",
        "| Model | Core Equation |\n",
        "|-------|----------------|\n",
        "| Hopfield | $$E=-\\frac12\\sum w_{ij}s_i s_j$$ |\n",
        "| Boltzmann Machine | $$P(s)=\\frac{1}{Z}e^{-E/T}$$ |\n",
        "| RBM | $$E=-a^{\\top}v - b^{\\top}h - v^{\\top}Wh$$ |\n",
        "| DBN | $$P(v,h)=\\text{RBM} \\times \\text{BM}$$ |\n",
        "| Diffusion | $$q(x_t\\mid x_0)=\\mathcal{N}(x_t;\\sqrt{\\bar{\\alpha}_t}x_0,(1-\\bar{\\alpha}_t)I)$$ |\n",
        "\n"
      ],
      "metadata": {
        "id": "LuCgWqw8KoW1"
      }
    }
  ]
}