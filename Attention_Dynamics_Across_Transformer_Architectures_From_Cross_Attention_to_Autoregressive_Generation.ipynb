{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Dynamics Across Transformer Architectures: From Cross-Attention to Autoregressive Generation\n",
        "\n",
        "Cross-attention exists in both training and inference, but the way it’s executed differs slightly because of how the decoder’s inputs are handled.\n",
        "\n",
        "## 1. During Training (Teacher Forcing Mode)\n",
        "\n",
        "In training, we already know the full target sequence (the correct translation).  \n",
        "So, the decoder can process the entire sequence in parallel.\n",
        "\n",
        "### What Happens:\n",
        "\n",
        "- The encoder encodes the source sentence (same as inference).  \n",
        "- The decoder receives all target tokens shifted by one position (e.g., `<start> ¿Cómo estás` instead of `¿Cómo estás?`) so it can learn to predict the next token at each position.  \n",
        "- Inside each decoder layer:\n",
        "  - Masked self-attention ensures that position *t* cannot “see” tokens after *t*.\n",
        "  - Cross-attention uses the encoder’s outputs (K, V) and the decoder’s current hidden states (Q) to compute alignment.\n",
        "\n",
        "### Parallel Computation\n",
        "\n",
        "Because the whole target sentence is known, all decoder time steps are processed in parallel, with masking enforcing causality.  \n",
        "So, the cross-attention matrix\n",
        "\n",
        "$$\n",
        "\\frac{QK^T}{\\sqrt{d_k}}\n",
        "$$\n",
        "\n",
        "is computed for all target positions at once.\n",
        "\n",
        "### Q, K, V Sources (same as inference)\n",
        "\n",
        "| Symbol | Source | Description |\n",
        "| ------- | ------- | ----------- |\n",
        "| Q | Decoder hidden states (current layer) | Each target position’s query |\n",
        "| K | Encoder outputs | Source-side context keys |\n",
        "| V | Encoder outputs | Source-side context values |\n",
        "\n",
        "### Summary\n",
        "\n",
        "Cross-attention here learns how to align target tokens with relevant source tokens through loss minimization (usually cross-entropy on predicted next-token probabilities).\n",
        "\n",
        "---\n",
        "\n",
        "## 2. During Inference (Autoregressive Generation Mode)\n",
        "\n",
        "In inference, the model must generate tokens one by one, because the ground truth future tokens are unknown.\n",
        "\n",
        "### What Happens:\n",
        "\n",
        "- Encoder runs once — producing a fixed set of K, V vectors for the source sequence (these are cached for efficiency).  \n",
        "- Decoder runs in a loop:\n",
        "  - At each step *t*, it has previously generated tokens [y₁, y₂, …, yₜ₋₁].\n",
        "  - It performs:\n",
        "    - Masked self-attention: attends only to previously generated tokens.\n",
        "    - Cross-attention: uses the same encoder K, V (fixed memory) and the decoder’s new Q (for the next token).\n",
        "  - The output passes through the linear + softmax layer to predict the next token yₜ.\n",
        "  - The predicted token yₜ is appended to the sequence, and the process repeats.\n",
        "\n",
        "### Incremental Computation\n",
        "\n",
        "- Self-attention can be cached (previous K, V from decoder are reused) to avoid recomputation.  \n",
        "- Cross-attention always uses the same encoder K, V, since the source does not change.\n",
        "\n",
        "---\n",
        "\n",
        "## Core Difference\n",
        "\n",
        "| Phase | Decoder Input | Parallelization | Cross-Attention K,V |\n",
        "| ------ | -------------- | ---------------- | -------------------- |\n",
        "| Training | Full target sentence (shifted) | Parallel across time steps | Computed once per batch |\n",
        "| Inference | Generated tokens (step-by-step) | Sequential (token by token) | Encoder outputs cached and reused |\n",
        "\n",
        "---\n",
        "\n",
        "## Mathematical Consistency\n",
        "\n",
        "Even though the runtime behavior differs, the mathematical form of cross-attention is identical:\n",
        "\n",
        "$$\n",
        "\\text{CrossAttn}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "Only how **Q** is produced differs:\n",
        "\n",
        "- In training → computed for all target positions in parallel.  \n",
        "- In inference → computed incrementally for one position at a time.\n",
        "\n",
        "---\n",
        "\n",
        "## Visualization Summary\n",
        "\n",
        "| Stage | Encoder | Decoder | Cross-Attention Role |\n",
        "| ------ | -------- | -------- | -------------------- |\n",
        "| Training | Encodes all source tokens once | Uses all target tokens (teacher forcing) | Learns alignment patterns from gold data |\n",
        "| Inference | Encodes once | Generates sequentially (auto-regressive) | Uses learned patterns to align new tokens with source |\n",
        "\n",
        "---\n",
        "\n",
        "## Final Insight\n",
        "\n",
        "Cross-attention is the same mechanism in both modes —  \n",
        "the difference is procedural: in training, it runs in parallel with teacher-forced target sequences;  \n",
        "in inference, it runs sequentially with cached encoder memory and dynamically updated decoder queries.\n"
      ],
      "metadata": {
        "id": "EqzaQe1tjm5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conceptual Goal\n",
        "\n",
        "Cross-attention (also called encoder–decoder attention) allows the decoder to “look at” or “attend to” the encoder’s output while generating the translation (or any output sequence).\n",
        "\n",
        "It’s what connects the source language representation (encoder output) to the target generation process (decoder state).\n",
        "\n",
        "---\n",
        "\n",
        "## Step-by-Step Mechanism\n",
        "\n",
        "Let’s assume:\n",
        "\n",
        "- The encoder has already processed the input sequence (e.g., an English sentence).  \n",
        "  It produced contextual embeddings  \n",
        "\n",
        "  $$\n",
        "  H_{enc} \\in \\mathbb{R}^{n_{src} \\times d_{model}}\n",
        "  $$\n",
        "\n",
        "- The decoder is now generating tokens one by one (e.g., Spanish words).  \n",
        "  Its current hidden states are  \n",
        "\n",
        "  $$\n",
        "  H_{dec} \\in \\mathbb{R}^{n_{tgt} \\times d_{model}}\n",
        "  $$\n",
        "\n",
        "Now, at a given decoder layer that includes cross-attention, we compute:\n",
        "\n",
        "$$\n",
        "Q = W_Q \\cdot H_{dec}\n",
        "$$\n",
        "\n",
        "$$\n",
        "K = W_K \\cdot H_{enc}\n",
        "$$\n",
        "\n",
        "$$\n",
        "V = W_V \\cdot H_{enc}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Where They Come From\n",
        "\n",
        "| Vector | Source | Meaning |\n",
        "| ------- | ------- | ------- |\n",
        "| **Q (Query)** | Comes from the decoder’s current hidden states (output of the previous decoder sub-layer) | Represents what the decoder is currently trying to find or focus on. |\n",
        "| **K (Key)** | Comes from the encoder’s output representations | Describes what information is available in the encoded source. |\n",
        "| **V (Value)** | Also comes from the encoder’s output representations | Carries the actual content to be retrieved and used for generation. |\n",
        "\n",
        "So, the decoder “asks” using **Q** what information it needs,  \n",
        "and the encoder “answers” using **K** and **V** — the keys tell where the information is, and the values give what it is.\n",
        "\n",
        "---\n",
        "\n",
        "## Computation\n",
        "\n",
        "### Compute attention scores\n",
        "\n",
        "$$\n",
        "\\text{scores} = \\frac{QK^T}{\\sqrt{d_k}}\n",
        "$$\n",
        "\n",
        "→ Measures how relevant each encoder token is to the current decoder token.\n",
        "\n",
        "### Normalize via softmax\n",
        "\n",
        "$$\n",
        "\\text{weights} = \\text{softmax}(\\text{scores})\n",
        "$$\n",
        "\n",
        "→ Turns them into probabilities (focus distribution over source words).\n",
        "\n",
        "### Combine with values\n",
        "\n",
        "$$\n",
        "\\text{Context} = \\text{weights} \\times V\n",
        "$$\n",
        "\n",
        "→ Produces a weighted sum of encoder representations, forming a context vector for each target position.\n",
        "\n",
        "### Feed into next sub-layer\n",
        "\n",
        "→ The context is merged with decoder representations via residual connections and normalization.\n",
        "\n",
        "---\n",
        "\n",
        "## Intuitive Analogy\n",
        "\n",
        "Think of the decoder as a translator:\n",
        "\n",
        "- **Query (Q):** The translator’s current mental focus — “I’m about to produce the next Spanish word; which English words matter now?”  \n",
        "- **Keys (K):** The labels on the translator’s notebook (encoder outputs) — “These correspond to each English word.”  \n",
        "- **Values (V):** The actual meanings or concepts written in that notebook.  \n",
        "\n",
        "The attention matrix tells how much the translator looks at each note before speaking the next word.\n",
        "\n",
        "---\n",
        "\n",
        "## Contrast with Self-Attention\n",
        "\n",
        "| Type | Q source | K/V source | Purpose |\n",
        "| ------ | -------- | ----------- | -------- |\n",
        "| **Encoder self-attention** | Encoder | Encoder | Capture relationships between source tokens |\n",
        "| **Decoder self-attention** | Decoder | Decoder | Capture relationships between generated tokens (masked) |\n",
        "| **Cross-attention** | Decoder | Encoder | Align generated tokens with encoded source information |\n",
        "\n",
        "---\n",
        "\n",
        "## Final Intuition\n",
        "\n",
        "At each decoding step:\n",
        "\n",
        "- The decoder’s **Q** asks: “Given what I’ve generated so far, which parts of the source sentence are relevant?”  \n",
        "- The encoder’s **K, V** provide the memory of the input.  \n",
        "\n",
        "Cross-attention computes a weighted contextual summary from the encoder that directly guides the decoder’s next prediction.\n"
      ],
      "metadata": {
        "id": "pf2V_1HdjtfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Context\n",
        "\n",
        "In machine translation, we have two neural components:\n",
        "\n",
        "- **Encoder:** processes the source sentence (e.g., French input)  \n",
        "- **Decoder:** generates the target sentence (e.g., English output)\n",
        "\n",
        "Both produce internal vector representations, and **cross-attention** is the bridge that lets the decoder look back at the encoder’s output while generating each target token.\n",
        "\n",
        "---\n",
        "\n",
        "## Where do Q, K, and V come from?\n",
        "\n",
        "### 1. Self-Attention (inside the Encoder or Decoder)\n",
        "\n",
        "In **self-attention**, Q, K, and V all come from the same sequence.\n",
        "\n",
        "Example (encoder self-attention):\n",
        "\n",
        "$$\n",
        "Q = W_Q \\cdot H_{enc}\n",
        "$$\n",
        "\n",
        "$$\n",
        "K = W_K \\cdot H_{enc}\n",
        "$$\n",
        "\n",
        "$$\n",
        "V = W_V \\cdot H_{enc}\n",
        "$$\n",
        "\n",
        "This allows the model to capture **intra-sentence dependencies** within the same side (source or target).\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Cross-Attention (the bridge between encoder and decoder)\n",
        "\n",
        "In **cross-attention**, the sources differ:\n",
        "\n",
        "| Component | Comes From | Meaning |\n",
        "| ---------- | ----------- | -------- |\n",
        "| **Q (Query)** | Decoder hidden states $H_{dec}$ | Represents what the decoder is currently trying to generate (context of the target so far). |\n",
        "| **K (Key)** | Encoder output $H_{enc}$ | Represents the meaning of the source tokens. |\n",
        "| **V (Value)** | Encoder output $H_{enc}$ | Provides the semantic content of the source tokens. |\n",
        "\n",
        "So, in formulas:\n",
        "\n",
        "$$\n",
        "Q = W_Q \\cdot H_{dec}\n",
        "$$\n",
        "\n",
        "$$\n",
        "K = W_K \\cdot H_{enc}\n",
        "$$\n",
        "\n",
        "$$\n",
        "V = W_V \\cdot H_{enc}\n",
        "$$\n",
        "\n",
        "Then attention weights are computed as:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "This allows the decoder to attend to relevant parts of the source sentence while producing each target word.\n",
        "\n",
        "---\n",
        "\n",
        "## Intuitive Picture\n",
        "\n",
        "Think of it like a **translator (decoder)** looking back at their **notes (encoder output):**\n",
        "\n",
        "- The translator’s current mental query (**Q**) depends on what word they’re about to produce.  \n",
        "- The **keys (K)** describe all the possible “source meanings” encoded from the input.  \n",
        "- The **values (V)** are the actual contextual embeddings of those source words.  \n",
        "\n",
        "Cross-attention matches **Q** with **K** to find which parts of the input are relevant,  \n",
        "and then extracts their information from **V**.\n",
        "\n",
        "---\n",
        "\n",
        "## Summary Table\n",
        "\n",
        "| Attention Type | Q Source | K Source | V Source | Purpose |\n",
        "| --------------- | -------- | -------- | -------- | -------- |\n",
        "| **Encoder Self-Attention** | Encoder | Encoder | Encoder | Capture source token relations |\n",
        "| **Decoder Self-Attention** | Decoder | Decoder | Decoder | Capture target token relations (masked) |\n",
        "| **Cross-Attention** | Decoder | Encoder | Encoder | Align target generation with source meaning |\n"
      ],
      "metadata": {
        "id": "e21cS-vSj4AC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT (Decoder-Only Transformer) — Training vs. Inference\n",
        "\n",
        "---\n",
        "\n",
        "### Training Mode\n",
        "\n",
        "- **Input:** a full text sequence (e.g., a paragraph).  \n",
        "- The model sees **all previous tokens** and learns to **predict the next token** at each position.  \n",
        "- Uses **causal masking** → token *t* can only attend to tokens ≤ *t*.  \n",
        "- All positions are processed **in parallel** (teacher forcing).  \n",
        "- **Loss:** cross-entropy between predicted next token and the true next token.  \n",
        "\n",
        " **Goal:** learn next-token prediction patterns across massive data.\n",
        "\n",
        "---\n",
        "\n",
        "### Inference Mode\n",
        "\n",
        "- **Input:** prompt tokens (user query).  \n",
        "- The model **generates one token at a time** (auto-regressive).  \n",
        "- At each step:\n",
        "  - Uses **cached keys & values** from prior steps (self-attention memory).  \n",
        "  - Computes attention only for the new token.  \n",
        "  - Samples or picks the next token → appends it to the sequence.  \n",
        "- Repeats until stop condition (e.g., `<EOS>` or max length).  \n",
        "\n",
        " **Goal:** generate coherent continuation, one token per step.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary Table\n",
        "\n",
        "| Phase | Access to Future Tokens | Parallelism | Caching | Purpose |\n",
        "|-------|--------------------------|--------------|----------|----------|\n",
        "| **Training** | No (masked) | Yes (all tokens at once) | Not needed | Learn next-token prediction |\n",
        "| **Inference** | No | Sequential (one token at a time) | Yes (reuse past K,V) | Generate new text |\n"
      ],
      "metadata": {
        "id": "uUkF7n7nj-Sp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1️ What Is *Teacher Forcing*?\n",
        "\n",
        "**Teacher forcing** is a **training technique** for sequence models where,\n",
        "at each time step, the model is given the **true previous token (from the training data)** rather than its **own predicted token** as input.\n",
        "\n",
        "It’s called *“teacher forcing”* because during training, the *teacher* (ground-truth data) **forces** the model to stay on the correct path through the sequence.\n",
        "\n",
        "---\n",
        "\n",
        "## 2️ Example (Intuitive)\n",
        "\n",
        "Suppose we train a model to generate a translation:  \n",
        "**Input (English):** “How are you?”  \n",
        "**Output (Spanish):** “¿Cómo estás?”\n",
        "\n",
        "Let’s say at time step 1:\n",
        "\n",
        "* Model sees `<start>` → predicts “¿”.\n",
        "\n",
        "At time step 2:\n",
        "\n",
        "* **With teacher forcing:** the input is the **true previous word**, “¿”.\n",
        "* **Without teacher forcing:** the input would be the **model’s own prediction**, which might be wrong (“De”, for example).\n",
        "\n",
        "So the model always gets the *correct prefix* during training, even if its previous prediction was incorrect.\n",
        "\n",
        "---\n",
        "\n",
        "## 3️ How It Works Mathematically\n",
        "\n",
        "For an output sequence $ (y_1, y_2, \\dots, y_T) $:\n",
        "\n",
        "At each step $ t $, the decoder input is:\n",
        "\n",
        "$$\n",
        "x_t =\n",
        "\\begin{cases}\n",
        "y_{t-1}^{\\text{true}} & \\text{(teacher forcing)} \\\\\n",
        "\\hat{y}_{t-1} & \\text{(free running / inference)}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "And the loss (usually cross-entropy) is computed on the predicted probability of the next token:\n",
        "\n",
        "$$\n",
        "L = -\\sum_{t} \\log P(y_t^{\\text{true}} \\mid y_{<t}^{\\text{true}}, x)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 4️ Why We Use It\n",
        "\n",
        "| Benefit | Explanation |\n",
        "| -------- | ------------ |\n",
        "| **Faster convergence** | The model always follows correct input context, avoiding compounding mistakes early in training. |\n",
        "| **Stable gradients** | Errors remain local to each step instead of exploding over long sequences. |\n",
        "| **Easier optimization** | The target at every step is known and unambiguous. |\n",
        "\n",
        "In short, teacher forcing gives the model a **“clean supervised signal”** at every time step, making learning efficient.\n",
        "\n",
        "---\n",
        "\n",
        "## 5️ The Limitation — *Exposure Bias*\n",
        "\n",
        "During **inference**, the model doesn’t have access to the true tokens anymore — it must feed **its own predictions** back as input.\n",
        "\n",
        "This causes **a mismatch between training and inference conditions**:\n",
        "\n",
        "| During Training | During Inference |\n",
        "| ---------------- | ---------------- |\n",
        "| Inputs are always correct (ground truth). | Inputs may be wrong (predicted tokens). |\n",
        "| Errors don’t accumulate. | Errors can snowball through the sequence. |\n",
        "\n",
        "This discrepancy is called **exposure bias**, and it can make a model generate nonsensical text if early predictions go off track.\n",
        "\n",
        "---\n",
        "\n",
        "## 6️ Techniques to Mitigate It\n",
        "\n",
        "1. **Scheduled Sampling** (Bengio et al., 2015)  \n",
        "   * Gradually replace teacher-forced inputs with the model’s own predictions during training.  \n",
        "   * Probability of using the true token decays over epochs.\n",
        "\n",
        "2. **Professor Forcing** (Lamb et al., 2016)  \n",
        "   * Uses an adversarial objective to align the model’s hidden-state dynamics between training (teacher-forced) and inference modes.\n",
        "\n",
        "3. **Data Augmentation / Noising**  \n",
        "   * Add random perturbations to inputs to improve robustness against prediction errors.\n",
        "\n",
        "4. **Reinforcement Learning Fine-Tuning** (e.g., RLHF in ChatGPT)  \n",
        "   * Let the model experience its own outputs and learn reward signals from human preferences or task-specific objectives.\n",
        "\n",
        "---\n",
        "\n",
        "## 7️ Teacher Forcing in GPT vs. Seq2Seq\n",
        "\n",
        "| Model Type | How Teacher Forcing Appears |\n",
        "| ----------- | ---------------------------- |\n",
        "| **Encoder–Decoder (e.g., translation)** | Decoder gets gold previous tokens at every step during training. |\n",
        "| **Decoder-Only (GPT)** | Model is trained on contiguous text; masking ensures each position predicts the next token using all true previous tokens. |\n",
        "| **Inference** | No teacher forcing; the model feeds its own outputs sequentially. |\n",
        "\n",
        "In GPT, it’s essentially *teacher forcing with causal masking* across the whole sequence.\n",
        "\n",
        "---\n",
        "\n",
        "## 8️ Summary\n",
        "\n",
        "**Teacher forcing = feeding the ground truth as previous input during training.**\n",
        "\n",
        "* Helps model learn quickly and accurately.  \n",
        "* Causes a mismatch with inference (exposure bias).  \n",
        "* Addressed via scheduled sampling, adversarial, or RL fine-tuning.  \n",
        "* In GPT: training = teacher forcing; inference = autoregressive generation.\n"
      ],
      "metadata": {
        "id": "mYffXNX8l0W0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Foundational Papers in NLP & RNN and Transformer Evolution\n",
        "\n",
        "---\n",
        "\n",
        "##  1️ **Sequence Modeling Foundations**\n",
        "\n",
        "| Year     | Authors                                   | Title / Venue                                                                                   | Key Contribution                                                             |\n",
        "| -------- | ----------------------------------------- | ----------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |\n",
        "| **1989** | Rumelhart, Hinton & Williams              | *Learning representations by back-propagating errors*                                           | Introduced backpropagation — foundation for all neural sequence models.      |\n",
        "| **1997** | Sepp Hochreiter & Jürgen Schmidhuber      | *Long Short-Term Memory* (Neural Computation)                                                   | Solved vanishing gradients; made RNNs practical for long sequences.          |\n",
        "| **2014** | Cho et al. (Kyunghyun Cho, Yoshua Bengio) | *Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation* | Introduced the encoder–decoder concept for sequence transduction.            |\n",
        "| **2014** | Sutskever, Vinyals, Le (Google)           | *Sequence to Sequence Learning with Neural Networks* (NIPS)                                     | Popularized encoder–decoder LSTM architecture; formalized *teacher forcing*. |\n",
        "\n",
        "---\n",
        "\n",
        "##  2️ **Attention Mechanisms — The Step Before Transformers**\n",
        "\n",
        "| Year     | Authors                | Title / Venue                                                           | Key Contribution                                                                     |\n",
        "| -------- | ---------------------- | ----------------------------------------------------------------------- | ------------------------------------------------------------------------------------ |\n",
        "| **2014** | Bahdanau, Cho & Bengio | *Neural Machine Translation by Jointly Learning to Align and Translate* | Introduced the *additive attention* mechanism (alignment between encoder & decoder). |\n",
        "| **2015** | Luong, Pham & Manning  | *Effective Approaches to Attention-based Neural Machine Translation*    | Refined attention (dot, general, concat) and coined *global/local attention*.        |\n",
        "| **2016** | Xu et al.              | *Show, Attend and Tell*                                                 | Applied attention to vision (image captioning).                                      |\n",
        "\n",
        "---\n",
        "\n",
        "##  3️ **Transformer Revolution**\n",
        "\n",
        "| Year     | Authors                       | Title / Venue                                                                      | Key Contribution                                                                                                                  |\n",
        "| -------- | ----------------------------- | ---------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **2017** | Vaswani et al. (Google Brain) | *Attention Is All You Need* (NeurIPS)                                              | Introduced the **Transformer** architecture — self-attention, positional encoding, encoder–decoder blocks, and parallel training. |\n",
        "| **2018** | Devlin et al. (Google)        | *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding* | Introduced **bidirectional pretraining** and the masked-language-model objective.                                                 |\n",
        "| **2018** | Radford et al. (OpenAI)       | *Improving Language Understanding by Generative Pre-Training*                      | Introduced **GPT-1** — a decoder-only Transformer trained via next-token prediction.                                              |\n",
        "| **2019** | Radford et al.                | *Language Models are Unsupervised Multitask Learners*                              | **GPT-2** — large-scale unsupervised language model demonstrating zero-shot generalization.                                       |\n",
        "| **2020** | Brown et al. (OpenAI)         | *Language Models are Few-Shot Learners*                                            | **GPT-3** — massive scaling; established the *in-context learning* paradigm.                                                      |\n",
        "\n",
        "---\n",
        "\n",
        "##  4️ **Training Dynamics & Teacher Forcing Extensions**\n",
        "\n",
        "| Year          | Authors                   | Title / Venue                                                                      | Key Contribution                                                                                |\n",
        "| ------------- | ------------------------- | ---------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- |\n",
        "| **2015**      | Bengio et al.             | *Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks* (NIPS) | Proposed **scheduled sampling** to reduce *exposure bias* from teacher forcing.                 |\n",
        "| **2016**      | Lamb et al.               | *Professor Forcing: A New Algorithm for Training Recurrent Networks* (NIPS)        | Adversarially aligned the hidden-state dynamics between training and inference.                 |\n",
        "| **2017–2023** | OpenAI, Anthropic, Google | *RLHF and Instruction-Tuning papers (InstructGPT, Constitutional AI, FLAN)*        | Reinforced fine-tuning to align generation quality and mitigate exposure bias during inference. |\n",
        "\n",
        "---\n",
        "\n",
        "##  5️ **Efficient Attention & Scaling Extensions**\n",
        "\n",
        "| Year     | Authors        | Title / Venue                                               | Key Contribution                                                                  |\n",
        "| -------- | -------------- | ----------------------------------------------------------- | --------------------------------------------------------------------------------- |\n",
        "| **2020** | Child et al.   | *Generating Long Sequences with Sparse Transformers*        | Introduced sparse self-attention for efficiency.                                  |\n",
        "| **2022** | Dao et al.     | *FlashAttention: Fast and Memory-Efficient Exact Attention* | Hardware-optimized attention computation.                                         |\n",
        "| **2023** | Touvron et al. | *LLaMA: Open and Efficient Foundation Language Models*      | Showed scaling laws and training recipes for efficient large decoder-only models. |\n",
        "\n",
        "---\n",
        "\n",
        "##  Summary of Core Concept Origins\n",
        "\n",
        "| Concept                                | Origin Paper                                                            | Year      |\n",
        "| -------------------------------------- | ----------------------------------------------------------------------- | --------- |\n",
        "| **Teacher Forcing**                    | Sutskever et al. — *Seq2Seq*                                            | 2014      |\n",
        "| **Attention Mechanism**                | Bahdanau et al.                                                         | 2014      |\n",
        "| **Cross-Attention (Encoder→Decoder)**  | Bahdanau et al.; Vaswani et al.                                         | 2014–2017 |\n",
        "| **Self-Attention**                     | Vaswani et al.                                                          | 2017      |\n",
        "| **Transformer Architecture**           | Vaswani et al.                                                          | 2017      |\n",
        "| **Decoder-Only (GPT)**                 | Radford et al.                                                          | 2018      |\n",
        "| **Exposure Bias / Scheduled Sampling** | Bengio et al.                                                           | 2015      |\n",
        "| **Foundation Models Concept**          | Bommasani et al., *On the Opportunities and Risks of Foundation Models* | 2021      |\n",
        "\n",
        "---\n",
        "\n",
        "Would you like me to expand this into a **visual timeline (text-based chart)** showing how each breakthrough built on the previous one — from *teacher forcing → attention → Transformer → GPT*?\n"
      ],
      "metadata": {
        "id": "vvSIMW95mG4g"
      }
    }
  ]
}